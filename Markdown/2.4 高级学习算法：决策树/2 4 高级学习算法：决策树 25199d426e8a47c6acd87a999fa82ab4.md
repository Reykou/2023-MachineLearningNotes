# 2.4 高级学习算法：决策树

復習: Done
日付: 2023年4月8日
最終更新日時: 2023年4月15日 22:27

---

# 决策树：**Decision trees**

## 决策树模型：[Decision tree model](https://www.coursera.org/learn/advanced-learning-algorithms/lecture/HFvPH/decision-tree-model)

*dəˈsɪʒ(ə)n*

- **决策树：决策树是一种监督学习算法，用于解决分类和回归任务。**在这个例子中，我们使用决策树对猫进行分类。
- **特征与标签**：输入特征（X）是用于预测的数据，例如动物的耳朵形状、脸型和是否有胡须等。标签（Y）是我们尝试预测的目标输出，例如是否是猫。
    
    ![截屏2023-04-08 16.43.01.png](2%204%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E5%86%B3%E7%AD%96%E6%A0%91%2025199d426e8a47c6acd87a999fa82ab4/%25E6%2588%25AA%25E5%25B1%258F2023-04-08_16.43.01.png)
    
- **节点**：决策树中的每个节点分为以下几种：
    - **根节点**：位于树顶部的节点，从这里开始分类过程。
    - **决策节点**：根据特征值，根据特征值决定向左还是向右继续遍历树的节点。
    - **叶子节点**：树底部的节点，包含最终的预测结果。
    
    ![截屏2023-04-08 16.43.11.png](2%204%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E5%86%B3%E7%AD%96%E6%A0%91%2025199d426e8a47c6acd87a999fa82ab4/%25E6%2588%25AA%25E5%25B1%258F2023-04-08_16.43.11.png)
    
- **分类过程**：**在决策树中，从根节点开始，根据节点内的特征，选择向左或向右遍历树，直到达到叶子节点。叶子节点包含分类结果。**
- **决策树学习算法**：决策树学习算法的任务是在所有可能的决策树中选择一个表现良好的决策树，即在**训练集上表现良好且能很好地泛化到新数据（如交叉验证集和测试集）的决策树**。
    
    ![截屏2023-04-08 16.43.18.png](2%204%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E5%86%B3%E7%AD%96%E6%A0%91%2025199d426e8a47c6acd87a999fa82ab4/%25E6%2588%25AA%25E5%25B1%258F2023-04-08_16.43.18.png)
    

> **本节一句话总结：**
监督学习-决策树算法，用于分类和回归问题。
决策树的节点类型：根节点、决策节点、叶子节点（分类结果y）
决策树的分类过程：从根开始，根据特征选择左右遍历，指导叶子。
选择训练集和交叉验证集、测试集均表现好的决策树
> 

## 根据训练集构建决策树：[Learning Process](https://www.coursera.org/learn/advanced-learning-algorithms/lecture/5ysdd/learning-process)

在这个视频中，我们介绍了如何构建决策树。决策树是一种监督学习方法，用于对输入数据进行分类。以下是构建决策树的关键知识点：

- **选择用于分割的特征**：**决策树的每个节点都需要选择一个特征来分割数据。**我们希望选择能够使得分割后的数据尽量纯净（即同一类别的数据尽量聚集在一起）的特征。在后续视频中，**我们将学习如何通过计算熵来评估不纯度，并选择能够最大化纯度的特征进行分割。**
    
    ![截屏2023-04-08 16.58.36.png](2%204%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E5%86%B3%E7%AD%96%E6%A0%91%2025199d426e8a47c6acd87a999fa82ab4/%25E6%2588%25AA%25E5%25B1%258F2023-04-08_16.58.36.png)
    
    ![截屏2023-04-08 17.11.13.png](2%204%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E5%86%B3%E7%AD%96%E6%A0%91%2025199d426e8a47c6acd87a999fa82ab4/%25E6%2588%25AA%25E5%25B1%258F2023-04-08_17.11.13.png)
    
    ![截屏2023-04-08 17.03.15.png](2%204%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E5%86%B3%E7%AD%96%E6%A0%91%2025199d426e8a47c6acd87a999fa82ab4/%25E6%2588%25AA%25E5%25B1%258F2023-04-08_17.03.15.png)
    
- **决定何时停止分割**：在构建决策树时，我们需要确定何时停止分割数据。这里有几种可能的情况：
    - **当节点中的数据完全纯净**（即全部是同一类别）时，我们可以创建一个叶节点进行分类预测。
    - **当树的深度达到预设的最大深度时，**可以停止分割。这样可以保证决策树不会过于复杂，从而降低过拟合的风险。
    - **当分割带来的纯度改善很小或低于一定阈值时**，可以停止分割。这同样有助于保持决策树的简单性并减少过拟合风险。
    - **当节点中的数据数量低于一定阈值时**，也可以停止分割。
    
    ![截屏2023-04-08 17.10.55.png](2%204%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E5%86%B3%E7%AD%96%E6%A0%91%2025199d426e8a47c6acd87a999fa82ab4/%25E6%2588%25AA%25E5%25B1%258F2023-04-08_17.10.55.png)
    

> **本节一句话总结：**
决策树的每个节点都是一个特征分割数据，选分割后尽量纯净的特征。
停止分割：纯净度高或纯度改善小、树深度高、节点中的数据量低。
> 

---

# 决策树学习：**Decision tree learning**

## 衡量纯度：[Measuring purity](https://www.coursera.org/learn/advanced-learning-algorithms/lecture/6jL2z/measuring-purity)

*ˈmɛʒər*

在这个视频中，我们讨论了**如何衡量一组样本的纯度**。纯度的概念在决策树算法中非常重要，因为它帮助我们决定在何处对数据进行分割。为了量化纯度，我们引入了熵（entropy *ˈɛntrəpi*）的概念。**熵是一种衡量数据集不纯度的指标**。

- 首先，我们定义了 p_1，它表示样本中类别为 1（例如猫）的实例所占的比例。我们使用**熵（H）**来衡量样本的不纯度，其中横轴表示 p_1，纵轴表示熵的值。**当样本中类别 1 的实例和类别 0 的实例数量相等时（例如猫和狗各占一半），熵值最大，不纯度最高。**相反，当样本中全部是类别 1 或类别 0 时，熵值为 0，纯度最高。
    
    ![截屏2023-04-08 17.32.30.png](2%204%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E5%86%B3%E7%AD%96%E6%A0%91%2025199d426e8a47c6acd87a999fa82ab4/%25E6%2588%25AA%25E5%25B1%258F2023-04-08_17.32.30.png)
    
    熵的公式如下：
    
    H(p_1) = -p_1 * log2(p_1) - p_0 * log2(p_0)
    
    其中，p_0 为样本中类别 0 的实例所占比例，且 p_0 = 1 - p_1。在计算熵时，我们采用以 2 为底的对数，而不是以 e 为底。这样做是为了让曲线的峰值等于 1，使得结果更容易解释。
    
    在构建决策树时，我们可以使用熵来决定在节点处应该根据哪个特征进行分割。通常情况下，我们会**选择能使熵最大程度降低的特征进行分割**。这意味着我们希望通过分割使得子集中的纯度尽可能高。
    

此外，还有其他类似于熵的不纯度度量指标，如基尼指数（Gini index）。在实际应用中，基尼指数和熵都可以用于构建决策树。在这个视频中，我们主要关注熵，因为它通常适用于大多数应用场景。

总之，熵函数是衡量数据集不纯度的指标，它可以帮助我们决定如何在决策树节点处根据特征进行分割。在后续的学习中，我们将进一步讨论如何使用熵来构建决策树。

> **本节一句话总结：**
熵：混乱程度，衡量不纯度。
熵： `**Entropy(P) = -P * log2(P) - (1 - P) * log2(1 - P)` ，**P 是正例占总数的比例。
样本 1/0占比相等时、熵最大。 
选择熵最大程度降低的特征进行分割。
> 

## 选择拆分：信息增益：[Choosing a split: Information Gain](https://www.coursera.org/learn/advanced-learning-algorithms/lecture/ZSbs2/choosing-a-split-information-gain)

*splɪt 分割*  

在上面的视频中，我们讨论了构建决策树时如何计算信息增益（Information Gain）以选择在节点上进行拆分的特征。**信息增益衡量了通过进行拆分在树中获得的熵（Entropy *ˈɛntrəpi*）减少**。这里，我们回顾关键知识点并详细解释。

- 熵（Entropy）: 熵是一种衡量数据集不纯度（impurity）或混乱程度的指标。当熵较低时，数据集的纯度较高。计算熵的公式为：Entropy(P) = -P * log2(P) - (1 - P) * log2(1 - P)，其中 P 是正例占总数的比例。
- 信息增益（**Information Gain**）: **信息增益是根据选择特征进行拆分后的熵减少量**。我们通过计算不同特征拆分后的熵减少量来确定哪个特征是最佳拆分特征。
    
    ![截屏2023-04-08 17.46.43.png](2%204%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E5%86%B3%E7%AD%96%E6%A0%91%2025199d426e8a47c6acd87a999fa82ab4/%25E6%2588%25AA%25E5%25B1%258F2023-04-08_17.46.43.png)
    
- **加权熵（Weighted Entropy）**: 为了选择最佳拆分特征，我们需要计算每个可能特征的加权熵。加权熵计算方法为：
    
    w_left * Entropy(p1_left) + w_right * Entropy(p1_right)
    
    **其中 w_left 和 w_right 分别表示左子树和右子树中样本所占的比例。**
    
    ![截屏2023-04-08 20.12.21.png](2%204%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E5%86%B3%E7%AD%96%E6%A0%91%2025199d426e8a47c6acd87a999fa82ab4/%25E6%2588%25AA%25E5%25B1%258F2023-04-08_20.12.21.png)
    
- 计算信息增益的公式为：
Information Gain = Entropy(p1_root) - Weighted Entropy
    
    其中 Entropy(p1_root) 是根节点的熵，Weighted Entropy 是根据拆分特征计算的加权熵。
    
    ![截屏2023-04-08 17.46.49.png](2%204%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E5%86%B3%E7%AD%96%E6%A0%91%2025199d426e8a47c6acd87a999fa82ab4/%25E6%2588%25AA%25E5%25B1%258F2023-04-08_17.46.49.png)
    
- 在构建决策树时，我们按照以下步骤操作：
    - 计算根节点的熵。
    - 对于每个可能的特征，计算拆分后的加权熵。
    - 计算每个特征的信息增益。
    - **选择具有最大信息增益的特征进行拆分**。
    
    如此反复执行，**直到满足停止条件**（如树的深度达到预定阈值，或信息增益小于预定阈值），最终得到决策树。这种基于信息增益选择拆分特征的方法有助于提高决策树的预测准确性，并降低过拟合风险。
    

> **本节一句话总结：**
信息增量（Infomation Gain）： 拆分后的熵entropy 减少量。
选择具有最大信息增益的特征进行拆分。
**`Weighted Entropy = w_left * Entropy(p1_left) + w_right * Entropy(p1_right)`
`Information Gain = Entropy(p1_root) - Weighted Entropy`**
> 

## 整合：[Putting it together](https://www.coursera.org/learn/advanced-learning-algorithms/lecture/a51O3/putting-it-together)

详细回顾一下关键知识点：

- 信息增益（Information Gain）：用于评估特征划分的效果。信息增益越大，说明用该特征进行划分的效果越好。计算信息增益时，使用熵（Entropy）作为度量不纯度的标准。
- 熵（Entropy）：度量数据的不纯度。决策树构建过程中，目标是使子节点的熵尽可能地降低。根据特征划分数据时，选择信息增益最大的特征进行划分。
- 停止划分的条件（Stopping Criteria）：当满足以下条件之一时，可以停止决策树的划分过程：
    - **节点中样本全部属于同一类别。**
    - **划分后信息增益小于阈值。**
    - **达到设定的最大树深度。**
    - **节点中样本数量小于阈值。**
    
    ![截屏2023-04-08 18.04.45.png](2%204%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E5%86%B3%E7%AD%96%E6%A0%91%2025199d426e8a47c6acd87a999fa82ab4/%25E6%2588%25AA%25E5%25B1%258F2023-04-08_18.04.45.png)
    
- **递归算法（Recursive Algorithm）**：在计算机科学中，递归算法指的是一个调用自身的算法。在构建决策树时，递归算法在**建立整个决策树的过程中用于构建左右子树**。这意味着你可以在左子树和右子树上重复应用相同的构建过程。
    
    ![截屏2023-04-08 18.05.04.png](2%204%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E5%86%B3%E7%AD%96%E6%A0%91%2025199d426e8a47c6acd87a999fa82ab4/%25E6%2588%25AA%25E5%25B1%258F2023-04-08_18.05.04.png)
    
    ![截屏2023-04-08 18.05.14.png](2%204%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E5%86%B3%E7%AD%96%E6%A0%91%2025199d426e8a47c6acd87a999fa82ab4/%25E6%2588%25AA%25E5%25B1%258F2023-04-08_18.05.14.png)
    
    ![截屏2023-04-08 18.05.21.png](2%204%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E5%86%B3%E7%AD%96%E6%A0%91%2025199d426e8a47c6acd87a999fa82ab4/%25E6%2588%25AA%25E5%25B1%258F2023-04-08_18.05.21.png)
    
    ![截屏2023-04-08 18.05.23.png](2%204%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E5%86%B3%E7%AD%96%E6%A0%91%2025199d426e8a47c6acd87a999fa82ab4/%25E6%2588%25AA%25E5%25B1%258F2023-04-08_18.05.23.png)
    
- **多值特征（Multi-valued Features）**：在某些情况下，特征可能具有多个离散值，而不仅仅是两个值。处理这种情况的方法是将多值特征转换为多个二值特征，以便在决策树中使用。
    - 🔰问题：什么是多个离散值？将多值特征转换为多个二值特征是什么？
        
        多个离散值（Multi-valued Features）是指某个特征可以具有多于两个的可能取值。例如，一个**描述动物颜色的特征可能具有 "红色"、"绿色" 和 "蓝色" 等多个离散值**。
        
        将多值特征转换为多个二值特征是一种处理多值特征的方法，主要使用“独热编码”（One-Hot Encoding）技术。独热编码将一个具有 n 个可能取值的多值特征转换为 n 个二值特征，每个二值特征对应原特征的一个可能取值。这些新的二值特征在决策树中可以像其他二值特征一样处理。
        
        以动物颜色为例，假设我们有以下三个样本：
        
        | 样本 | 颜色 |
        | --- | --- |
        | 1 | 红色 |
        | 2 | 绿色 |
        | 3 | 蓝色 |
        
        将颜色特征进行独热编码，我们得到以下形式：
        
        | 样本 | 红色 | 绿色 | 蓝色 |
        | --- | --- | --- | --- |
        | 1 | 1 | 0 | 0 |
        | 2 | 0 | 1 | 0 |
        | 3 | 0 | 0 | 1 |
        
        现在，原本的一个多值特征（颜色）被转换为三个二值特征（红色、绿色和蓝色）。这些新特征可以在决策树中使用，就像处理其他二值特征一样。
        

通过以上关键知识点，您可以更好地理解决策树构建过程以及如何使用递归算法在决策树中创建子树。为了获得更好的模型泛化能力，可以调整停止划分条件、设置最大树深度等参数以防止过拟合。

> **本节一句话总结：**
递归算法：建立整个决策树的过程复用于构建左右子树。
停止条件：节点中样本全部属于同一类别、划分后信息增益小于阈值、达到设定的最大树深度、节点中样本数量小于阈值。
> 

## **独热编码：**[Using one-hot encoding of categorical features](https://www.coursera.org/learn/advanced-learning-algorithms/lecture/RQVdw/using-one-hot-encoding-of-categorical-features)

在这段视频中，讲解了如何使用**独热编码（One-hot Encoding）处理具有多个离散值的特征。**以下是关键知识点的详细解释：

- **多值特征**：在这个例子中，耳朵形状不再仅限于尖形和下垂，还可以是椭圆形。这意味着原始特征仍然是一个分类值特征，但它可以有三个可能的值而不仅仅是两个。
    
    ![截屏2023-04-08 19.08.21.png](2%204%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E5%86%B3%E7%AD%96%E6%A0%91%2025199d426e8a47c6acd87a999fa82ab4/%25E6%2588%25AA%25E5%25B1%258F2023-04-08_19.08.21.png)
    
- **独热编码**：为了解决这个问题，可以使用独热编码。通过创建三个新特征来替换原始的耳朵形状特征：尖形耳朵、下垂耳朵和椭圆形耳朵。这样，**每个样本的耳朵形状特征将被三个二值特征（0或1）替代。**
    
    ![截屏2023-04-08 19.09.20.png](2%204%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E5%86%B3%E7%AD%96%E6%A0%91%2025199d426e8a47c6acd87a999fa82ab4/%25E6%2588%25AA%25E5%25B1%258F2023-04-08_19.09.20.png)
    
- 独热编码的通用性：**独热编码不仅适用于决策树学习，还可以用于神经网络、线性回归和逻辑回归。**将分类特征编码为0和1的形式，使其可以作为神经网络等模型的输入。

总结一下，独热编码是一种处理具有多个离散值的特征的方法。通过将这些特征转换为二值特征（0或1），可以在决策树、神经网络、线性回归和逻辑回归等模型中使用。对于连续值特征，需要采用其他方法来处理。

> **本节一句话总结：**
one-hot encoding ：处理多个离散的特征值→ 0/1 特征、适用于决策树/神经网络/线性回归/逻辑回归
> 

## 连续特征值：[Continuous valued features](https://www.coursera.org/learn/advanced-learning-algorithms/lecture/a4v1O/continuous-valued-features)

在这段视频中，讲解了**如何修改决策树以处理连续值特征**，而不仅仅是离散值特征。以下是关键知识点的详细解释：

- **连续值特征**：在这个例子中，增加了一个新特征，即动物的体重。体重特征有助于区分猫和狗，因为平均而言，猫的体重比狗轻。那么如何让决策树使用这样的特征呢？
- 处理连续值特征：与之前的方法类似，决策树学习算法需要考虑在不同特征上进行分割，包括体重特征。如果在体重特征上分割可以获得更好的信息增益，那么就选择该特征进行分割。但如何确定如何在体重特征上进行分割呢？
    
    ![截屏2023-04-08 19.08.21.png](2%204%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E5%86%B3%E7%AD%96%E6%A0%91%2025199d426e8a47c6acd87a999fa82ab4/%25E6%2588%25AA%25E5%25B1%258F2023-04-08_19.08.21%201.png)
    
- **选择阈值：为了在连续值特征上进行分割，需要尝试多个阈值，然后选择能够产生最佳信息增益的阈值。**例如，在这个例子中，算法可以尝试将阈值设置为8、9和13等不同的值，然后根据信息增益的计算结果选择最佳阈值。
    
    ![截屏2023-04-08 19.16.43.png](2%204%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E5%86%B3%E7%AD%96%E6%A0%91%2025199d426e8a47c6acd87a999fa82ab4/%25E6%2588%25AA%25E5%25B1%258F2023-04-08_19.16.43.png)
    
- 实际应用中的阈值选择：在实际应用中，可能需要尝试更多的阈值。一种惯例是根据特征值对所有示例进行排序，然后在排序列表的中点处取值。这样，如果有10个训练示例，将测试9个不同的阈值，然后选择能够产生最高信息增益的阈值。
- 分割数据集：如果某个阈值的信息增益高于其他特征的信息增益，则根据该阈值对数据集进行分割。然后，可以使用分割后的数据集构建子树。

总之，为了让决策树处理连续值特征，需要在每个节点尝试不同的分割值，进行信息增益计算，并在信息增益最大时选择连续值特征进行分割。通过尝试不同的阈值、进行信息增益计算并选择产生最佳信息增益的连续值特征进行分割，可以使决策树适应连续值特征。在核心决策树算法的讲解部分，这是需要了解的关键知识点。

> **本节一句话总结：**
决策树用于处理离散特征值
分割连续特征值：尝试多个阈值后，选择最大信息增益的阈值。
> 

## 回归树：[Regression Trees (optional)](https://www.coursera.org/learn/advanced-learning-algorithms/lecture/XlM5n/regression-trees-optional)

这个视频讲解了**如何使用决策树进行回归分析，预测一个连续值（例如动物的体重）**。为了解释关键知识点，我们首先需要了解决策树的基本概念，然后讨论如何将其扩展到回归问题。

决策树是一种树形结构的机器学习算法，用于分类或回归任务。决策树中的每个内部节点表示一个属性或特征，每个分支表示一个特征值，叶节点表示一个类别（分类任务）或一个值（回归任务）。

关键知识点如下：

- 决策树回归与分类的区别：在分类问题中，我们试图预测一个离散值（例如动物是否是猫）。在回归问题中，我们试图预测一个连续值（例如动物的体重）。
- 预测方法：对于决策树回归，**预测是通过计算训练样本中相应叶节点的目标值（例如体重）的平均值来进行的**。
    
    ![截屏2023-04-08 19.37.08.png](2%204%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E5%86%B3%E7%AD%96%E6%A0%91%2025199d426e8a47c6acd87a999fa82ab4/%25E6%2588%25AA%25E5%25B1%258F2023-04-08_19.37.08.png)
    
- 分裂特征的选择：在构建决策树时，需要确定用于分裂的特征。在回归树中，我们试图最小化每个子集中目标值（例如体重）的方差。与分类任务中的熵减类似，我们使用加权方差的减少来度量分裂的质量。
- 方差：方差是一个统计量，用于度量一组数字的变化程度。较小的方差表示数据点相对集中，而较大的方差表示数据点分散。
    - 方差 = Σ((每个值 - 平均值)²) / (值的数量)
- 加权方差：在评估分裂质量时，我们计算左侧分支和右侧分支的加权方差。这与分类问题中使用的加权平均熵非常相似。
- 方差减少：与信息增益类似，我们不仅仅关注平均加权方差，还关注方差的减少。**选择使方差减少最大的特征作为分裂特征。**
    
    ![截屏2023-04-08 19.37.18.png](2%204%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E5%86%B3%E7%AD%96%E6%A0%91%2025199d426e8a47c6acd87a999fa82ab4/%25E6%2588%25AA%25E5%25B1%258F2023-04-08_19.37.18.png)
    

总之，在回归问题中，决策树的构建和分裂特征的选择基于方差和方差减少。预测是通过计算叶节点处训练样本目标值的平均值来进行的。通过这种方法，决策树可以应用于回归问题，预测连续值。

> **本节一句话总结：**
决策树用于回归问题，预测连续值。
选择 Infomation Gain 最大的特征作为分裂特征。
通过叶节点的平均值来预测。
> 

## [Optional Lab: Decision Trees](https://www.coursera.org/learn/advanced-learning-algorithms/ungradedLab/hPtix/optional-lab-decision-trees)

```python
root_indices = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]

# Feel free to play around with these variables
# The dataset only has three features, so this value can be 0 (Brown Cap), 1 (Tapering Stalk Shape) or 2 (Solitary)
feature = 0

left_indices, right_indices = split_dataset(X_train, root_indices, feature)

print("Left indices: ", left_indices)
print("Right indices: ", right_indices)

# UNIT TESTS    
split_dataset_test(split_dataset)
```

这段代码在使用 **`split_dataset`** 函数将数据集根据特定的特征（feature）拆分为左侧和右侧的子集。这是决策树算法中的一步，用于在每个节点上确定最佳拆分特征。

在这个例子中：

- **`root_indices`** 是一个包含数据集前 10 个样本索引的列表。
- **`feature`** 是一个整数，表示选择哪个特征进行拆分。在这里，值为 0，即基于 "Brown Cap" 特征进行拆分。

然后，代码调用了 **`split_dataset`** 函数，将数据集根据 "Brown Cap" 特征拆分为左侧和右侧的子集。**`split_dataset`** 函数返回两个列表，一个是左侧子集的索引，另一个是右侧子集的索引。

最后，代码打印了左侧和右侧子集的索引，并运行了一个单元测试，以确保 **`split_dataset`** 函数的实现是正确的。

下面的代码和上面这个类似，更规整。

![截屏2023-04-08 20.03.48.png](2%204%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E5%86%B3%E7%AD%96%E6%A0%91%2025199d426e8a47c6acd87a999fa82ab4/%25E6%2588%25AA%25E5%25B1%258F2023-04-08_20.03.48.png)

![截屏2023-04-08 20.04.49.png](2%204%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E5%86%B3%E7%AD%96%E6%A0%91%2025199d426e8a47c6acd87a999fa82ab4/%25E6%2588%25AA%25E5%25B1%258F2023-04-08_20.04.49.png)

![截屏2023-04-09 11.06.35.png](2%204%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E5%86%B3%E7%AD%96%E6%A0%91%2025199d426e8a47c6acd87a999fa82ab4/%25E6%2588%25AA%25E5%25B1%258F2023-04-09_11.06.35.png)

![截屏2023-04-09 11.06.46.png](2%204%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E5%86%B3%E7%AD%96%E6%A0%91%2025199d426e8a47c6acd87a999fa82ab4/%25E6%2588%25AA%25E5%25B1%258F2023-04-09_11.06.46.png)

- 👆🏻 把整个集合 根据 inxdex_feature =0 （决策节点 X[0]）分割成左右 2 个子集（叶子集合）

![截屏2023-04-08 20.10.21.png](2%204%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E5%86%B3%E7%AD%96%E6%A0%91%2025199d426e8a47c6acd87a999fa82ab4/%25E6%2588%25AA%25E5%25B1%258F2023-04-08_20.10.21.png)

- 👆🏻 `weighted_entropy` 的参数分别是原始集合的 `X_train`, `y_train` 和分割后的集合 `left_indices`，`right_indices`

![Figure 3.png](2%204%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E5%86%B3%E7%AD%96%E6%A0%91%2025199d426e8a47c6acd87a999fa82ab4/Figure_3.png)

---

# 树集：**Tree ensembles**

## 使用多决策树：[Using multiple decision trees](https://www.coursera.org/learn/advanced-learning-algorithms/lecture/3Epc2/using-multiple-decision-trees)

决策树算法的鲁棒性问题和树集成方法的介绍。我将为您详细解释其中的关键知识点。

- 单一决策树的缺点：单一决策树可能对数据中的小变化非常敏感，从而导致其鲁棒性较差。
    
    ![截屏2023-04-08 20.36.08.png](2%204%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E5%86%B3%E7%AD%96%E6%A0%91%2025199d426e8a47c6acd87a999fa82ab4/%25E6%2588%25AA%25E5%25B1%258F2023-04-08_20.36.08.png)
    
- **树集成（Tree Ensemble** *ɑnˈsɑmbəl***）**：为了解决单一决策树的鲁棒性问题，可以使用树集成方法。树集成是指构建多个决策树并将它们组合在一起，以提高模型的鲁棒性和预测准确性。
- 投票策略：在树集成中，每个决策树都会对新的测试样本进行预测，然后根据这些预测进行投票。**多数投票决定最终的预测结果。**
    
    ![截屏2023-04-08 20.42.56.png](2%204%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E5%86%B3%E7%AD%96%E6%A0%91%2025199d426e8a47c6acd87a999fa82ab4/%25E6%2588%25AA%25E5%25B1%258F2023-04-08_20.42.56.png)
    

> **本节一句话总结：**
单一树的缺点：对小变化敏感，导致鲁棒性差
****树集成→ 投票策略
> 

## 有放回抽样：[Sampling with replacement](https://www.coursera.org/learn/advanced-learning-algorithms/lecture/zZ6pa/sampling-with-replacement)

- 有放回抽样（Sampling with Replacement）：**有放回抽样是一种统计方法，其中从数据集中随机选择样本，并在抽样后将样本放回数据集。**这意味着同一个样本可能会被多次选择
    - 在构建树集成时，**有放回抽样可以用于从原始数据集中生成多个不同的训练子集，以便为每个子集训练一个独立的决策树。**
    - 这样可以**确保树集成中的决策树具有足够的多样性，提高模型的鲁棒性**。
    
    ![截屏2023-04-08 20.47.13.png](2%204%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E5%86%B3%E7%AD%96%E6%A0%91%2025199d426e8a47c6acd87a999fa82ab4/%25E6%2588%25AA%25E5%25B1%258F2023-04-08_20.47.13.png)
    
- 例如，我们有一个包含10个猫和狗的训练样本。我们将这10个训练样本放入一个理论上的袋子中。然后，我们从袋子中随机抽取一个训练样本，记录它，然后将其放回袋子。我们重复这个过程10次，得到一个新的训练子集。新训练子集可能包含重复的样本，并且可能不包含原始训练集中的所有样本，但这并不影响。
    
    ![截屏2023-04-08 20.48.56.png](2%204%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E5%86%B3%E7%AD%96%E6%A0%91%2025199d426e8a47c6acd87a999fa82ab4/%25E6%2588%25AA%25E5%25B1%258F2023-04-08_20.48.56.png)
    

> **本节一句话总结：**
有放回抽样：随机选择样本、抽样后放回。
优点：从原始数据集生成不同的训练子集，每个子集训练一个独立的决策树。
→树集成：多样性、提高鲁棒性。
> 

## 随机森林算法：[Random forest algorithm](https://www.coursera.org/learn/advanced-learning-algorithms/lecture/7MtSD/random-forest-algorithm)

介绍了如何使用有放回抽样生成树集成，并特别关注了随机森林算法。以下是关键知识点的详细解释：

- 随机森林算法：这是一种强大的基于树的集成学习算法，性能比单个决策树要好得多。**通过使用有放回抽样生成多个不同的训练子集，我们可以为每个子集训练一个独立的决策树。**然后，**我们可以将这些决策树组合成一个集成模型，对于预测任务，让这些树进行投票以得到最终的预测结果。**
- 创建随机森林的步骤：
    - 给定一个大小为M的训练集；
    - 对于B=1到大写B（B表示要构建的树的数量，通常为100左右），执行以下操作：
        - 使用有放回抽样创建一个大小为M的新训练集；
        - 在新训练集上训练一个决策树；
    - 对于预测任务，让这些决策树进行投票以得到最终的预测结果。
    
    ![截屏2023-04-08 20.50.53.png](2%204%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E5%86%B3%E7%AD%96%E6%A0%91%2025199d426e8a47c6acd87a999fa82ab4/%25E6%2588%25AA%25E5%25B1%258F2023-04-08_20.50.53.png)
    
- 袋装决策树：这是一种特定的树集成方法，将训练样本放入一个虚拟袋子中，然后使用有放回抽样生成训练子集。这种方法是随机森林的基础。
- **从袋装决策树到随机森林的改进**：为了使树集成中的决策树更具多样性，我们可以在**每个节点选择划分特征时进一步引入随机性。**具体来说，在每个节点，我们可以**从所有N个特征中随机选择一个大小为K（K小于N）的子集，**然后**从这个子集中选择具有最高信息增益的特征作为划分特征。**当特征数量N较大时，一个典型的选择是令K等于N的平方根。
    
    ![截屏2023-04-08 20.50.58.png](2%204%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E5%86%B3%E7%AD%96%E6%A0%91%2025199d426e8a47c6acd87a999fa82ab4/%25E6%2588%25AA%25E5%25B1%258F2023-04-08_20.50.58.png)
    
- 随机森林的鲁棒性：由于有放回抽样过程使得算法探索了许多对数据的小变化，并且在训练不同的决策树时对这些变化进行平均，所以对训练集的进一步小变化不太可能对随机森林算法的整体输出产生巨大影响。这使得随机森林比单个决策树更具鲁棒性。

> **本节一句话总结：**
随机森林算法：原始数据→有放回抽样的子集→分别训练不同的树→多个树投票决策
随机森林的改进：每个节点选择划分特征时引入随机性。N 个特征→选 K 的子集→最高信息增益的特征做为划分特征。
> 

## 极端梯度提升:[XGBoost](https://www.coursera.org/learn/advanced-learning-algorithms/lecture/op26P/xgboost)

在这门课程的这一部分，重点介绍了XGBoost，这是一种流行且高效的提升决策树实现方法。以下是关键知识点：

- 提升（Boosting）：提升是一种集成方法，专注于当前集成中表现不佳的示例。它通过修改各个样本的权重，在创建每个树时强调较难的示例。这个概念**类似于刻意练习的思想，即学习者专注于自己最难掌握的领域。**
    
    ![截屏2023-04-08 21.12.06.png](2%204%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E5%86%B3%E7%AD%96%E6%A0%91%2025199d426e8a47c6acd87a999fa82ab4/%25E6%2588%25AA%25E5%25B1%258F2023-04-08_21.12.06.png)
    
- 实现：XGBoost（极端梯度提升）是提升决策树的一种开源实现，速度快、效率高。**XGBoost具有良好的默认分割标准和停止分割的标准。**XGBoost的一个创新之处在于它具有**内置的正则化功能，以防止过拟合**。在机器学习竞赛中，如Kaggle等竞赛网站，XGBoost通常是一种具有竞争力的算法。事实上，XGBoost和深度学习算法似乎是赢得这些竞赛的两种类型的算法。
    
    ![截屏2023-04-08 21.12.11.png](2%204%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E5%86%B3%E7%AD%96%E6%A0%91%2025199d426e8a47c6acd87a999fa82ab4/%25E6%2588%25AA%25E5%25B1%258F2023-04-08_21.12.11.png)
    
    - 🔰问题：XGBoost 是为错误预测的数据创建一个训练集合，然后专门用不同的树训练这个错误集合，一直到效果提升吗？
        
        XGBoost **的基本思想是通过迭代训练一系列决策树，每棵树都专注于纠正前一棵树的错误。**对于每个迭代步骤，XGBoost 会根据前一棵树的预测错误分配不同的权重给训练样本。这样，随着迭代进行，后续的树将更有可能关注那些被误分类或预测不准确的样本。
        
        简而言之，XGBoost 不是一次性为错误预测的数据创建一个训练集合，而是在每次迭代过程中根据当前模型的错误动态调整样本权重。这样的过程会持续进行，直到达到预定的迭代次数或者满足某些早停条件。这个过程使得 XGBoost 能够更好地关注难以预测的样本，从而提高整体模型的预测性能。
        
- 使用XGBoost：要使用XGBoost，您只需导入XGBoost库，然后按照以下步骤初始化一个模型，作为XGBoost分类器。接下来，训练模型，最后使用这个提升决策树算法进行预测。如果您希望将XGBoost用于回归而非分类，只需将这里的"XGBClassifier"替换为"XGBRegressor"，其他代码依然类似。
    
    ![截屏2023-04-08 21.12.18.png](2%204%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E5%86%B3%E7%AD%96%E6%A0%91%2025199d426e8a47c6acd87a999fa82ab4/%25E6%2588%25AA%25E5%25B1%258F2023-04-08_21.12.18.png)
    

> **本节一句话总结：**
Boosting：专注于当前集成中表现不佳的示例。
****XGBoost：迭代训练一些列的决策树，每棵树都专注纠正前一棵树的错误。线性回归**`model = XGBRegressor()`**  、分类 `**model = XGBClassifier()**`
> 

## 什么时候使用决策树：[When to use decision trees](https://www.coursera.org/learn/advanced-learning-algorithms/lecture/vh1V7/when-to-use-decision-trees)

本视频讲述了如何在决策树（包括树集成）和神经网络之间进行选择。下面是每个方法的优缺点：

- 决策树和树集成：
    - **通常在表格数据（也称为结构化数据）上表现良好**。如果你的数据集看起来像一个巨大的电子表格，那么决策树值得考虑。
    - 不推荐在非结构化数据（如图像、视频、音频和文本）上使用决策树和树集成。
    - 决策树和树集成可以非常快速地进行训练，这有助于更快地迭代和优化算法。
    - 小型决策树可能具有可解释性，但大型决策树或树集成的可解释性可能被高估。
    - 如果使用决策树或树集成，建议使用 XGBoost。
- 神经网络：
    - **在所有类型的数据上表现良好，包括结构化数据、非结构化数据以及混合数据。**
    - 与决策树相比，神经网络可能训练速度较慢。
    - **神经网络适用于迁移学习**，对于只有少量数据的应用而言，这非常重要。
    - 构建多个机器学习模型协同工作的系统时，将多个神经网络连接并训练可能比多个决策树更容易。
    
    ![截屏2023-04-08 21.32.05.png](2%204%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E5%86%B3%E7%AD%96%E6%A0%91%2025199d426e8a47c6acd87a999fa82ab4/%25E6%2588%25AA%25E5%25B1%258F2023-04-08_21.32.05.png)
    

> **本节一句话总结：**
决策树和树集成：表格数据、训练快速、小型决策树具有可解释性
神经网络：所有类型数据、训练慢、适用于迁移学习、协同工作
> 

## [Optional Lab: Tree Ensembles](https://www.coursera.org/learn/advanced-learning-algorithms/ungradedLab/kkC4N/optional-lab-tree-ensembles)

- 初步理解
    
    ![截屏2023-04-08 21.50.35.png](2%204%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E5%86%B3%E7%AD%96%E6%A0%91%2025199d426e8a47c6acd87a999fa82ab4/%25E6%2588%25AA%25E5%25B1%258F2023-04-08_21.50.35.png)
    
    ![截屏2023-04-08 21.50.43.png](2%204%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E5%86%B3%E7%AD%96%E6%A0%91%2025199d426e8a47c6acd87a999fa82ab4/%25E6%2588%25AA%25E5%25B1%258F2023-04-08_21.50.43.png)
    
    ```python
    X_train, X_val, y_train, y_val = train_test_split(df[features], df['HeartDisease'], train_size = 0.8, random_state = RANDOM_STATE)
    ```
    
    这段代码在做以下事情：
    
    1. 首先，将原始数据集（**`df`**）分为特征（**`features`**）和目标变量（**`HeartDisease`**）。
    2. 接着，使用 **`train_test_split`** 函数将数据集划分为训练集和验证集。划分比例为 80% 的训练集（**`X_train`** 和 **`y_train`**）和 20% 的验证集（**`X_val`** 和 **`y_val`**）。
    3. **`random_state`** 参数用于设置随机数生成器的种子，以确保在多次运行时获得相同的数据划分。这有助于确保实验结果的可重复性。
    4. 由于数据集没有时间依赖性，所以设置 **`shuffle=True`**（默认值）。这意味着在划分数据集之前，数据将被随机打乱。这样做可以确保训练集和验证集都是随机的样本，有助于更好地评估模型在未知数据上的性能。
    
    简而言之，这段代码将数据集随机划分为训练集和验证集，以便在训练和评估机器学习模型时使用。
    
- ****Decision Tree 决策树****
    
    ```python
    min_samples_split_list = [2,10, 30, 50, 100, 200, 300, 700] ## If the number is an integer, then it is the actual quantity of samples,
    max_depth_list = [1,2, 3, 4, 8, 16, 32, 64, None] # None means that there is no depth limit.
    
    accuracy_list_train = []
    accuracy_list_val = []
    for min_samples_split in min_samples_split_list:
        # You can fit the model at the same time you define it, because the fit function returns the fitted estimator.
        model = DecisionTreeClassifier(min_samples_split = min_samples_split,
                                       random_state = RANDOM_STATE).fit(X_train,y_train) 
        predictions_train = model.predict(X_train) ## The predicted values for the train dataset
        predictions_val = model.predict(X_val) ## The predicted values for the test dataset
        accuracy_train = accuracy_score(predictions_train,y_train)
        accuracy_val = accuracy_score(predictions_val,y_val)
        accuracy_list_train.append(accuracy_train)
        accuracy_list_val.append(accuracy_val)
    
    plt.title('Train x Validation metrics')
    plt.xlabel('min_samples_split')
    plt.ylabel('accuracy')
    plt.xticks(ticks = range(len(min_samples_split_list )),labels=min_samples_split_list)
    plt.plot(accuracy_list_train)
    plt.plot(accuracy_list_val)
    plt.legend(['Train','Validation'])
    ```
    
    这段代码在做以下事情：
    
    1. 定义两个参数列表：**`min_samples_split_list`** 和 **`max_depth_list`**。**`min_samples_split_list`** 包含用于决策树中的最小样本分割数量，**`max_depth_list`** 包含用于决策树的最大深度。
    2. 初始化两个空列表：**`accuracy_list_train`** 和 **`accuracy_list_val`**，用于**存储训练集和验证集的准确性分数。**
    3. 对于 **`min_samples_split_list`** 中的每个 **`min_samples_split`** 值，使用该值创建一个 **`DecisionTreeClassifier`**，并使用训练数据对其进行拟合。
    4. 对训练集和验证集进行预测，并计算准确性分数。将这些分数添加到相应的列表中。
    5. 绘制训练集和验证集的准确性分数随 **`min_samples_split`** 变化的折线图。
    
    ![截屏2023-04-08 22.02.13.png](2%204%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E5%86%B3%E7%AD%96%E6%A0%91%2025199d426e8a47c6acd87a999fa82ab4/%25E6%2588%25AA%25E5%25B1%258F2023-04-08_22.02.13.png)
    
    输出图中的两条线表示：
    
    - 蓝线：表示训练集的准确性分数。这显示了不同 **`min_samples_split`** 值下决策树在训练集上的性能。
    - 橙线：表示验证集的准确性分数。这显示了不同 **`min_samples_split`** 值下决策树在验证集上的性能。
    
    通过比较这两条线，您可以了解不同 **`min_samples_split`** 参数值对决策树性能的影响，并选择一个合适的参数值以获得最佳模型性能。
    
    ![截屏2023-04-08 22.05.18.png](2%204%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E5%86%B3%E7%AD%96%E6%A0%91%2025199d426e8a47c6acd87a999fa82ab4/%25E6%2588%25AA%25E5%25B1%258F2023-04-08_22.05.18.png)
    
    最终选择：
    
    - `max_depth = 4`
    - `min_samples_split = 50`
    
    ![截屏2023-04-08 22.10.04.png](2%204%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E5%86%B3%E7%AD%96%E6%A0%91%2025199d426e8a47c6acd87a999fa82ab4/%25E6%2588%25AA%25E5%25B1%258F2023-04-08_22.10.04.png)
    
    - 🔰问题：这是得到了一个决策树吗？ 这个 Model 可用来预测，但为什么又预测了一遍predict(X_train),y_train)？打印了一遍再测试集和训练集上的结果？
        
        这段代码创建了一个决策树模型并用训练集进行拟合。在这个例子中，设置了**`min_samples_split`**为50，**`max_depth`**为3，以及一个固定的随机状态（**`random_state`**）。
        
        接下来，该代码使用训练集（**`X_train`**）和验证集（**`X_val`**）对模型进行预测，并计算预测结果的准确性分数。这里的目的是查看模型在训练集和验证集上的性能表现。
        
        虽然这个模型已经拟合并可以用来进行预测，但是我们需要了解模型在训练集和验证集上的表现，以评估模型是否过拟合或者欠拟合。为此，我们对训练集和验证集再次进行预测并计算准确性分数。然后将训练集和验证集上的准确性分数打印出来，以便我们了解模型的性能。
        
- ****Random Forest****
    
    ![截屏2023-04-09 12.28.45.png](2%204%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E5%86%B3%E7%AD%96%E6%A0%91%2025199d426e8a47c6acd87a999fa82ab4/%25E6%2588%25AA%25E5%25B1%258F2023-04-09_12.28.45.png)
    
    ![截屏2023-04-09 12.28.52.png](2%204%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E5%86%B3%E7%AD%96%E6%A0%91%2025199d426e8a47c6acd87a999fa82ab4/%25E6%2588%25AA%25E5%25B1%258F2023-04-09_12.28.52.png)
    
    ![截屏2023-04-09 12.29.04.png](2%204%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E5%86%B3%E7%AD%96%E6%A0%91%2025199d426e8a47c6acd87a999fa82ab4/%25E6%2588%25AA%25E5%25B1%258F2023-04-09_12.29.04.png)
    
    ![截屏2023-04-09 12.29.50.png](2%204%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E5%86%B3%E7%AD%96%E6%A0%91%2025199d426e8a47c6acd87a999fa82ab4/%25E6%2588%25AA%25E5%25B1%258F2023-04-09_12.29.50.png)
    
    - 🔰问题：这是用前面的几次得到了各自的最佳模型，最后用全部参数算一个新模型出来？
        
        是的，这段代码基于之前分别评估 **`n_estimators`**、**`max_depth`** 和 **`min_samples_split`** 参数得到的最佳值来创建一个新的 **`RandomForestClassifier`** 模型。这里，根据之前的调参结果，使用了100棵树（**`n_estimators=100`**）、最大深度为16（**`max_depth=16`**）和最小样本分割数为10（**`min_samples_split=10`**）。
        
        然后，用训练数据集 **`X_train`** 和 **`y_train`** 拟合这个新的随机森林模型。这个模型将结合之前评估过程中找到的最佳参数，以期望在验证集和测试集上获得更好的性能。
        
- XGBoost
    
    ```python
    n = int(len(X_train)*0.8) ## Let's use 80% to train and 20% to eval
    X_train_fit, X_train_eval, y_train_fit, y_train_eval = X_train[:n], X_train[n:], y_train[:n], y_train[n:]
    xgb_model = XGBClassifier(n_estimators = 500, learning_rate = 0.1,verbosity = 1, random_state = RANDOM_STATE)
    xgb_model.fit(X_train_fit,y_train_fit, eval_set = [(X_train_eval,y_train_eval)], early_stopping_rounds = 10)
    xgb_model.best_iteration
    print(f"Metrics train:\n\tAccuracy score: {accuracy_score(xgb_model.predict(X_train),y_train):.4f}\nMetrics test:\n\tAccuracy score: {accuracy_score(xgb_model.predict(X_val),y_val):.4f}")
    ```
    
    1. 将训练集的80%用于训练，剩余的20%用于评估。这里将训练集拆分为**`X_train_fit`**和**`X_train_eval`**，标签拆分为**`y_train_fit`**和**`y_train_eval`**。
    2. 创建一个XGBoost分类器（**`XGBClassifier`**）模型。设置**`n_estimators`**为500（决策树的数量），**`learning_rate`**为0.1，**`verbosity`**为1（打印训练过程的消息）以及一个固定的随机状态（**`random_state`**）。
    3. 使用**`X_train_fit`**和**`y_train_fit`**对XGBoost模型进行拟合。同时，将**`X_train_eval`**和**`y_train_eval`**作为评估集。设置**`early_stopping_rounds`**为10，表示在连续10轮评估过程中，如果模型性能没有改善，则提前停止训练。
    4. 输出模型的最佳迭代次数（**`xgb_model.best_iteration`**），即模型在训练过程中达到最佳性能的迭代次数。
    5. 使用模型对训练集（**`X_train`**）和验证集（**`X_val`**）进行预测，计算预测结果的准确性分数。然后打印训练集和验证集上的准确性分数，以便我们了解模型的性能。

## **Practice Lab: Decision Trees1 graded assessment left**

![截屏2023-04-08 23.01.08.png](2%204%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E5%86%B3%E7%AD%96%E6%A0%91%2025199d426e8a47c6acd87a999fa82ab4/%25E6%2588%25AA%25E5%25B1%258F2023-04-08_23.01.08.png)

![截屏2023-04-08 23.10.40.png](2%204%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E5%86%B3%E7%AD%96%E6%A0%91%2025199d426e8a47c6acd87a999fa82ab4/%25E6%2588%25AA%25E5%25B1%258F2023-04-08_23.10.40.png)

![截屏2023-04-08 23.12.24.png](2%204%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E5%86%B3%E7%AD%96%E6%A0%91%2025199d426e8a47c6acd87a999fa82ab4/%25E6%2588%25AA%25E5%25B1%258F2023-04-08_23.12.24.png)

![截屏2023-04-08 23.11.11.png](2%204%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E5%86%B3%E7%AD%96%E6%A0%91%2025199d426e8a47c6acd87a999fa82ab4/%25E6%2588%25AA%25E5%25B1%258F2023-04-08_23.11.11.png)