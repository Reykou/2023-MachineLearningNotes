# 1.3 监督学习：分类

復習: Done
日付: 2023年3月19日
最終更新日時: 2023年4月14日 15:32

# 逻辑回归：Classification with logistic regression

## 动机：[motivations](https://www.coursera.org/learn/machine-learning/lecture/aoMt6/motivations)

本周课程的主题是**分类问题，这是一种输出变量y只能在一小部分可能值中取一个值的情况，而不是在无穷范围内的任何数字。**线性回归不适用于分类问题，因此我们将学习逻辑回归算法，这是当今最流行、最广泛使用的学习算法之一。

- 在**分类问题中，我们关注二元分类问题，其中只有两个可能的输出类别。**这些类别通常用0和1表示，0表示负类（或否/假），1表示正类（或是/真）。例如，在垃圾邮件分类问题中，非垃圾邮件（否/假）是负类，垃圾邮件（是/真）是正类。
    
    ![Untitled](1%203%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%88%86%E7%B1%BB%20d9e22694e13d4a64bfb9ae6519b87843/Untitled.png)
    
- **尝试使用线性回归解决分类问题可能会导致问题。**例如，在肿瘤恶性与良性分类问题中，当我们尝试使用线性回归拟合数据时，可能会得到一个不理想的分类边界。**当我们添加一个新的训练样本时，线性回归的最佳拟合线可能会改变，导致分类边界的改变。这表明线性回归不适合处理分类问题。**
    
    ![Untitled](1%203%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%88%86%E7%B1%BB%20d9e22694e13d4a64bfb9ae6519b87843/Untitled%201.png)
    
    ![Untitled](1%203%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%88%86%E7%B1%BB%20d9e22694e13d4a64bfb9ae6519b87843/Untitled%202.png)
    
    - 🔰名词解释：二分类（Binary Classification）/'baɪnəri/
        
        二分类是机器学习中的一种基本任务，它涉及到将输入数据分为两个互斥的类别。这种类型的问题在现实世界中非常常见，例如垃圾邮件过滤（垃圾邮件或正常邮件）、疾病诊断（患病或未患病）等。逻辑回归是一种非常适合处理二分类问题的算法。
        
    - 🔰名词解释：阈值（Threshold）/ˈθrɛʃˌhoʊld/
        
        在逻辑回归中，阈值是一个设定的数值，用于将模型输出的概率值转换为类别标签。通常，阈值设置为0.5，如果逻辑回归模型输出的概率值大于0.5，则将输入样本划分为正类（例如恶性，Malignant），否则将其划分为负类（例如良性，Benign）。通过调整阈值，可以在一定程度上控制模型的精确度和召回率，以满足不同应用场景的需求。
        

为了解决这个问题，我们使用逻辑回归算法。尽管名字中包含“回归”，但逻辑回归实际上用于分类问题，其中输出标签y为0或1。逻辑回归的输出值始终在0和1之间，这有助于避免在处理分类问题时出现的问题。

接下来的课程将详细介绍逻辑回归算法以及如何应用它来解决分类问题。

> **本节一句话总结：**
分类问题：y 为离散值
分类问题-二元分类：输出为0 或 1  
如果用线性回归解决分类问题→添加新样本时，最佳拟合线会改变，不适用。
> 

## 实验室：[Classification](https://www.coursera.org/learn/machine-learning/ungradedLab/0Ew8Q/optional-lab-classification)

![截屏2023-03-19 19.13.17.png](1%203%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%88%86%E7%B1%BB%20d9e22694e13d4a64bfb9ae6519b87843/%25E6%2588%25AA%25E5%25B1%258F2023-03-19_19.13.17.png)

- 🔰疑问：右边这张图，这样的y=0 和y=1 有什么实际意义吗？
    
    在这里，y=1和y=0是目标变量的两个类别。实际上，这是一个二分类问题，例如在逻辑回归或其他分类任务中。y=1和y=0可以代表任何具有两个类别的实际问题。例如：
    
    1. 在垃圾邮件检测任务中，y=1可以表示一封邮件是垃圾邮件，而y=0表示它不是垃圾邮件。
    2. 在金融领域的信用风险评估中，y=1表示贷款申请人可能会违约，而y=0表示贷款申请人不太可能违约。
    3. 在医学诊断中，y=1可能表示病人患有某种疾病，而y=0表示病人未患病。
    
    这些**示例数据集只是为了说明目标变量是如何区分两个类别的**。实际应用中，您可以根据具体问题设置类别标签。
    
- 🔰疑问：怎么样的分布算是好的呢？离的越远，越不交叉越好是吗？
    
    是的，**对于一个分类问题，理想情况下，我们希望数据点在特征空间中按类别清晰地分离。**换句话说，我们希望同一类别的数据点聚集在一起，不同类别的数据点相互分离，以便找到一个边界来区分它们。
    

### 线性回归方法

![截屏2023-03-19 19.17.21.png](1%203%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%88%86%E7%B1%BB%20d9e22694e13d4a64bfb9ae6519b87843/%25E6%2588%25AA%25E5%25B1%258F2023-03-19_19.17.21.png)

## 逻辑回归：[Logistic regression](https://www.coursera.org/learn/machine-learning/lecture/zNxaw/logistic-regression)

**逻辑回归（Logistic Regression）/loʊˈdʒɪstɪk rɪˈɡrɛʃən/** ，它是一个广泛使用的分类算法。它的主要目的是**根据一组输入特征（自变量）来预测一个离散的输出标签（因变量），通常表示为两个类别（0和1）。**这里以判断肿瘤是否恶性为例，讲解了逻辑回归的基本概念和原理。

![Untitled](1%203%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%88%86%E7%B1%BB%20d9e22694e13d4a64bfb9ae6519b87843/Untitled%203.png)

- 逻辑回归与线性回归相比，更适合处理分类问题。在逻辑回归中，我们使用 **Sigmoid 函数（也叫逻辑函数）将线性回归的输出结果映射到 [0,1] 区间。**Sigmoid 函数的定义为 g(z) = 1 / (1 + e^(-z))，其中 e 是自然对数的底，约等于 2.7。
- 逻辑回归的算法可以分为两步：
    - **计算 z = wx + b，其中 w 和 x 是特征权重和输入特征，b 是偏置项。**
    - **将 z 代入 Sigmoid 函数，得到 g(z) 的值，这个值就是逻辑回归模型的输出。**
    
    ![Untitled](1%203%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%88%86%E7%B1%BB%20d9e22694e13d4a64bfb9ae6519b87843/Untitled%204.png)
    
- **逻辑回归的输出可以理解为某个类别（例如恶性肿瘤，y = 1）的概率。**例如，如果一个肿瘤的尺寸 x 对应的逻辑回归输出为 0.7，那么意味着这个肿瘤有 70% 的可能性是恶性的。与此同时，y = 0（良性肿瘤）的概率为 1 - 0.7 = 0.3。
    
    ![Untitled](1%203%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%88%86%E7%B1%BB%20d9e22694e13d4a64bfb9ae6519b87843/Untitled%205.png)
    

逻辑回归算法在许多应用领域都有着广泛的应用，例如广告投放等。接下来的课程内容将继续深入讨论逻辑回归的细节，如决策边界等。

[data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%2730%27%20height=%2730%27/%3e](data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%2730%27%20height=%2730%27/%3e)

> **本节一句话总结：**
逻辑回归：解决分类问题，根据输入预测离散输出。
Sigmoid 函数g(z) ，z=wx+b ，输出结果为 [0,1] 的概率区间。
> 

## 实验室：[Sigmoid function and logistic regression](https://www.coursera.org/learn/machine-learning/ungradedLab/9LMHw/optional-lab-sigmoid-function-and-logistic-regression)

[Coursera | Online Courses & Credentials From Top Educators. Join for Free | Coursera](https://www.coursera.org/learn/machine-learning/ungradedLab/9LMHw/optional-lab-sigmoid-function-and-logistic-regression/lab?path=/notebooks/C1_W3_Lab02_Sigmoid_function_Soln.ipynb)

### ****Sigmoid or Logistic Function****

```python
# Plot z vs sigmoid(z)
fig,ax = plt.subplots(1,1,figsize=(5,3))
ax.plot(z_tmp, y, c="b")

ax.set_title("Sigmoid function")
ax.set_ylabel('sigmoid(z)')
ax.set_xlabel('z')
draw_vthresh(ax,0)
```

![截屏2023-03-19 19.45.02.png](1%203%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%88%86%E7%B1%BB%20d9e22694e13d4a64bfb9ae6519b87843/%25E6%2588%25AA%25E5%25B1%258F2023-03-19_19.45.02.png)

- 📌代码解析：
    
    在这个代码片段中，**`plt.subplots(1, 1, figsize=(5, 3))`** 是用来创建一个画布（Figure）和一个或多个子图（Axes）的。**`subplots`**函数的前两个参数表示子图的行数和列数。在这个例子中，它们分别是 1 和 1，意味着在画布上只有一个子图。**`figsize`** 参数设置画布的大小，这里设置为宽度 5 英寸，高度 3 英寸。
    
    关于图中的两个箭头，它们表示 z=0 处的阈值。当 z 的值大于 0 时，sigmoid 函数的值大于 0.5；当 z 的值小于 0 时，sigmoid 函数的值小于 0.5。这两个箭头分别指向相反方向，用于强调这个阈值。当我们用 sigmoid 函数作为激活函数时，这个阈值可以帮助我们将输出值转换为类别标签（例如，大于 0.5 的值被分类为类别 1，小于 0.5 的值被分类为类别 0）。
    

### ****Logistic Regression****

![截屏2023-03-19 19.47.20.png](1%203%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%88%86%E7%B1%BB%20d9e22694e13d4a64bfb9ae6519b87843/%25E6%2588%25AA%25E5%25B1%258F2023-03-19_19.47.20.png)

- 🔰问题：使用梯度下降法进行线性回归，并在图上绘制拟合的线。 ---为什么看到这条线是逐步逼近最终的动态线的呢？
    
    在**`calc_linear`**方法中，梯度下降法用于逐步更新权重和截距以最小化损失函数。为了使这个过程可视化，方法中有一个循环，它在每次迭代后更新并绘制新的拟合线。这样做的目的是向用户展示模型是如何逐渐逼近最优解的。
    
    这里的循环使用了一个迭代次数序列（**`[1,1,1,1,1,2,4,8,16,32,64,128,256]`**
    ）。在每次迭代中，梯度下降法会被应用，权重和截距会根据学习率（0.01）和迭代次数更新。然后，原始的拟合线和图例会被移除，并用新的拟合线和图例替换。这个过程使用了**`time.sleep(0.3)`**
    来暂停0.3秒，以便用户能看到线条是如何逐步更新的。在每次迭代后，**`self.fig.canvas.draw()`**
    会调用，以便在图上显示新的拟合线。
    

## 决策边界：[Decission boundary](https://www.coursera.org/learn/machine-learning/lecture/qrxwU/decision-boundary)

在这段文字中，关键知识点包括逻辑回归（Logistic Regression）、决策边界（Decision Boundary）和多项式特征（Polynomial Features）。

- 逻辑回归（Logistic Regression）/dɪˈsɪʒən ˈbaʊndəri/：逻辑回归是一种用于分类任务的机器学习算法，尤其适用于二分类问题。逻辑回归通过使用 Sigmoid 函数将线性模型的输出转换为概率，从而进行分类。
    
    ![Untitled](1%203%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%88%86%E7%B1%BB%20d9e22694e13d4a64bfb9ae6519b87843/Untitled%206.png)
    
    - 📌推导过程：
        
        我们要找到一个阈值，使得当 **g(z) ≥ 0.5 时，我们将其分类为类别 1，否则分类为类别 0。**现在，让我们推导一下：
        
        1. g(z) ≥ 0.5
        2. 由于我们使用 sigmoid 函数，它在 z=0 时等于 0.5，所以当 z≥0 时，g(z)≥0.5。
        3. 现在，我们用 f_wb(x) 替换 z，即 f_wb(x) = w^T * x + b。
        4. 当 f_wb(x)≥0 时，g(f_wb(x))≥0.5。
        
        这里的推导说明，**我们可以通过检查 f_wb(x) 是否大于等于 0 来确定数据点属于哪个类别。**如果 f_wb(x)≥0，则数据点属于类别 1，否则属于类别 0。**这个条件定义了我们的决策边界**。
        
- **决策边界（Decision Boundary）**：决策边界是一个超平面，用于区分逻辑回归模型的不同类别。在二维空间中，**决策边界可以是一条线，而在更高维度的空间中，它可以是一个超平面。**通过这个边界，模型可以对输入数据进行分类。**在逻辑回归模型中，决策边界是由参数 w 和 b 确定的**。
    
    ![Untitled](1%203%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%88%86%E7%B1%BB%20d9e22694e13d4a64bfb9ae6519b87843/Untitled%207.png)
    
    - 非线性决策边界：Non-linear decision boundaries /dɪˈsɪʒ.ən/ /ˈbaʊn.dər.iz/ 意味着这个**边界不是一条直线**（对于二维数据）或者平面（对于三维数据）等线性结构，而**是一个更复杂的形状**。
    
    ![Untitled](1%203%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%88%86%E7%B1%BB%20d9e22694e13d4a64bfb9ae6519b87843/Untitled%208.png)
    
    ![Untitled](1%203%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%88%86%E7%B1%BB%20d9e22694e13d4a64bfb9ae6519b87843/Untitled%209.png)
    
- **多项式特征（Polynomial Features）**：在逻辑回归模型中，**可以通过添加多项式特征（如平方、立方等）来获得更复杂的决策边界。**这使得模型能够拟合复杂的数据分布。然而，需要注意的是，**如果没有添加更高阶的多项式特征，逻辑回归的决策边界将始终是线性的。**

通过了解这些关键知识点，您应该能更好地理解逻辑回归模型如何进行分类以及如何调整模型以适应不同复杂度的数据。在接下来的课程中，您将学习逻辑回归的成本函数以及如何应用梯度下降方法对其进行优化。

- 🔰名词解释：高阶多项式方程在机器学习和统计中通常用来捕捉数据中的非线性关系。一个高阶多项式方程可以表示为：
    
    其中，$y$ 是输出变量，$x$ 是输入特征，$a_i$ 是多项式的系数，$n$ 是多项式的阶数。
    
    y = a_0 + a_1x + a_2x^2 + a_3x^3 + ... + a_nx^n
    
    在一个直观的层面上，高阶多项式方程可以帮助我们建立更复杂的模型，以便更好地适应数据。随着多项式阶数的增加，方程所代表的函数形状会变得更加复杂和灵活。这可以使模型能够捕捉到数据中的更复杂的模式和关系。
    
    绘制高阶多项式方程的图像通常会显示出一些弯曲的曲线，而不是简单的直线。例如，二次方程（n=2）将形成一个抛物线形状，三次方程（n=3）可能形成一个类似于“S”形的曲线。随着阶数的增加，曲线可能会变得更加复杂。
    
- 🔰为什么阈值 0.2 更好呢？
    
    在肿瘤检测算法的情况下，我们通常更关注敏感性，即正确识别出患者中存在肿瘤的能力。将阈值设置得较低（如0.2）可以使算法对检测到潜在肿瘤更为敏感。这意味着可能会有更多的阳性结果（包括真阳性和假阳性），但这可以确保更少的肿瘤被漏掉。
    
    然而，较低的阈值也可能导致更高的假阳性率，这意味着一些健康的患者可能被错误地识别为有肿瘤。但在这种情况下，**我们更愿意让专家检查更多的患者，以确保所有潜在的肿瘤都能被识别出来。这是因为漏诊肿瘤的代价可能远高于一些不必要的检查。**所以在这个案例中，选择一个较低的阈值（如0.2）可能更适合。
    
    ![截屏2023-03-19 20.25.27.png](1%203%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%88%86%E7%B1%BB%20d9e22694e13d4a64bfb9ae6519b87843/%25E6%2588%25AA%25E5%25B1%258F2023-03-19_20.25.27.png)
    

> **本节一句话总结：**
逻辑回归的决策边界：g(z) ≥ 0.5 时，将其分类为类别 1，否则分类为类别 0。
决策边界：可以是直线、也可以是曲线或超平面（通过添加多项式特征，获得平方、立方体、不规则形状等）
> 

---

# 逻辑回归分类：Classification with logistic regression

## 逻辑回归的成本函数：[Cost function for logistic regression](https://www.coursera.org/learn/machine-learning/lecture/0hpr8/cost-function-for-logistic-regression)

在这段视频中，讲解了**为什么平方误差成本函数（squared error cost function）不适用于逻辑回归（logistic regression），并引入了一个新的适用于逻辑回归的成本函数。**以下是关键知识点的总结：

- **平方误差成本函数不适用于逻辑回归**：**对于逻辑回归，如果使用平方误差成本函数，将导致非凸（non-convex）的成本函数。**这意味着在尝试使用梯度下降时，可能会陷入许多局部最小值。
    - 🔰名词解释：凸函数是什么？
        
        凸函数是一种在整个定义域内局部最小值也是全局最小值的函数。换句话说，**凸函数的任何两点之间的线段都位于函数图像的上方。这意味着凸函数没有局部最小值，只有一个全局最小值。**
        
    
    ![Untitled](1%203%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%88%86%E7%B1%BB%20d9e22694e13d4a64bfb9ae6519b87843/Untitled%2010.png)
    
    ![Untitled](1%203%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%88%86%E7%B1%BB%20d9e22694e13d4a64bfb9ae6519b87843/Untitled%2011.png)
    
- 为了构建一个新的适用于逻辑回归的成本函数，需要使用不同的损失函数。损失函数衡量了单个训练样本上的预测性能，整个成本函数是所有训练样本上损失函数的平均值。
- **逻辑回归损失函数定义：如果标签 y 等于 1，则损失为 -log(f(x))。如果标签 y 等于 0，则损失为 -log(1 - f(x))。**这种损失函数对于正确预测的奖励和错误预测的惩罚具有合理的平衡。
    
    ![Untitled](1%203%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%88%86%E7%B1%BB%20d9e22694e13d4a64bfb9ae6519b87843/Untitled%2012.png)
    
    - 🔰解释：👆🏻图中绘制的是公式中蓝色部分、即上面的一行。**当 y[i]=1 时，**横轴是-log(f)的f、即f_wb，y 轴是L(f_wb)。所以，**当 f_wb接近1 时，损失函数越接近 0。**
    - 🔰解释：👇🏻图中绘制的是公式中蓝色部分、即下面一行。**当 y[i]=0时， f_wb接近0 ，则损失函数L越接近 0。**
    
    ![Untitled](1%203%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%88%86%E7%B1%BB%20d9e22694e13d4a64bfb9ae6519b87843/Untitled%2013.png)
    
- 使用这个新的损失函数，整个成本函数将是凸的，因此可以使用梯度下降来可靠地找到全局最小值。
    
    ![截屏2023-04-14 11.45.31.png](1%203%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%88%86%E7%B1%BB%20d9e22694e13d4a64bfb9ae6519b87843/%25E6%2588%25AA%25E5%25B1%258F2023-04-14_11.45.31.png)
    
- 在接下来的视频中，将继续深入了解这个新的损失函数，并利用它定义整个训练集的成本函数。然后，将简化成本函数的表示，以便运行梯度下降以找到逻辑回归的良好参数。

总之，这段视频解释了为什么需要为逻辑回归选择一个不同于线性回归的成本函数，并引入了一个新的适用于逻辑回归的成本函数。

- 测试：为什么平方误差成本没有用于逻辑回归？
    
    如果使用均方误差进行逻辑回归，成本函数是“非凸”，因此梯度下降更难找到参数w和b的最佳值。
    
    ![截屏2023-03-19 21.48.30.png](1%203%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%88%86%E7%B1%BB%20d9e22694e13d4a64bfb9ae6519b87843/%25E6%2588%25AA%25E5%25B1%258F2023-03-19_21.48.30.png)
    
    - 🔰问题：是因为这样绘制的J(w,b)曲线是不会则的，有高低起伏，很难找到最小值是吗？
        
        是的，您理解正确。如果在逻辑回归中使用均方误差（mean squared error, MSE）作为损失函数，得到的损失函数将是非凸的。非凸损失函数在优化过程中可能存在多个局部最小值。这使得梯度下降等优化算法很难找到一个全局最优解，因为算法可能会在局部最小值处停止，而不是全局最小值。
        
        为了避免这个问题，在逻辑回归中我们使用交叉熵损失（Cross-Entropy Loss），这是一个凸损失函数。使用交叉熵损失时，我们可以保证在优化过程中找到全局最优解，从而得到最佳的模型参数。
        

> **本节一句话总结：**
平方误差成本函数不适合逻辑回归。（非凸函数）
逻辑回归损失函数定义：如果标签 y 等于 1，则损失为 -log(f(x))。如果标签 y 等于 0，则损失为 -log(1 - f(x))。
> 

## 实验室：[Logistic loss](https://www.coursera.org/learn/machine-learning/ungradedLab/pOtDs/optional-lab-logistic-loss)

### 线性回归中的均方误差（mean squared error, MSE）损失函数

这个函数绘制了一个二次函数 (quadratic function) 的 3D 曲面，代表线性回归中的均方误差（mean squared error, MSE）损失函数。在这个例子中，我们考虑了两个参数 w 和 b，构造了一个关于 w 和 b 的二次损失函数。这个损失函数的形状类似于一个碗，因此被称为 "soup bowl"（汤碗）。

这个图形展示了在线性回归中使用均方误差损失函数时，参数 w 和 b 的损失函数曲面。这个碗状的曲面是凸的，因此我们可以通过梯度下降等优化算法找到全局最小值，从而找到最佳的模型参数。

```python
def soup_bowl():
    """ creates 3D quadratic error surface """
    #Create figure and plot with a 3D projection
    # 首先，创建一个 3D 图形，并设置一些图形属性。
    fig = plt.figure(figsize=(4,4))
    fig.canvas.toolbar_visible = False
    fig.canvas.header_visible = False
    fig.canvas.footer_visible = False

    #Plot configuration
    # 在图形 fig 中添加一个子图（subplot），编号为 1x1 的网格的第 1 个。
    # 设置子图的投影为 '3d'，这样我们可以在这个子图上绘制 3D 图形。
    ax = fig.add_subplot(111, projection='3d')
    # ax.xaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))：设置 x 轴的面板颜色。
    # 颜色值是一个四元组，表示 RGBA（红、绿、蓝、透明度）颜色。
    # 这里设置为 (1.0, 1.0, 1.0, 0.0)，表示完全透明的白色，使 x 轴面板不可见。
    ax.xaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))
    ax.yaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))
    ax.zaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))
    # 设置 z 轴标签是否旋转。在这里，我们禁用了 z 轴标签的旋转，使其保持水平状态。
    ax.zaxis.set_rotate_label(False)
    # 设置 3D 图形的初始视角。这个函数接受两个参数，分别是俯仰角（elevation）和方位角（azimuth）。
    # 在这里，我们设置俯仰角为 15 度，方位角为 -120 度，使图形从一个易于观察的角度展示。
    ax.view_init(15, -120)

    #Useful linearspaces to give values to the parameters w and b
    # 定义参数 w 和 b 的取值范围，这里 w 和 b 的范围都是从 -20 到 20，共有 100 个取值。
    w = np.linspace(-20, 20, 100)
    b = np.linspace(-20, 20, 100)

    #Get the z value for a bowl-shaped cost function
    # 计算损失函数的值。在这个例子中，损失函数是二次函数，形式为 z = x^2 + y^2，
    # 其中 x 代表 w，y 代表 b。
    # 通过嵌套循环，我们为 w 和 b 的每个组合计算损失函数值，然后将结果存储在矩阵 z 中。
    z=np.zeros((len(w), len(b)))
    j=0
    for x in w:
        i=0
        for y in b:
            z[i,j] = x**2 + y**2
            i+=1
        j+=1

    #Meshgrid used for plotting 3D functions
    #使用 np.meshgrid 函数创建网格，用于绘制 3D 曲面。
    W, B = np.meshgrid(w, b)

    #Create the 3D surface plot of the bowl-shaped cost function
    # 最后，使用 ax.plot_surface 和 ax.plot_wireframe 函数绘制损失函数的 3D 曲面。
    # 损失函数的形状类似于一个碗，因此被称为 "soup bowl"（汤碗）。
    ax.plot_surface(W, B, z, cmap = "Spectral_r", alpha=0.7, antialiased=False)
    ax.plot_wireframe(W, B, z, color='k', alpha=0.1)
    ax.set_xlabel("$w$")
    ax.set_ylabel("$b$")
    ax.set_zlabel("Cost", rotation=90)
    ax.set_title("Squared Error Cost used in Linear Regression")

    plt.show()
```

![截屏2023-03-19 22.57.11.png](1%203%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%88%86%E7%B1%BB%20d9e22694e13d4a64bfb9ae6519b87843/%25E6%2588%25AA%25E5%25B1%258F2023-03-19_22.57.11.png)

- 🔰问题：为什么这个碗上面是偏红色，下面偏蓝色呢
    
    这个碗的颜色变化是由 **`cmap`** 参数控制的，它决定了绘制 3D 曲面时使用的颜色映射。在这个例子中，我们使用了 **`"Spectral_r"`** 颜色映射。
    
    "Spectral_r" 是一种预定义的颜色映射，表示 "Spectral" 颜色映射的逆序版本。"Spectral" 颜色映射在数值较低时为红色，数值较高时为蓝色；而 "Spectral_r" 则是相反的顺序，数值较低时为蓝色，数值较高时为红色。在这个例子中，使用 "Spectral_r" 颜色映射有助于突出损失函数值较低的区域。
    
    ```python
    #颜色是根据第三个参数，即 **z**值，来映射的。
    ax.plot_surface(W, B, z, cmap = "Spectral_r", alpha=0.7, antialiased=False)
    ```
    
    颜色映射（colormap）是一种将数值数据映射到颜色的方法，通常用于可视化中以表达数据的大小、密度等属性。在这个例子中，颜色映射是根据损失函数的值（z 坐标）来设置的。损失函数值较低（碗底部）时，颜色映射为蓝色；损失函数值较高（碗的上部）时，颜色映射为红色。这样的颜色映射有助于我们直观地理解损失函数值的变化情况。
    

### 平方误差损失函数

这个图展示的是对于一组数据（X，y），使用平方误差作为损失函数的逻辑回归模型的损失函数（cost function）随参数w和b的变化而变化的情况。

这里，x轴代表权重w，y轴代表偏置项b，z轴表示损失函数的值。图像中的颜色表示损失函数的高度（值），颜色越深（如蓝色）表示损失函数值越低，颜色越浅（如红色）表示损失函数值越高。这个图像展示了损失函数在参数空间（w, b）中的形状，它有很多局部最小值，因此优化非常困难。

![截屏2023-03-19 23.12.08.png](1%203%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%88%86%E7%B1%BB%20d9e22694e13d4a64bfb9ae6519b87843/%25E6%2588%25AA%25E5%25B1%258F2023-03-19_23.12.08.png)

- 🔰问题：这个图片好漂亮耶，是决策边界的三d版对吗？
    
    不，这个图并不是决策边界。
    

### 逻辑回归的损失函数 和 损失函数

![截屏2023-03-19 23.23.40.png](1%203%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%88%86%E7%B1%BB%20d9e22694e13d4a64bfb9ae6519b87843/%25E6%2588%25AA%25E5%25B1%258F2023-03-19_23.23.40.png)

- 🔰 问题：为什么要取对数呢？他们都是逻辑回归中损失函数的一种表现方式，只是对数形式更加方便后续使用，这么理解对吗？
    
    是的，您的理解是正确的。**对数损失函数和逻辑损失函数都是逻辑回归中损失函数的表现形式，但使用对数损失函数具有更多的优点。**对数损失函数能够直接针对概率进行优化，同时在数学上具有更好的性质，如易于计算梯度和二阶导数。这些特点使得对数损失函数在逻辑回归问题中更加适用和方便。
    

## 逻辑回归的简化成本函数：[Simplified Cost Function for Logistic Regression](https://www.coursera.org/learn/machine-learning/lecture/Zjj2j/simplified-cost-function-for-logistic-regression)

在这个视频中，你学习了简化的损失函数和成本函数，这使得在实现梯度下降算法时更简单。以下是这个视频的关键知识点：

- 简化的损失函数：通过观察二元分类问题（y 只能是 0 或 1）中的损失函数，我们可以将其简化为一个表达式：L = -y * log(f) - (1 - y) * log(1 - f)。这个简化的损失函数在 y=0 或 y=1 时都与原始损失函数相等。
    
    ![Untitled](1%203%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%88%86%E7%B1%BB%20d9e22694e13d4a64bfb9ae6519b87843/Untitled%2014.png)
    
- 使用简化的损失函数，我们可以写出逻辑回归的成本函数。**成本函数 J 是整个训练集（m 个样本）的平均损失：J = (1/m) * Σ[-y * log(f) - (1 - y) * log(1 - f)]。**这个成本函数被广泛应用于训练逻辑回归模型。
    
    ![Untitled](1%203%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%88%86%E7%B1%BB%20d9e22694e13d4a64bfb9ae6519b87843/Untitled%2015.png)
    
    ![截屏2023-03-20 21.13.05.png](1%203%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%88%86%E7%B1%BB%20d9e22694e13d4a64bfb9ae6519b87843/%25E6%2588%25AA%25E5%25B1%258F2023-03-20_21.13.05.png)
    
- 为什么选择这个特定的成本函数？这个成本函数是基于统计学中的极大似然估计法推导出的。尽管我们在这个课程中不会详细讨论极大似然估计法，但这是这个成本函数背后的深层次原理和理由。此外，这个成本函数具有凸性，这是一个很好的性质。

接下来，通过学习梯度下降算法应用于逻辑回归，你将能够使用简化的成本函数来找到逻辑回归模型的最佳参数。

> **本节一句话总结：**
逻辑回归成本函数：J = (1/m) * Σ[-y * log(f) - (1 - y) * log(1 - f)]、整个训练集的平均损失。
> 

## 实验室: [Cost function for logistic regression](https://www.coursera.org/learn/machine-learning/ungradedLab/m9laL/optional-lab-cost-function-for-logistic-regression)

[Coursera | Online Courses & Credentials From Top Educators. Join for Free | Coursera](https://www.coursera.org/learn/machine-learning/ungradedLab/m9laL/optional-lab-cost-function-for-logistic-regression/lab?path=/notebooks/C1_W3_Lab05_Cost_Function_Soln.ipynb)

### 逻辑分类的损失函数

![截屏2023-03-20 21.15.34.png](1%203%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%88%86%E7%B1%BB%20d9e22694e13d4a64bfb9ae6519b87843/%25E6%2588%25AA%25E5%25B1%258F2023-03-20_21.15.34.png)

### 展示不同的决策边界对输出的影响

![截屏2023-03-20 21.41.15.png](1%203%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%88%86%E7%B1%BB%20d9e22694e13d4a64bfb9ae6519b87843/%25E6%2588%25AA%25E5%25B1%258F2023-03-20_21.41.15.png)

- 🔰问题：x1，x1_other 这 2 条线和图中的点和叉叉有什么关系呢？哪里有计算什么吗？
    
    这里，我们选择了两个简单的线性方程（$x_1 = 3 - x_0$ 和 $x_1 = 4 - x_0$）作为决策边界。它们没有通过任何优化或计算与数据点的关系。这个示例仅用于演示如何绘制决策边界以及它们如何与数据点相关。
    
    简单的线性的完整形式是y=wx+b,所以对应的是b=3,w=-1,和b=4,x=-1
    

---

# 逻辑回归的成本函数：Cost function for logistic regression

## 梯度下降的实现：[Gradient Descent Implementation](https://www.coursera.org/learn/machine-learning/lecture/Ha1RP/gradient-descent-implementation)

/ˈɡreɪdiənt dɪˈsent ɪmpləmɛnˈteɪʃən/

在这个视频中，你学习了如何使用梯度下降算法来拟合逻辑回归模型的参数。以下是视频的关键知识点：

- 为了拟合逻辑回归模型的参数 w 和 b，我们需要最小化成本函数 J(w, b)。为了实现这一目标，我们将使用梯度下降算法。
    
    ![Untitled](1%203%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%88%86%E7%B1%BB%20d9e22694e13d4a64bfb9ae6519b87843/Untitled%2016.png)
    
- 梯度下降算法通过不断更新参数 w 和 b，以最小化成本函数 J。更新规则如下：
    - w_j = w_j - α * ∂J/∂w_j
    - b = b - α * ∂J/∂b
    其中，α 是学习率，∂J/∂w_j 和 ∂J/∂b 是成本函数 J 关于参数 w_j 和 b 的偏导数。
- 成本函数 J 关于参数 w_j 和 b 的偏导数分别为：
    - **∂J/∂w_j = (1/m) * Σ(f(x_i) - y_i) * x_i_j**
    - **∂J/∂b = (1/m) * Σ(f(x_i) - y_i)**
    其中，m 是训练样本的数量，x_i_j 是第 i 个训练样本的第 j 个特征。
    
    ![Untitled](1%203%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%88%86%E7%B1%BB%20d9e22694e13d4a64bfb9ae6519b87843/Untitled%2017.png)
    
- 尽管逻辑回归和线性回归的梯度下降算法在表面上看起来相似，但它们实际上是两种非常不同的算法，因为预测函数 f(x) 的定义不同。**在线性回归中，f(x) = w^T x + b；而在逻辑回归中，f(x) = sigmoid(w^T x + b)。**
    
    ![Untitled](1%203%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%88%86%E7%B1%BB%20d9e22694e13d4a64bfb9ae6519b87843/Untitled%2018.png)
    
- 你**可以像监控线性回归的梯度下降收敛情况一样，监控逻辑回归的梯度下降算法。**同样，也可以使用向量化实现加速梯度下降算法。
- **特征缩放**（即将所有特征缩放到相似的值范围，如 -1 到 1 之间）可以加速线性回归和逻辑回归的梯度下降算法。
- scikit-learn 是一个流行的机器学习库，可以用来训练逻辑回归模型。许多机器学习从业者在工作中经常使用 scikit-learn。

通过学习这些知识点，你现在应该知道如何实现逻辑回归了。逻辑回归是一个非常强大且广泛使用的学习算法，现在你已经掌握了如何自己实现它。恭喜！

[data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%2730%27%20height=%2730%27/%3e](data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%2730%27%20height=%2730%27/%3e)

> **本节一句话总结：**
逻辑回归的梯度下降算法（与线性回归很相似）：在线性回归中，f(x) = w^T x + b；而在逻辑回归中，f(x) = sigmoid(w^T x + b)。
> 

## Optional lab: [Gradient descent for logistic regression](https://www.coursera.org/learn/machine-learning/ungradedLab/KtxCL/optional-lab-gradient-descent-for-logistic-regression)

![截屏2023-03-20 22.07.56.png](1%203%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%88%86%E7%B1%BB%20d9e22694e13d4a64bfb9ae6519b87843/%25E6%2588%25AA%25E5%25B1%258F2023-03-20_22.07.56.png)

- 计算dj_db,dj_dw
    
    ![截屏2023-03-20 22.07.29.png](1%203%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%88%86%E7%B1%BB%20d9e22694e13d4a64bfb9ae6519b87843/%25E6%2588%25AA%25E5%25B1%258F2023-03-20_22.07.29.png)
    
- 通过梯度下降计算 w,b
    
    ![截屏2023-03-20 22.11.32.png](1%203%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%88%86%E7%B1%BB%20d9e22694e13d4a64bfb9ae6519b87843/%25E6%2588%25AA%25E5%25B1%258F2023-03-20_22.11.32.png)
    
    ![截屏2023-03-20 22.21.17.png](1%203%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%88%86%E7%B1%BB%20d9e22694e13d4a64bfb9ae6519b87843/%25E6%2588%25AA%25E5%25B1%258F2023-03-20_22.21.17.png)
    
    ![截屏2023-03-20 22.32.16.png](1%203%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%88%86%E7%B1%BB%20d9e22694e13d4a64bfb9ae6519b87843/%25E6%2588%25AA%25E5%25B1%258F2023-03-20_22.32.16.png)
    
- 🔰问题：在使用梯度下降寻找逻辑回归最佳wb 时，初始 wb 除了影响查找速度，对最终结果又影响吗？为什么？
    
    在逻辑回归中，损失函数是凸函数，这意味着存在唯一的全局最小值。因此，在理论上，不管我们从何处开始梯度下降（即初始 w 和 b 的值），最终都会收敛到相同的最优解。
    
    然而，实际上初始值确实会影响梯度下降的收敛速度。如果选择了较差的初始值，可能需要更多迭代次数才能达到所需的精度。此外，在实际问题中，可能会遇到数值问题，如梯度消失或梯度爆炸，这些问题可能会导致梯度下降算法在某些初始值下表现不佳。
    
    因此，在实践中，通常选择接近零的随机小值作为初始 w 和 b，以确保梯度下降从一个相对合理的起点开始。这样可以加快收敛速度，同时避免梯度下降陷入慢速收敛或者数值问题。
    
- 🔰问题：为什么选择不同的初始 wb，log(cost)-iteration 这张图有时是弧线，有时是类似直角的折线，这是为什么呢？虽然最终在逻辑回归中，损失函数是凸函数，这意味着存在唯一的全局最小值。那这张图为什么会不一样，这张图表达了什么意思呢?
    
    不同的初始 w 和 b 导致 log(cost)-iteration 图形有不同的形状，这是因为损失函数下降过程和收敛速度受初始参数值的影响。这些图形揭示了梯度下降算法在寻找最优解过程中的一些特点。
    
    1. 弧线形状：这通常表明从初始参数值开始，梯度下降算法沿着损失函数的梯度方向逐步收敛。在每次迭代过程中，算法都朝着梯度方向更新参数，逐渐逼近最优解。
    2. 类似直角的折线形状：这表明梯度下降在某些迭代过程中可能突然发生较大的变化。这种情况可能出现在学习率较高或损失函数曲面较陡峭的地方。此时，梯度下降算法可能在最优解附近震荡，需要更多的迭代次数来收敛。
    
    这些不同的图形揭示了损失函数在参数空间中的下降路径和收敛速度。尽管逻辑回归的损失函数是凸函数，存在唯一的全局最小值，但梯度下降算法在实际操作过程中可能会受到学习率、初始参数选择等因素的影响。因此，**这些图形有助于我们了解梯度下降算法在不同条件下的性能，从而进行相应的调整，以提高算法的收敛速度和稳定性。**
    
    例如，如果观察到类似直角的折线形状，我们可能需要尝试降低学习率，以避免在最优解附近震荡。如果收敛速度过慢，我们可以尝试增加学习率或调整初始参数值，以便更快地接近最优解。
    

## Optional lab: [Logistic regression with scikit-learn](https://www.coursera.org/learn/machine-learning/ungradedLab/F3ZpI/optional-lab-logistic-regression-with-scikit-learn)

![截屏2023-03-20 22.41.13.png](1%203%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%88%86%E7%B1%BB%20d9e22694e13d4a64bfb9ae6519b87843/%25E6%2588%25AA%25E5%25B1%258F2023-03-20_22.41.13.png)

- 🔰问题：这是在做什么lr_model.fit(X, y)已经帮通过梯度下降算法计算好了wb吗？已经有结果了吗？
    
    是的。 **`lr_model.fit(X, y)`**：这一步是在用梯度下降算法（或其他优化算法）训练逻辑回归模型。在这个过程中，算法会根据训练数据集 **`X`** 和标签 **`y`** 来计算最佳的参数（权重 **`w`** 和偏置 **`b`**），以使得损失函数值最小。执行完 **`fit()`** 函数后，模型就已经训练好了，参数 **`w`** 和 **`b`** 已经被计算出来。
    
- 🔰问题：y_pred = lr_model.predict(X) 是用已经求解过的 wb,在推断y?
    
    是的。 **`y_pred = lr_model.predict(X)`**：这一步是使用训练好的逻辑回归模型（已经求解过的参数 **`w`** 和 **`b`**）对训练数据集 **`X`** 进行预测。**`predict()`** 函数会根据训练好的模型计算预测的 **`y`** 值（**`y_pred`**），这些预测值可以用来评估模型在训练数据集上的表现。在这个例子中，由于我们使用的是训练数据集进行预测，因此可以看到训练集上的准确率。在实际应用中，我们通常还会使用一个独立的测试数据集来评估模型的泛化能力。
    

---

# 过拟合：**The problem of overfitting**

## 过拟合的问题：[The problem of overfitting](https://www.coursera.org/learn/machine-learning/lecture/erGPe/the-problem-of-overfitting)

在这个视频中，主要讨论了过拟合和欠拟合的概念。以下是关键知识点：

- **欠拟合（Underfitting）**：当一个学习算法不能很好地捕捉到训练数据中的模式，导致模型在训练数据上的表现也不好时，我们称其为欠拟合。**欠拟合的模型在训练数据和测试数据上的误差都可能很大。欠**拟合的另一个术语是**高偏差（High Bias）**。
- **过拟合（Overfitting）**：当一个学习算法在训练数据上表现得过于优秀，以至于它捕捉到了训练数据中的噪声，而不能很好地泛化到新的数据时，我们称其为过拟合。**过拟合的模型在训练数据上的误差可能非常小，但在测试数据上的误差可能很大**。过拟合的另一个术语是**高方差（High Variance）**。
    
    ![Untitled](1%203%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%88%86%E7%B1%BB%20d9e22694e13d4a64bfb9ae6519b87843/Untitled%2019.png)
    
- 过拟合和欠拟合的示例：视频中以线性回归预测房价和逻辑回归预测肿瘤恶性与良性为例，说明了过拟合和欠拟合的现象。
    
    ![Untitled](1%203%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%88%86%E7%B1%BB%20d9e22694e13d4a64bfb9ae6519b87843/Untitled%2020.png)
    
- 机器学习的目标：在实际应用中，我们希望找到一个既不过拟合也不欠拟合的模型，即模型具有较低的偏差和较低的方差，能够很好地泛化到新的数据。

在接下来的视频中，作者将介绍一种称为正则化（Regularization）的方法，以解决过拟合问题，从而使学习算法表现得更好。

> **本节一句话总结：**
欠拟合（Underfitting）、high Bias：训练集和测试集表现都不好
过拟合(overfit)、High variance：训练集表现好，测试集表现不好
> 

## 解决过度拟合：[Addressing overfitting](https://www.coursera.org/learn/machine-learning/lecture/HvDkF/addressing-overfitting)

在这节课程中，关键知识点包括：

- 过拟合（Overfitting）：当模型过于复杂，过度拟合训练数据，导致在新数据上的表现不佳时，称之为过拟合。过拟合的模型捕获了训练数据中的噪音而不仅仅是潜在的数据模式。
- 解决过拟合的三种方法：
    - **收集更多数据：**增加训练数据可以帮助模型学习更普遍的模式，从而降低过拟合的风险。
        
        ![Untitled](1%203%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%88%86%E7%B1%BB%20d9e22694e13d4a64bfb9ae6519b87843/Untitled%2021.png)
        
    - **减少特征数量**（特征选择）：通过选择和使用特征子集，可以**降低模型复杂度**并减少过拟合。这可以通过直观地选择最相关的特征或使用算法自动选择特征来实现。在课程二中，你将学习更多关于特征选择的知识。
        
        ![Untitled](1%203%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%88%86%E7%B1%BB%20d9e22694e13d4a64bfb9ae6519b87843/Untitled%2022.png)
        
    - **正则化（Regularization）**：正则化是一种减小模型参数值的方法，从而防止特征对模型产生过大的影响，这有时会导致过拟合。通过限制模型参数的大小，可以在保留所有特征的同时减小过拟合的风险。
        
        ![Untitled](1%203%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%88%86%E7%B1%BB%20d9e22694e13d4a64bfb9ae6519b87843/Untitled%2023.png)
        
    
    ![Untitled](1%203%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%88%86%E7%B1%BB%20d9e22694e13d4a64bfb9ae6519b87843/Untitled%2024.png)
    

接下来的课程将更详细地介绍正则化方法。在此之前，可以尝试实验室练习以加深对过拟合以及解决过拟合问题的方法的理解。

> **本节一句话总结：**
解决过拟合：增加训练数据、减少特征、使用正则化
> 

## 实验室: [Overfitting](https://www.coursera.org/learn/machine-learning/ungradedLab/3nraU/optional-lab-overfitting)

![截屏2023-03-20 23.32.27.png](1%203%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%88%86%E7%B1%BB%20d9e22694e13d4a64bfb9ae6519b87843/%25E6%2588%25AA%25E5%25B1%258F2023-03-20_23.32.27.png)

![截屏2023-03-20 23.32.04.png](1%203%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%88%86%E7%B1%BB%20d9e22694e13d4a64bfb9ae6519b87843/%25E6%2588%25AA%25E5%25B1%258F2023-03-20_23.32.04.png)

![截屏2023-03-20 23.32.45.png](1%203%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%88%86%E7%B1%BB%20d9e22694e13d4a64bfb9ae6519b87843/%25E6%2588%25AA%25E5%25B1%258F2023-03-20_23.32.45.png)

![截屏2023-03-20 23.33.29.png](1%203%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%88%86%E7%B1%BB%20d9e22694e13d4a64bfb9ae6519b87843/%25E6%2588%25AA%25E5%25B1%258F2023-03-20_23.33.29.png)

## 具有正则化的成本函数：[Cost function with regularization](https://www.coursera.org/learn/machine-learning/lecture/UZTPk/cost-function-with-regularization)

在这个课程中，视频主要介绍了正则化（Regularization）的概念和如何用它来解决过拟合问题。以下是关键知识点：

- **正则化的想法是通过惩罚较大的参数值来简化模型，使其更不容易过拟合。**简化后的模型可能具有较少的特征，因此更不容易过拟合。
    
    ![截屏2023-03-20 23.42.59.png](1%203%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%88%86%E7%B1%BB%20d9e22694e13d4a64bfb9ae6519b87843/%25E6%2588%25AA%25E5%25B1%258F2023-03-20_23.42.59.png)
    
- 通过在损失函数中添加正则化项，可以在模型拟合训练数据的同时，使参数保持较小的值。正则化项通常包括所有权重参数的平方和。
- **正则化参数（λ）用于平衡模型拟合训练数据和保持较小参数值之间的权衡。较小的λ值可能导致过拟合，而较大的λ值可能导致欠拟合。**选择合适的λ值可以在这两个目标之间实现平衡。
    
    ![截屏2023-03-20 23.46.21.png](1%203%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%88%86%E7%B1%BB%20d9e22694e13d4a64bfb9ae6519b87843/%25E6%2588%25AA%25E5%25B1%258F2023-03-20_23.46.21.png)
    
    ![截屏2023-03-21 00.08.24.png](1%203%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%88%86%E7%B1%BB%20d9e22694e13d4a64bfb9ae6519b87843/%25E6%2588%25AA%25E5%25B1%258F2023-03-21_00.08.24.png)
    
    - 🔰名词解释：**`lambda`**(λ) 是正则化参数（regularization parameter）
        
        它是一个非负数，用于控制正则化项的权重。**λ 的值越大，正则化项对损失函数的影响越大，从而更强烈地约束模型参数的大小，降低模型复杂度。**相反，当 λ 的值越小，正则化项对损失函数的影响越小，模型参数将趋向于更复杂的解。通过调整 λ 的值，我们可以在模型的偏差（bias）和方差（variance）之间找到一个平衡点，从而获得性能更好的模型。
        
    - 🔰问题：λ 的值 是怎么确定的呢？是人为用结果判断吗？
        
        **λ（正则化参数）的值通常是通过交叉验证（cross-validation）来确定的。**交叉验证是一种评估模型性能的方法，它将训练数据集分成 k 个子集，对于每一个子集，都使用剩下的 k-1 个子集作为训练数据来训练模型，然后用该子集进行验证。这样，**我们可以得到 k 个验证结果，对这些结果求平均，得到一个综合性能指标。**
        
        为了确定 λ 的最佳值，我们需要在多个候选值中进行搜索。一种常见的方法是使用网格搜索（grid search）：给定一个 λ 值的范围和步长，我们在这个范围内尝试不同的值，然后使用交叉验证计算每个值的模型性能。最后，选择性能最佳的那个 λ 值作为最终参数。
        
        另外，也可以使用更先进的搜索方法，如随机搜索（random search）或贝叶斯优化（Bayesian optimization），这些方法可能在搜索效率和找到更好的参数上有优势。
        
        需要注意的是，λ 的选择是一个超参数调优的过程，与模型的训练过程相互独立。我们通常在训练之前或者训练过程中（如使用验证集）调整超参数，而不是在训练完成后。
        
    - 🔰问题：λ（正则化参数）后面的 j=1,最大值到n，这个j 是什么呢？和损失函数的i是不一样的意思吧？
        
        是的，这里的 j 和损失函数中的 i 是不同的概念。**在损失函数中，i 通常表示数据点的索引，**它用于遍历训练数据集中的每个样本。**在正则化项中，j 表示模型参数（权重）的索引。**当我们谈论正则化时，我们关心的是模型参数的数量和大小。
        
        以线性回归为例，假设有 n 个特征，那么模型参数将有 n+1 个（包括权重 w 和偏置项 b）。我们对权重进行正则化，但通常不对偏置项进行正则化。因此，正则化项从 j=1 开始，一直到 j=n。这表示我们只对权重进行正则化惩罚，从而控制模型的复杂度，避免过拟合。
        
        ![截屏2023-03-21 00.19.44.png](1%203%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%88%86%E7%B1%BB%20d9e22694e13d4a64bfb9ae6519b87843/%25E6%2588%25AA%25E5%25B1%258F2023-03-21_00.19.44.png)
        
    - 🔰问题：这个结论什么意思？前面说过“λ 的值越大，正则化项对损失函数的影响越大，从而更强烈地约束模型参数的大小，降低模型复杂度。” 那么这个强烈的约束模型参数的大小和这里说的是一样的吗？当取一个相当大值的λ，又想取J_wb的最小值，那么 w_j的和就要足够小，所以造成x_i*w_i 趋近于 0，是这样的意思吗？
        
        是的，你理解得对。正则化参数λ越大，它对损失函数的影响越大，从而更强烈地约束模型参数的大小。这就是为什么增加λ会减小参数w1、w2、...、wn的大小。
        
        当λ取一个相对较大的值时，为了使损失函数最小化，模型参数wj（权重）必须足够小。这导致特征xi和权重wi相乘的结果接近于0。这样可以减少特定特征在模型中的影响，从而降低模型的复杂度，有助于减轻过拟合现象。
        
- 视频介绍了正则化在线性回归和逻辑回归中的应用。接下来的课程将详细讲解如何将梯度下降法应用于这两种方法，以避免过拟合。

总之，在这门课程中，你学到了正则化如何帮助解决过拟合问题，以及如何在损失函数中添加正则化项。在实际应用中，正则化是一个非常有用的技术，可以提高模型在新数据上的泛化能力。

> **本节一句话总结：**
正则化：惩罚较大的参数来简化模型。
正则化参数（λ）：较小的λ值可能导致过拟合（惩罚小，近似无正则化状态），而较大的λ值可能导致欠拟合。
> 

## 正则化线性回归：[Regularized linear regression](https://www.coursera.org/learn/machine-learning/lecture/WRULa/regularized-linear-regression)

在这个视频中，我们学习了如何将梯度下降与正则化线性回归结合使用。以下是关键知识点：

- **正则化线性回归（Regularized Linear Regression **）*ˈrɛɡjələˌraɪz* 的成本函数：成本函数包括常规的均方误差成本函数和一个额外的正则化项，其中Lambda是正则化参数**。我们的目标是找到最小化正则化成本函数的参数w和b。
    
    ![Untitled](1%203%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%88%86%E7%B1%BB%20d9e22694e13d4a64bfb9ae6519b87843/Untitled%2025.png)
    
- 梯度下降算法：在正则化线性回归中，梯度下降算法的更新规则与未正则化的线性回归相似，但成本函数J的定义略有不同。**对于新的成本函数J，梯度下降算法的更新规则为**：
    - **更新w_j：w_j = w_j * (1 - α * λ / m) - α * (1/m) * Σ(w * x + b - y) * x_j**
    - **更新b：b = b - α * (1/m) * Σ(w * x + b - y)**
    注意：这里的α是学习率，λ是正则化参数，m是训练样本数量。
    
    ![Untitled](1%203%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%88%86%E7%B1%BB%20d9e22694e13d4a64bfb9ae6519b87843/Untitled%2026.png)
    
    ![Untitled](1%203%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%88%86%E7%B1%BB%20d9e22694e13d4a64bfb9ae6519b87843/Untitled%2027.png)
    
    ![截屏2023-03-21 20.26.33.png](1%203%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%88%86%E7%B1%BB%20d9e22694e13d4a64bfb9ae6519b87843/%25E6%2588%25AA%25E5%25B1%258F2023-03-21_20.26.33.png)
    
- 正则化项的影响：正则化项的存在使得每次迭代过程中，w_j都会乘以一个略小于1的数，从而缩小w_j的值。这解释了为什么正则化会使参数w_j在每次迭代中逐渐缩小。
    
    ![Untitled](1%203%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%88%86%E7%B1%BB%20d9e22694e13d4a64bfb9ae6519b87843/Untitled%2028.png)
    

通过应用正则化线性回归，可以在具有大量特征和相对较小训练集的情况下减少过拟合现象，从而使线性回归在许多问题上表现得更好。在下一个视频中，我们将正则化思想应用于逻辑回归，以避免逻辑回归中的过拟合现象。

> **本节一句话总结：**
正则化线性回归公式变化：成本函数（正则化）→梯度下降（正则化）
> 

## 正则化逻辑回归：[Regularized logistic regression](https://www.coursera.org/learn/machine-learning/lecture/cAxpF/regularized-logistic-regression)

在这个视频中，我们学习了如何实现正则化逻辑回归。以下是关键知识点：

- 正则化逻辑回归的目的：正则化逻辑回归可以减少过拟合现象，特别是当使用高阶多项式特征或大量特征时。
- 正则化逻辑回归的成本函数：在原始逻辑回归的成本函数基础上，我们添加了一个正则化项，其中λ是正则化参数。我们的目标是找到最小化正则化成本函数的参数w和b。
    
    ![Untitled](1%203%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%88%86%E7%B1%BB%20d9e22694e13d4a64bfb9ae6519b87843/Untitled%2029.png)
    
- 梯度下降算法：在正则化逻辑回归中，梯度下降算法的更新规则与未正则化的逻辑回归相似，但成本函数J的定义略有不同。对于新的成本函数J，梯度下降算法的更新规则为：
    - **更新w_j：w_j = w_j - α * (1/m) * Σ(f(z) - y) * x_j + λ / m * w_j**
    - **更新b：b = b - α * (1/m) * Σ(f(z) - y)**
    注意：这里的α是学习率，λ是正则化参数，m是训练样本数量。
    
    ![Untitled](1%203%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%88%86%E7%B1%BB%20d9e22694e13d4a64bfb9ae6519b87843/Untitled%2030.png)
    
- 参数更新的影响：正如正则化线性回归中所提到的，正则化项的存在使得每次迭代过程中，w_j都会逐渐缩小。这解释了为什么正则化会使参数w_j在每次迭代中逐渐缩小。

通过应用正则化逻辑回归，即使在具有大量特征的情况下，也可以减少过拟合现象，从而使逻辑回归在许多问题上表现得更好。此外，了解何时以及如何减少过拟合在现实世界中也是一项非常有价值的技能。

> **本节一句话总结：**
正则化逻辑回归公式变化：成本函数（正则化）→梯度下降（正则化）  
正则化逻辑回归和正则化线性回归公式很像，只有 f(x）的定义不同。
在逻辑回归中，f(x) = sigmoid(w^T x + b)；在线性回归中，f(x) = w^T x + b。
> 

## 实验室:[Regularization](https://www.coursera.org/learn/machine-learning/ungradedLab/36A9A/optional-lab-regularization)

- 损失函数：正则化和没有正则化
    
    ![截屏2023-03-21 20.45.58.png](1%203%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%88%86%E7%B1%BB%20d9e22694e13d4a64bfb9ae6519b87843/%25E6%2588%25AA%25E5%25B1%258F2023-03-21_20.45.58.png)
    
    ![截屏2023-03-21 20.48.05.png](1%203%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%88%86%E7%B1%BB%20d9e22694e13d4a64bfb9ae6519b87843/%25E6%2588%25AA%25E5%25B1%258F2023-03-21_20.48.05.png)
    
- 梯度下降
    
    ![截屏2023-03-21 20.49.25.png](1%203%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%88%86%E7%B1%BB%20d9e22694e13d4a64bfb9ae6519b87843/%25E6%2588%25AA%25E5%25B1%258F2023-03-21_20.49.25.png)
    
- 运行后效果
    
    λ 是正则化参数 取值 0.2 就有明显的变化了
    
    ![截屏2023-03-21 20.55.15.png](1%203%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%88%86%E7%B1%BB%20d9e22694e13d4a64bfb9ae6519b87843/%25E6%2588%25AA%25E5%25B1%258F2023-03-21_20.55.15.png)
    
    ![截屏2023-03-21 20.55.08.png](1%203%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%88%86%E7%B1%BB%20d9e22694e13d4a64bfb9ae6519b87843/%25E6%2588%25AA%25E5%25B1%258F2023-03-21_20.55.08.png)
    
    ![截屏2023-03-21 20.55.23.png](1%203%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%88%86%E7%B1%BB%20d9e22694e13d4a64bfb9ae6519b87843/%25E6%2588%25AA%25E5%25B1%258F2023-03-21_20.55.23.png)
    
- 🔰问题：正则化参数0.2就有很明显的变化了、一直到增加到 1 ，似乎变化就不明显了。一般来说取更偏向于 0 的更好吗？
    
    正则化参数的选择需要根据实际情况进行调整。选择一个合适的正则化参数是一个权衡的过程。**正则化参数的值越大，模型越倾向于简化，有助于防止过拟合；**而正则化参数的值越小，模型则更倾向于拟合训练数据，可能导致过拟合。
    
    正如你所观察到的，**较小的正则化参数（如0.2）可能已经产生了明显的效果，而进一步增加正则化参数（如1）可能对模型性能的改善不再明显。**在实际应用中，我们需要找到一个合适的正则化参数，使模型在训练数据和未知数据之间取得一个好的平衡。
    
    **为了找到合适的正则化参数，可以使用交叉验证方法。交叉验证通过将训练数据集划分为若干个子集，并在不同的子集上进行训练和验证**，可以帮助我们评估不同正则化参数下模型的性能。通过比较不同正则化参数下的交叉验证得分，我们可以选择一个表现最好的正则化参数。
    
    总之，并没有固定的规则来说明正则化参数应该更接近0或者更大。你需要根据实际情况和数据进行调整，并使用交叉验证等技术来选择最佳的正则化参数。
    

> **本节一句话总结：**
完整公式及程序运行后的图形
> 

## **Week 3 practice lab: logistic regression**

![截屏2023-03-21 22.01.07.png](1%203%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%88%86%E7%B1%BB%20d9e22694e13d4a64bfb9ae6519b87843/%25E6%2588%25AA%25E5%25B1%258F2023-03-21_22.01.07.png)

![截屏2023-03-21 21.59.56.png](1%203%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%88%86%E7%B1%BB%20d9e22694e13d4a64bfb9ae6519b87843/%25E6%2588%25AA%25E5%25B1%258F2023-03-21_21.59.56.png)