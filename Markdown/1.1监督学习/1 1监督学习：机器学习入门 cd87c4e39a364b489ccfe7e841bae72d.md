# 1.1监督学习：机器学习入门

KeyWord: 回归模型, 学习率, 成本函数, 梯度下降, 监督学习和非监督学习, 线性回归
復習: Done
日付: 2023年3月12日
最終更新日時: 2023年4月13日 10:15
種別: 授業

# 机器学习概述：**Overview of Machine Learning**

---

## 欢迎来到机器学习：[Welcome to machine learning!](https://www.coursera.org/learn/machine-learning/lecture/iYR2y/welcome-to-machine-learning)

本段视频简要介绍了机器学习在我们日常生活中的各种应用以及它如何在各行各业产生影响。本视频中提到的关键知识点如下：

1. 机器学习是**让计算机在没有明确编程的情况下进行学习的科学**。
2. 机器学习被应用于我们每天接触到的许多应用程序，例如：
    - **网络搜索**（谷歌、必应、百度）对网页进行排名。
    - 社交媒体（Instagram、Snapchat）**识别并标记照片中的好友**。
    - 视频流媒体服务（如 Netflix，Hulu）根据用户喜好**推荐**电影和电视节目。
    - 语音转文字和虚拟助手（Siri、谷歌助手）进行**语音识别和自然语言理解**。
    - **电子邮件服务进行垃圾邮件检测和过滤**。
    
    ![截屏2023-04-12 18.09.59.png](1%201%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%20cd87c4e39a364b489ccfe7e841bae72d/%25E6%2588%25AA%25E5%25B1%258F2023-04-12_18.09.59.png)
    
3. 机器学习也正在进入各种行业和大公司，例如：
    - **优化风力涡轮发电机的发电效率**，以应对气候变化。
    - 在医疗保健领域，**协助医生做出准确的诊断**。
    - 在工厂中实施计算机视觉，**检查产品是否有瑕疵**。
4. 这门课程将教你有关机器学习的知识，以及如何在代码中实现机器学习。许多学过这门课程的学生已经建立了成功的机器学习系统或者在人工智能领域开展了成功的职业生涯。

通过学习和理解这些关键知识点，你将对机器学习有一个全面的了解。

> **本节精炼总结：**
机器学习：在没有明确编程的情况下进行学习的科学。
机器学习对各行各业都有影响：应用（搜索、推荐、语音识别、垃圾邮件）、行业公司（医疗、发电、工厂）
> 

## 机器学习的应用：[Applications of machine learning](https://www.coursera.org/learn/machine-learning/lecture/IjrpM/applications-of-machine-learning)

在这门课程中，您将学习机器学习的最新技术，并亲手实现机器学习算法。以下是一些关键知识点：

1. 本课程将**讲授最重要的机器学习算法**，其中一些算法正是当今大型AI公司和科技公司所使用的。**除了学习算法，您还将了解如何在实践中使它们表现良好的重要技巧和窍门**。
2. 机器学习如今被广泛应用，因为对于许多有趣的任务（如网络搜索、语音识别、X光诊断疾病或构建自动驾驶汽车），我们无法编写显式程序来完成。唯一的方法是让机器自主学习。
    
    ![截屏2023-04-12 19.10.19.png](1%201%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%20cd87c4e39a364b489ccfe7e841bae72d/%25E6%2588%25AA%25E5%25B1%258F2023-04-12_19.10.19.png)
    
3. 当今，成千上万的人正在研究机器学习应用。学会这些技能后，您也可以在不同的应用和行业中尝试和发展。几乎所有行业都将在现在或不久的将来受到机器学习的重大影响。
4. 人工智能领域有一个梦想，即有朝一日能够构建具有人类智能的机器，这被称为人工通用智能（AGI）。虽然我们离实现这一目标还很遥远，但大多数AI研究人员相信，使用学习算法是实现这一目标的最佳途径。
5. 根据麦肯锡的一项研究，到2030年，人工智能和机器学习预计每年将额外创造13万亿美元的价值。尽管机器学习已经在软件行业创造了巨大的价值，但在零售、旅游、交通、汽车、材料制造等领域还有更大的潜在价值等待创造。**由于如此多的不同领域中存在大量未开发的机会，目前对这一技能的需求巨大。**这也是为什么现在学习机器学习是个绝佳时机。

通过学习这些关键知识点，您将更好地掌握机器学习领域的技能。在下一个视频中，我们将更正式地讨论机器学习的定义，了解机器学习问题和算法的主要类型，并开始学习相关术语。

> **本节精炼总结：**
本课程讲解机器学习算法和在实践中使其表现良好的技巧。
机器学习除了在软件行业，不同领域内存在大量未开发的机会，现在是学习机器学习的绝佳时机。
> 

# 监督式与非监督式学习：**Supervised vs. Unsupervised Machine Learning**

---

## [What is machine learning?](https://www.coursera.org/learn/machine-learning/lecture/PNeuX/what-is-machine-learning)

在这个视频中，你将学习机器学习的定义，并了解何时可能需要应用它。以下是一些关键知识点：

1. **机器学习的定义：机器学习是一门使计算机在没有明确编程的情况下具备学习能力的研究领域。**这个定义归功于Arthur Samuel。
2. 本课程将涵盖许多不同的学习算法。**机器学习的两大类型是：监督学习和无监督学习。**接下来的视频将详细介绍这些术语的含义。
    
    ![截屏2023-04-12 19.28.18.png](1%201%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%20cd87c4e39a364b489ccfe7e841bae72d/%25E6%2588%25AA%25E5%25B1%258F2023-04-12_19.28.18.png)
    
3. 监督学习在现实世界的应用中使用得最多，也取得了最快的进步和创新。在这个专项课程中（共有三门课程），第一和第二门课程将重点关注监督学习，第三门课程将关注无监督学习、推荐系统和强化学习。
4. 目前最常用的学习算法类型是：监督学习、无监督学习和推荐系统。
5. 本课程还将花大量时间讲解应用学习算法的实用建议。在这里，你将学到这些工具以及如何有效应用它们的技能。
6. 本课程的一个相对独特之处是：你将学到如何实际开发出实用、有价值的机器学习系统的最佳实践。这样，你就不太可能成为那些朝错误方向努力六个月的团队之一。
7. 通过本课程，**你将了解最熟练的机器学习工程师如何构建系统**，并希望你最终成为当今世界中极少数懂得设计和构建高级机器学习系统的人之一。

在下一个视频中，我们将更深入地了解什么是监督学习和无监督学习，以及何时可能需要使用它们。

> **本节精炼总结：**
机器学习的类型：监督学习和无监督学习。
通过本课程，将了解最熟练的机器学习工程师如何构建系统。
> 

## 监督学习1 ：[Supervised learning part](https://www.coursera.org/learn/machine-learning/lecture/s91wX/supervised-learning-part-1)

在这个视频中，我们将讨论**监督学习以及其中的两个主要类型：回归和分类。**以下是一些关键知识点：

1. 监督学习：监督学习是机器学习中最常用的类型，占据了机器学习创造的经济价值的99%。**监督学习算法通过学习输入（x）到输出（y）的映射关系。**在监督学习中，你需要为算法提供一些带有**正确答案（正确的y值）的样本来进行学习**。
    
    ![截屏2023-02-26 19.29.18.png](1%201%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%20cd87c4e39a364b489ccfe7e841bae72d/%25E6%2588%25AA%25E5%25B1%258F2023-02-26_19.29.18.png)
    
2. **回归**：**回归是监督学习的一种类型，任务是预测一个连续的数值。**例如，根据房屋面积预测房价。在这个问题中，我们可以使用不同的算法来拟合数据（例如线性回归或曲线拟合）。
    
    ![截屏2023-04-12 19.33.24.png](1%201%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%20cd87c4e39a364b489ccfe7e841bae72d/%25E6%2588%25AA%25E5%25B1%258F2023-04-12_19.33.24.png)
    
3. **分类**：分类是监督学习的另一种类型，**任务是将输入数据分配到有限数量的类别中。**例如，根据电子邮件内容判断是否为垃圾邮件。
    
    
    | input X | output Y | Application |
    | --- | --- | --- |
    | email | 是垃圾？ 0/1 | 垃圾分类 |
    | 英文语音 | 中文文字翻译 | 机器语音翻译软件 |
    | 英语 | 日语 | 机器翻译 |
    | 广告、客户信息 | click？0/1 | 在线广告策略分析 |
    | 图片、雷达等 | 与其他车的位置关系 | 自动驾驶 |
4. 监督学习的应用：监督学习在许多领域都有广泛的应用，例如垃圾邮件过滤、语音识别、机器翻译、在线广告、自动驾驶汽车、制造业视觉检测等。

在下一个视频中，我们将更详细地讨论分类问题以及如何解决它们。

> **本节精炼总结：**
监督学习：通过学习 x到 y 的映射关系。需提供有正确答案的 y
监督学习分类：回归（预测连续数值。如预测房价）、分类（预测离散数值。如判断垃圾邮件）
> 

## 监督学习2：[Supervised learning part 2](https://www.coursera.org/learn/machine-learning/lecture/Q8Vvp/supervised-learning-part-2)

在这个视频中，我们将讨论监督学习的另一种主要类型——分类算法。以下是一些关键知识点：

1. **分类算法：**分类算法是监督学习的一种类型，**任务是将输入数据分配到有限数量的类别中**。与回归不同，分类算法的输出是离散的类别，而不是连续的数值。例如，判断肿瘤是良性还是恶性。
    
    ![截屏2023-04-12 19.52.04.png](1%201%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%20cd87c4e39a364b489ccfe7e841bae72d/%25E6%2588%25AA%25E5%25B1%258F2023-04-12_19.52.04.png)
    
2. 二分类问题：分类算法可以用于解决二分类问题，即输出只有两个类别。在肿瘤检测的例子中，类别为良性（0）和恶性（1）。
    
    ![截屏2023-04-12 19.48.35.png](1%201%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%20cd87c4e39a364b489ccfe7e841bae72d/%25E6%2588%25AA%25E5%25B1%258F2023-04-12_19.48.35.png)
    
3. **输入特征：监督学习算法可以使用一个或多个输入特征进行预测。**例如，在肿瘤检测问题中，除了肿瘤大小外，还可以使用病人的年龄作为输入特征。
4. 多分类问题：**分类算法也可以解决多分类问题，即输出有多个类别。**例如，肿瘤可能被诊断为良性、恶性类型1或恶性类型2。
5. 边界线：在分类问题中，**学习算法需要找到一个边界线，将不同类别的数据点分开。**通过找到合适的边界线，算法可以帮助医生进行诊断。
    
    ![截屏2023-04-12 19.48.47.png](1%201%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%20cd87c4e39a364b489ccfe7e841bae72d/%25E6%2588%25AA%25E5%25B1%258F2023-04-12_19.48.47.png)
    
    ![截屏2023-04-12 19.49.07.png](1%201%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%20cd87c4e39a364b489ccfe7e841bae72d/%25E6%2588%25AA%25E5%25B1%258F2023-04-12_19.49.07.png)
    

总结一下，监督学习包括回归和分类两种主要类型。回归任务要求预测连续的数值，而分类任务要求预测离散的类别。接下来，我们将学习另一种主要的机器学习类型——无监督学习。请观看下一个视频以了解更多关于无监督学习的信息。

> **本节精炼总结：**
监督学习-分类算法：将输入分类到有限的类别中（需找到边界线）。
输入特征和输出都可以是多个。
> 

## **非监督式学习1：[Unsupervised Learning](https://www.coursera.org/learn/machine-learning/lecture/TxO6F/unsupervised-learning-part-1)**

在这个视频中，我们将讨论机器学习中另一种广泛使用的方法——无监督学习。以下是一些关键知识点：

1. **无监督学习**：与监督学习不同，**无监督学习算法不需要输出标签**。它们的**目标是从未标记的数据中找到某种结构、模式或者有趣的信息**。
    
    ![截屏2023-04-12 20.00.54.png](1%201%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%20cd87c4e39a364b489ccfe7e841bae72d/%25E6%2588%25AA%25E5%25B1%258F2023-04-12_20.00.54.png)
    
2. **聚类算法：聚类算法是一种无监督学习方法，将未标记的数据分为不同的群组或簇**。
    1. 例如，Google News 使用聚类算法将相关的新闻文章归类在一起。聚类算法会自动找出哪些词汇表明某些文章属于同一组。
        
        ![截屏2023-04-12 20.02.41.png](1%201%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%20cd87c4e39a364b489ccfe7e841bae72d/%25E6%2588%25AA%25E5%25B1%258F2023-04-12_20.02.41.png)
        
    2. **DNA聚类**：无监督学习也可以用于基因或DNA数据的聚类。通过将个体分为不同的类型，研究人员可以更好地理解基因特征。这是一种无监督学习方法，因为算法需要自动发现数据中的结构，而不是根据预先给定的类型进行分类。
        
        ![截屏2023-04-12 20.02.51.png](1%201%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%20cd87c4e39a364b489ccfe7e841bae72d/%25E6%2588%25AA%25E5%25B1%258F2023-04-12_20.02.51.png)
        
    3. **市场细分**：无监督学习还可以用于市场细分，将客户自动分为不同的市场群体，以便更高效地为客户提供服务。
        
        ![截屏2023-04-12 20.03.09.png](1%201%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%20cd87c4e39a364b489ccfe7e841bae72d/%25E6%2588%25AA%25E5%25B1%258F2023-04-12_20.03.09.png)
        
- 🔰问题：无监督学习的一种是聚类算法，那还有其他算法？
    
    是的，Google News、DNA数据的聚类和市场细分都属于聚类算法的应用。聚类算法是无监督学习的一种方法，它的主要目的是将相似的数据点分组在一起。
    
    除了聚类算法，无监督学习还有其他类型的方法，例如：
    
    1. 异常检测（Anomaly Detection）：异常检测是一种用于检测数据中异常或罕见事件的方法。它在诸如金融欺诈检测、网络入侵检测和设备故障预测等领域有广泛应用。
    2. 降维（Dimensionality Reduction）：降维是一种将高维数据压缩到低维数据的方法，同时尽量保留原始数据中的信息。降维可以用于数据可视化、提高算法性能、减少存储需求等。常见的降维方法有主成分分析（PCA）、t-分布邻域嵌入算法（t-SNE）等。
    3. 生成模型（Generative Models）：生成模型是一类可以生成新数据的无监督学习方法。最近，生成对抗网络（GANs）和变分自编码器（VAEs）等生成模型在图像生成、文本生成和风格迁移等领域取得了显著的成功。

> **本节精炼总结：**
无监督学习：训练数据没有标签。目标是找到某种结构、信息等。
无监督学习-聚类算法：新闻归类、DNA 数据分析、市场细分
> 

## 非监督式学习2：[Unsupervised learning part 2](https://www.coursera.org/learn/machine-learning/lecture/jKBHE/unsupervised-learning-part-2)

在上一个视频中，我们讨论了无监督学习及其一种类型——聚类。现在让我们给出一个更正式的无监督学习定义，并快速了解除聚类之外的其他类型的无监督学习。

无监督学习与监督学习不同之处在于，数据只包含输入x而不包含输出标签y。算法需要在数据中找到某种结构、模式或有趣的内容。我们已经看到了一种无监督学习的例子——聚类算法，它将相似的数据点分组在一起。

在这门课程中，你将学到聚类以及另外两种无监督学习类型。一种叫做**异常检测，用于检测异常事件。**这对于金融系统中的欺诈检测非常重要，因为异常事件、异常交易可能是欺诈的迹象，还有许多其他应用场景。**另一种是降维，它可以让你将一个大型数据集压缩成一个小得多的数据集，同时尽可能地减少信息损失。**如果你对异常检测和降维还不是很了解，不用担心，我们将在后面的课程中详细讲解。

![截屏2023-04-12 20.05.33.png](1%201%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%20cd87c4e39a364b489ccfe7e841bae72d/%25E6%2588%25AA%25E5%25B1%258F2023-04-12_20.05.33.png)

关于无监督学习的例子，我们来看下面的情景。垃圾邮件过滤问题，如果你有标记的数据，已知哪些邮件是垃圾邮件或非垃圾邮件，你可以将其视为一个监督学习问题。第二个例子是新闻报道，这正是上一个视频中提到的谷歌新闻的例子。你可以使用聚类算法将新闻文章进行分组，这是无监督学习。市场细分的例子，你可以将其视为无监督学习问题，因为你可以让算法自动发现市场细分。最后一个例子是诊断糖尿病，这实际上与我们在监督学习视频中的乳腺癌示例非常相似。只是我们现在要判断的是糖尿病或非糖尿病。你可以将其视为监督学习问题，就像我们在乳腺肿瘤分类问题中所做的那样。

![截屏2023-04-12 20.05.49.png](1%201%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%20cd87c4e39a364b489ccfe7e841bae72d/%25E6%2588%25AA%25E5%25B1%258F2023-04-12_20.05.49.png)

虽然在上一个视频中，我们主要讨论了聚类，但在这门课程的后面，我们将更深入地研究异常检测和降维。这就是无监督学习。

> **本节精炼总结：**
无监督学习-异常检测，如金融诈骗
无监督学习-降维，将大型数据集压缩为小数据，同时尽量少的丢失信息
> 

# 回归模型：**Regression Model**

---

## 线性回归模型 1：**[Linear Regression with One Variable](https://www.coursera.org/learn/machine-learning/lecture/1ACA2/linear-regression-model-part-1)**

在这个视频中，我们主要讨论了监督学习的整体过程，特别是线性回归模型。**线性回归模型是通过拟合一条直线来描述数据之间的关系。**在学习线性回归模型的过程中，你会发现许多概念同样适用于本专业课程后面介绍的其他机器学习模型。

以预测房价为例，我们可以使用线性回归模型来预测房价与房屋面积之间的关系。数据集包含了美国波特兰市的房屋尺寸和价格信息。我们可以在二维图中用横轴表示房屋面积（平方英尺），纵轴表示房价（千美元），然后在图上绘制各个房屋的数据点。

当你要帮助客户估算房价时，可以使用线性回归模型。模型会拟合一条直线，根据这条直线，你可以估算出某个特定面积的房屋大概可以卖多少钱。这就是一个**典型的监督学习模型**。因为我们**使用已知房屋面积和对应房价的数据来训练模型，这些房价就是正确的答案**。

![截屏2023-04-12 20.23.50.png](1%201%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%20cd87c4e39a364b489ccfe7e841bae72d/%25E6%2588%25AA%25E5%25B1%258F2023-04-12_20.23.50.png)

线性回归模型是监督学习模型的一种类型，称为回归模型，因为它的输出是一个连续的数值，如房价。监督学习的另一种常见类型是分类模型，它预测的是离散的类别，如判断一张图片是猫还是狗，或者根据病历判断患者是否患有某种疾病。

关于数据表示，我们通常用小写的**x表示输入变量，也称为特征**；用小写的**y表示输出变量**，也称为目标变量。**数据集中的每一行代表一个训练样本**。我们用小写的**m表示训练样本的总数**。为了表示**特定的训练样本**，我们使用带上标的表示法，如**x^(i)和y^(i)**，**上标i表示第i个训练样本**。

![Untitled](1%201%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%20cd87c4e39a364b489ccfe7e841bae72d/Untitled.png)

在这个视频中，你了解了训练集的概念以及描述训练集的标准表示法。在下一个视频中，我们将探讨如何将这个训练集输入到学习算法中，使算法从这些数据中学习。

- 🔰问题：线性回归和回归模型是一个意思吗？
    
    不是，线性回归是回归模型的一种，但回归模型还包括其他类型，如多项式回归、岭回归、套索回归等。所以，回归模型包含线性回归。
    
- 🔰**训练集（Training Set）**是用于训练模型的数据集合。它通常是一个包含输入特征和对应输出值（或目标变量）的数据集，模型使用这些数据来学习如何将输入特征映射到正确的输出值。
    
    简单来说，训练集就是模型使用的“参考材料”，用于学习预测输出的规律和模式。模型在训练过程中会根据训练集中的样本不断调整自己的参数和权重，从而使得模型的预测输出与训练集中的真实输出尽可能接近。
    
- 🔰**训练样本的数量（Number of Training Examples）**是指在机器学习中用于训练模型的数据样本的数量。通常来说，训练样本的数量越多，机器学习模型的泛化能力就越好，即对新的、未见过的数据的预测能力就越强。
    
    训练样本的数量可以影响模型的准确性和复杂度。当训练样本较少时，模型可能会过拟合训练数据，即对训练数据表现很好，但对新数据的泛化能力较差。而当训练样本足够多时，模型可能会更好地捕捉数据的规律，从而实现更好的泛化能力。
    
- 🔰**单个训练样本（Single Training Example）**指的是用于训练机器学习模型的单个输入数据样本。在监督学习中，单个训练样本通常包含一个输入特征向量和一个对应的输出标签或目标变量。
    
    例如，在一个二元分类问题中，一个单独的训练样本可能是一张图片和一个标签，其中图片是输入特征向量，标签是输出变量。在一个回归问题中，一个单独的训练样本可能是一个具有多个特征的房屋，和对应的售价，其中房屋特征是输入特征向量，售价是输出变量。
    

> **本节精炼总结：**
监督学习-回归模型-线性回归模型：通过拟合一条直线来描述数据关系。如使用已知房屋面积 x和房价 y，训练模型→ 预测x‘时房屋价格y’
x:特征 、y:输出、第 i 次训练样本等表示方式。
> 

## 线性回归模型 2：**[Linear Regression with One Variable](https://www.coursera.org/learn/machine-learning/lecture/nucNi/linear-regression-model-part-2)**

在这个视频中，我们来探讨监督学习的过程。监督学习算法输入一个数据集，然后具体做什么以及输出什么？让我们在这个视频中了解一下。

回顾一下，监督学习中的训练集包括输入特征（如房屋大小）以及输出目标（如房屋价格）。输出目标是模型从中学习的正确答案。为了训练模型，您需要将训练集（包括输入特征和输出目标）提供给学习算法。然后，监督学习算法将生成某种函数。我们将这个函数表示为小写字母f，其中f表示函数。

f的任务是接收一个新的输入x并输出一个估计值或预测值，我们将其称为y-hat，写作带有小帽子符号的字母y。在机器学习中，约定俗成的是y-hat表示y的估计值或预测值。**函数f被称为模型（modle）。X被称为输入或输入特征（feature），模型的输出是预测值y-hat。**模型的预测值是y的估计值。当符号只是字母y时，它指的是目标，即训练集中的实际真实值。相反，y-hat是一个估计值。它可能是或可能不是实际的真实值。当您帮助客户出售房子时，房子的真实价格在出售之前是未知的。您的模型f给出房子的大小，输出价格，这是对真实价格的估计，也就是预测值。

![Untitled](1%201%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%20cd87c4e39a364b489ccfe7e841bae72d/Untitled%201.png)

现在，当我们设计一个学习算法时，一个关键问题是：我们如何表示函数f？换句话说，我们将使用什么数学公式来计算f？现在，让我们坚持使用一条直线作为f。您的函数可以写成f_w, b(x) = wx + b。我很快就会定义w和b。但现在，请知道w和b是数字，为w和b选择的值将根据输入特征x确定预测值y-hat。这个f_w b(x)表示f是一个以x为输入的函数，根据w和b的值，f将输出某个预测值y-hat。作为替代写这个f_w，b(x)，我有时只写f(x)，而不是明确地将w和b包含到下标中。这只是一个更简单的表示法，意味着与f_w b(x)完全相同的事情。

让我们将训练集绘制在图上，其中输入特征x在水平轴上，输出目标y在垂直轴上。请记住，**算法从这些数据中学习并生成最佳拟合线，比如这里的这一条。**这条直线是线性函数f_w b(x) = wx + b。或者更简单地说，我们可以去掉w和b，只写**f(x) = wx + b。这个函数的作用是使用x的直线函数来预测y的值。**

你可能会问，为什么我们选择线性函数，线性函数只是一条直线的术语，而不是像曲线或抛物线这样的非线性函数？有时候你可能也想拟合更复杂的非线性函数，比如这样的曲线。但是，由于这个线性函数相对简单且易于操作，我们先使用线性模型作为基础，最终帮助您获得更复杂的非线性模型。这个特定模型有一个名字，叫做线性回归。更具体地说，这是具有一个变量的线性回归，其中“一个变量”这个短语意味着有一个单一的输入变量或特征x，即房屋的大小。具有一个输入变量的线性模型的另一个名称是单变量线性回归，其中“uni”在拉丁语中意味着一个，“变量”意味着变量。单变量只是一种表示一个变量的高雅方式。

在以后的视频中，您还将看到回归的一个变种，您将根据房子的其他特征（如卧室数量等）进行预测，而不仅仅是房子的大小。顺便说一下，在观看这个视频后，还有一个可选的实验室。您不需要编写任何代码。只需审查它，运行代码，看看它做了什么。这将向您展示如何在Python中定义直线函数。实验室将让您选择w和b的值来尝试拟合训练数据。如果您不想做实验室，您不必做，但我希望您在观看这个视频后去尝试一下。

> **本节精炼总结：**
函数 f：模型Model 、变量 X：特征 Feature 、输出 y-hat：预测值
算法（单变量线性回归）从训练集数据 X、Y 中学习并生成最佳拟合线 f(x)=wx+b。 
函数 f(x) 的作用是使用 x 的直线函数预测 y-hat。
> 

## 成本函数公式  : [Cost function formula](https://www.coursera.org/learn/machine-learning/lecture/1Z0TT/cost-function-formula)

**成本函数（Cost Function）是指机器学习模型中用于衡量预测结果与实际结果之间差异的函数。**成本函数通常用于优化机器学习模型的参数。

1. 线性函数：线性回归的基本形式是 f_w, b(x) = wx + b，其中 **b是截距（y-intercept），w是斜率（slope）**。通过训练数据来学习这个模型的参数b和w，使得模型能够对新的输入数据进行准确的预测。
    
    ![截屏2023-04-12 20.55.41.png](1%201%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%20cd87c4e39a364b489ccfe7e841bae72d/%25E6%2588%25AA%25E5%25B1%258F2023-04-12_20.55.41.png)
    
2. **参数（w 和 b）：机器学习模型中可以在训练过程中调整的变量**，有时也称为系数或权重。
    
    ![截屏2023-03-12 21.29.36.png](1%201%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%20cd87c4e39a364b489ccfe7e841bae72d/%25E6%2588%25AA%25E5%25B1%258F2023-03-12_21.29.36.png)
    
3. 预测值和实际值：对于给定的输入 x^i，函数 f 会生成一个预测值 y_hat^i，而 y^i 是实际目标值。
4. **平方误差成本函数（Squared error cost function）是一种用于衡量模型预测结果与真实结果之间误差大小的函数。**在一元线性回归模型中，通常使用平方误差成本函数作为优化目标，**目的是最小化预测值与真实值之间的平方误差**。我们可以找到合适的参数 w 和 b，使得模型能够更好地拟合训练数据。
    
    ![截屏2023-04-12 21.06.22.png](1%201%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%20cd87c4e39a364b489ccfe7e841bae72d/%25E6%2588%25AA%25E5%25B1%258F2023-04-12_21.06.22.png)
    
    - **J(w,b)表示损失函数，m表示训练集中样本的数量**，**y^{(i)} 表示第i个样本的真实标签值**，**x^{(i)} 表示第i个样本的输入特征，w和b分别表示模型的参数。**
    - 平方误差成本函数的意义是，对于所有的训练样本，计算模型预测值与真实值之间的差值，取差值的平方后再求平均值。最小化平方误差成本函数的过程，就是找到使预测值与真实值之间误差最小的模型参数。

> **本节精炼总结：**
成本函数：衡量预测结果与真是结果之间的差异的函数。
线性函数 f(x)=wx+b，w-斜率、b -截距。
机器学习模型在训练过程中调整参数w、b，目标是最小化成本函数。
成本函数-平方差成本函数（Squared error cost function）：J(w,b) ，直观感受是将所有 Y 与 y-hat的差相加、再取平均值。
> 

## 成本函数直觉：[Cost function intuition](https://www.coursera.org/learn/machine-learning/lecture/FthLz/cost-function-intuition)

在这个视频中，我们学习了代价函数（成本函数）的概念，并通过一个简化的线性回归问题建立了关于成本函数的直观理解。下面是一些关键知识点：

1. 成本函数（ cost function）J：代价函数衡量模型预测值与实际值之间的差异。在线性回归中，我们希望找到一组参数使得代价函数的值最小。
2. 线性回归模型：简化版的线性回归模型为 f_w(x) = wx，其中 w 是模型的参数。在这个简化版的问题中，我们的目标是找到一个 w 值，使得代价函数 J(w) 最小。
    
    ![截屏2023-03-12 21.56.16.png](1%201%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%20cd87c4e39a364b489ccfe7e841bae72d/%25E6%2588%25AA%25E5%25B1%258F2023-03-12_21.56.16.png)
    
3. 可视化成本函数：通过绘制不同的 w 值对应的直线和代价函数值，我们可以直观地理解代价函数 J(w) 如何随 w 值变化。选择使得代价函数 J(w) 达到最小值的 w 值，将得到一个拟合训练数据的好模型。
    
    ![截屏2023-04-12 21.22.17.png](1%201%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%20cd87c4e39a364b489ccfe7e841bae72d/%25E6%2588%25AA%25E5%25B1%258F2023-04-12_21.22.17.png)
    
4. **线性回归与成本函数的关系：通过改变参数 w（或 w 和 b），我们可以得到不同的直线。当这条直线贴近数据时，代价函数 J 较小。线性回归的目标是找到一组参数（w 或 w 和 b），使得成本函数 J 的值最小。**
    
    ![截屏2023-03-12 22.02.58.png](1%201%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%20cd87c4e39a364b489ccfe7e841bae72d/%25E6%2588%25AA%25E5%25B1%258F2023-03-12_22.02.58.png)
    
    ![竖轴为J(w)、横轴为 w，J(w)随着 w 变化的曲线。](1%201%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%20cd87c4e39a364b489ccfe7e841bae72d/%25E6%2588%25AA%25E5%25B1%258F2023-03-12_22.09.36.png)
    
    竖轴为J(w)、横轴为 w，J(w)随着 w 变化的曲线。
    

在这个视频中，我们使用了一个仅包含参数 w 的简化问题。在下一个视频中，我们将展示使用参数 w 和 b 的完整线性回归问题的代价函数的可视化效果。通过观察 3D 图形，我们可以更好地理解线性回归模型的原理。

- 🔰抛物线函数：标准形式表示为 y = ax^2 + bx + c，其中a、b、c是实数常数，x和y是自变量和因变量。
    
    抛物线函数通常呈现出一个U形的图像，它的开口方向和a的正负有关。如果a为正，则抛物线开口朝上，如果a为负，则开口朝下。b是x的一次系数，可以控制抛物线图像的水平偏移。c是y的常数项，可以控制抛物线图像的垂直偏移。
    

> **本节精炼总结：**
J(w,b)：抛物线。
线性回归f(x)和成本函数 J(w,b)：当直线贴近数据时，代价函数 J 比较小。
线性回归的目标是找到一组参数（w,b）让成本函数 J 最小。
> 

## 可视化成本函数：[Visualizing the cost function](https://www.coursera.org/learn/machine-learning/lecture/QI1h6/visualizing-the-cost-function)

在这段文字中，关键知识点包括线性回归模型、模型参数、损失函数（代价函数）以及如何使用可视化技巧来理解损失函数的表现。以下是这些知识点的概述：

1. 线性回归模型：线性回归是一种简单的机器学习算法，用于根据输入特征预测数值型目标变量。线性回归模型可以表示为 f(x) = wx + b，其中x是输入特征，w是权重，b是偏置项。
    
    ![截屏2023-04-12 21.41.49.png](1%201%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%20cd87c4e39a364b489ccfe7e841bae72d/%25E6%2588%25AA%25E5%25B1%258F2023-04-12_21.41.49.png)
    
2. 模型参数：模型参数是机器学习模型中用于调整模型性能的变量。在线性回归模型中，w和b就是模型参数。
3. 损失函数（代价函数）：损失函数用于衡量模型的预测与实际值之间的差异。在线性回归中，通常使用平均平方误差作为损失函数，表示为 J(w, b)。线性回归的目标是找到一组参数w和b，使得损失函数J(w, b)的值最小。
    
    ![截屏2023-04-12 21.42.56.png](1%201%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%20cd87c4e39a364b489ccfe7e841bae72d/%25E6%2588%25AA%25E5%25B1%258F2023-04-12_21.42.56.png)
    
4. 可视化损失函数：为了更好地理解损失函数在不同参数下的表现，可以通过可视化技巧进行观察。在这里，作者提到了两种可视化方法：三维曲面图和等高线图（轮廓图）。
    - 在这个碗状函数中，**最低点表示权重和截距的最佳值，即最小化成本函数的值**。在梯度下降中，我们的目标是通过迭代，从任何起点都能到达这个最低点。在这个过程中，我们会不断地下降，直到到达最低点。如果成本函数不是凸函数，则可能存在许多局部最小值，因此找到全局最小值可能会更加困难。
        
        ![沿 w 或 b 轴切一刀，相当于上一张图的抛物线](1%201%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%20cd87c4e39a364b489ccfe7e841bae72d/%25E6%2588%25AA%25E5%25B1%258F2023-03-12_22.25.40.png)
        
        沿 w 或 b 轴切一刀，相当于上一张图的抛物线
        
    - 通过绘制等高线图来更形象地展示成本函数的形状。等高线图将成本函数的高度J(w,b)表示为Z轴，而权重(斜率)w和截距b的值分别表示为X轴和Y轴。**等高线表示成本函数的高度相等的点，也就是成本函数的等高线。**等高线越靠近最低点，表示对应的权重和截距值越接近最优值。
        
        ![截屏2023-03-12 22.25.46.png](1%201%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%20cd87c4e39a364b489ccfe7e841bae72d/%25E6%2588%25AA%25E5%25B1%258F2023-03-12_22.25.46.png)
        

通过理解这些关键知识点，您将能够更好地了解线性回归及其相关概念，从而为学习更复杂的机器学习算法奠定基础。

> **本节精炼总结：**
损失函数三维绘制：直观感受是碗（碗底J最小，沿w 或 b 轴切一刀都是抛物线）、富士山（等高线表示 J 相等）。
> 

## 可视化成本函数的例子：[Visualization examples](https://www.coursera.org/learn/machine-learning/lecture/Ov8Zt/visualization-examples)

在这段文字中，关键知识点主要包括如何使用可视化方法来理解线性回归中参数 w 和 b 的选择对模型性能的影响。以下是这些知识点的概述：

1. 参数选择的可视化：为了更好地理解参数 w 和 b 在不同取值下模型性能的变化，这里给出了4个具体的例子。通过观察这些例子，**您可以看到不同参数选择如何影响模型 f(x) 的拟合效果，以及如何与代价函数 J 的不同取值相对应。**
2. 损失函数最小化：线性回归的目标是找到一组参数 w 和 b，使得损失函数 J(w, b) 的值最小。**通过观察这些例子，您可以看到更好的拟合线对应于代价函数 J 的最小值附近的点**。
    
    ![截屏2023-03-12 22.36.28.png](1%201%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%20cd87c4e39a364b489ccfe7e841bae72d/%25E6%2588%25AA%25E5%25B1%258F2023-03-12_22.36.28.png)
    
    ![截屏2023-03-12 22.36.35.png](1%201%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%20cd87c4e39a364b489ccfe7e841bae72d/%25E6%2588%25AA%25E5%25B1%258F2023-03-12_22.36.35.png)
    
    ![截屏2023-04-12 21.59.02.png](1%201%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%20cd87c4e39a364b489ccfe7e841bae72d/%25E6%2588%25AA%25E5%25B1%258F2023-04-12_21.59.02.png)
    
    ![截屏2023-03-12 22.36.39.png](1%201%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%20cd87c4e39a364b489ccfe7e841bae72d/%25E6%2588%25AA%25E5%25B1%258F2023-03-12_22.36.39.png)
    
3. 交互式可视化：在实际应用中，您可能需要借助交互式可视化工具来观察参数 w 和 b 的选择如何影响模型性能。这可以帮助您更直观地理解参数选择与模型性能之间的关系。
4. 梯度下降算法：在线性回归中，我们需要一种高效的算法来自动找到最佳的参数 w 和 b，以最小化损失函数 J。梯度下降算法正是实现这一目标的关键方法。梯度下降及其变种不仅用于线性回归，还用于训练许多最大、最复杂的人工智能模型。

通过理解这些关键知识点，您将能够更好地了解线性回归中参数选择与模型性能之间的关系，以及如何利用梯度下降算法优化模型。这些知识将为您学习更复杂的机器学习算法奠定基础。

> **本节精炼总结：**
直观的理解 wb 的选择对 J 的影响→更好的拟合对应J 最小值附近
需要一种算法自动找到最佳参数wb，以最小化损失函数 J → 梯度下降
> 

## 实验室：[Optional lab: Cost function](https://www.coursera.org/learn/machine-learning/ungradedLab/udPHh/optional-lab-cost-function)

- 🔰**二维矢量场图（quiver plot**），它用于展示梯度（gradient）在二维空间中的表现。横轴和纵轴通常表示参数空间中的两个自变量。在机器学习和深度学习的背景下，这些自变量通常是模型参数，例如权重（w）和偏置项（b）。
    
    ![截屏2023-03-16 22.18.41.png](1%201%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%20cd87c4e39a364b489ccfe7e841bae72d/%25E6%2588%25AA%25E5%25B1%258F2023-03-16_22.18.41.png)
    
- 🔰问题图中的箭头是什么意思？
    
    梯度是一个向量，它表示一个多变量函数在某个点上的方向导数沿着坐标轴的变化。在梯度下降算法中，梯度通常用于指导参数更新的方向，以便最小化目标函数（如损失函数）。
    
    在**二维矢量场图**中，箭头表示梯度向量。**箭头的长度表示梯度的大小（即梯度的模，直角三角形的斜边），而箭头的方向表示梯度的方向（为了最小化损失函数，我们需要朝着梯度的相反方向移动，即朝着函数值下降最快的方向）**。箭头越长，表示梯度在该点的值越大，函数在该点的变化越剧烈；箭头越短，表示梯度在该点的值越小，函数在该点的变化越平缓。这张图有助于我们理解模型参数空间中梯度的表现，从而更好地理解梯度下降算法在寻找最优解时的行为。
    
- 🔰问题：这两张图看起来很像，有什么区别？
    
    ![截屏2023-03-16 23.04.49.png](1%201%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%20cd87c4e39a364b489ccfe7e841bae72d/%25E6%2588%25AA%25E5%25B1%258F2023-03-16_23.04.49.png)
    
    这两张图表示的是梯度下降过程中，代价函数（损失函数）随迭代次数的变化。它们用于展示模型在训练过程中的收敛情况。
    
    第一张图（**`ax1`**）是损失函数在开始阶段（前100次迭代）的变化情况。这张图可以帮助你观察模型训练的初期阶段，损失函数是如何下降的。
    
    第二张图（**`ax2`**）则展示了损失函数在迭代次数较大（例如：迭代1000次之后）的情况。这张图主要用于查看模型在训练过程的后期阶段，损失函数是否已经趋于稳定，进一步确认模型是否已经收敛。
    
    通过观察这两张图，你可以了解梯度下降过程中损失函数的变化趋势，从而判断模型是否收敛以及训练是否有效。
    
- 🔰部分代码解析
    
    ```python
    		#文档位置：./Supervised Machine Learning Regression and Classification/week1/work/lab_utils_uni.py	
    		
    		# ===============
        # Second Subplot
        # ===============
        # 首先，为 w 和 b 创建网格。这里，w 的范围是从 -100 到 600，共 10 个点；
        # b 的范围是从 -200 到 200，共 10 个点。tmp_b 和 tmp_w 分别是形状为 (10, 10) 的二维 numpy 数组，包含 b 和 w 的网格值。
        tmp_b, tmp_w = np.meshgrid(
            np.linspace(-200, 200, 10), np.linspace(-100, 600, 10))
        # 初始化两个和 tmp_w、tmp_b 形状相同的零数组 U 和 V，用于存储计算出的梯度值。
        U = np.zeros_like(tmp_w)
        V = np.zeros_like(tmp_b)
        # 通过双层循环，针对每个 (w, b) 点计算梯度。U[i][j] 和 V[i][j] 分别表示 w 和 b 方向上的梯度值。
        for i in range(tmp_w.shape[0]):
            for j in range(tmp_w.shape[1]):
                U[i][j], V[i][j] = f_compute_gradient(
                    x_train, y_train, tmp_w[i][j], tmp_b[i][j])
        X = tmp_w
        Y = tmp_b
        # 计算颜色数组 color_array。这个数组用于表示矢量场图中箭头的颜色。
        # 颜色是基于梯度的大小计算的。
        # 这里，我们使用 np.sqrt(((V-n)/2)**2 + ((U-n)/2)**2) 计算颜色数组，其中 n 的值为 -2。
        n = -2
        color_array = np.sqrt(((V-n)/2)**2 + ((U-n)/2)**2)
        ax[1].set_title('Gradient shown in quiver plot')
        # 使用 ax[1].quiver() 函数绘制二维矢量场图。
        # X 和 Y 是箭头的起点坐标，U 和 V 是箭头的矢量分量，color_array 是箭头的颜色。
        # units='width' 参数表示箭头的长度将根据轴的宽度进行缩放。
        Q = ax[1].quiver(X, Y, U, V, color_array, units='width', )
        ax[1].quiverkey(
            Q, 0.9, 0.9, 2, r'$2 \frac{m}{s}$', labelpos='E', coordinates='figure')
        ax[1].set_xlabel("w")
        ax[1].set_ylabel("b")
    ```
    

# 用梯度下降训练模型 :**Train the model with gradient descent**

---

## 梯度下降：[Gradient descent](https://www.coursera.org/learn/machine-learning/lecture/2f2PA/gradient-descent)

这段文字中，关键知识点主要包括梯度下降算法的概念及其在机器学习中的广泛应用。以下是这些知识点的概述：

1. 梯度下降算法（Gradient Descent  *ˈɡreɪdiənt  dəˈsɛnt* ）：**梯度下降算法是一种用于寻找使损失函数最小化的参数 w 和 b 的值的方法。**它在机器学习中得到了广泛应用，不仅用于线性回归，还用于训练最先进的神经网络模型（也称为深度学习模型）。了解梯度下降算法是机器学习中最重要的基本知识之一。
    
    ![截屏2023-03-13 21.28.50.png](1%201%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%20cd87c4e39a364b489ccfe7e841bae72d/%25E6%2588%25AA%25E5%25B1%258F2023-03-13_21.28.50.png)
    
2. 梯度下降的通用性：尽管这里讨论的是线性回归中的损失函数，但**梯度下降算法可以应用于任何函数的最小化问题，包括具有多个参数的模型的损失函数。**
3. 梯度下降的过程：在梯度下降算法中，**首先选择一组初始参数 w 和 b。然后，通过每次微调参数 w 和 b 以减小损失函数 J(w, b) 的值，直到 J 达到或接近最小值。在每一步，选择使损失函数下降最快的方向进行更新。**
    
    ![截屏2023-03-13 21.28.58.png](1%201%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%20cd87c4e39a364b489ccfe7e841bae72d/%25E6%2588%25AA%25E5%25B1%258F2023-03-13_21.28.58.png)
    
4. 局部最小值：对于某些非凸函数，可能存在多个局部最小值。**梯度下降算法可能会根据初始参数值的选择而收敛到不同的局部最小值。**这是梯度下降算法的一个有趣特性，了解这一特性对于解决更复杂的优化问题非常重要。

通过理解这些关键知识点，您将能够更好地了解梯度下降算法的基本概念和在机器学习中的重要作用。这些知识将为您学习更复杂的机器学习算法奠定基础。

> **本节精炼总结：**
梯度下降：寻找使损失函数最小化的参数 w 和 b 的值的方法；适用于任何函数的最下化问题。
梯度下降过程：初始 wb，每次微调以损失函数下降最快的方向更新 J，直到 J 达到最小值。（人坐在山头往下滑）
梯度下降存根据初始值的不同收敛到不同的局部最下化。
> 

## 实现梯度下降：[Implementing gradient descent](https://www.coursera.org/learn/machine-learning/lecture/TXDBu/implementing-gradient-descent)

这段课程讲解了梯度下降算法的实现和关键知识点。以下是一些重要概念：

1. 梯度下降算法：梯度下降是一种用于最小化目标函数（如成本函数）的算法。通过不断调整参数（如 w 和 b），使目标函数达到或接近最小值。
2. 更新规则：在每次迭代中，参数 w 和 b 按以下规则进行更新：
    - w := w - α * (dJ/dw)
    - b := b - α * (dJ/db)
    其中，α 是学习率，dJ/dw 和 dJ/db 分别是成本函数 J(w, b) 关于 w 和 b 的偏导数。
    
    ![正确的做法：相当于 2 个参数的函数，同时更新后，取更新后的结果。](1%201%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%20cd87c4e39a364b489ccfe7e841bae72d/%25E6%2588%25AA%25E5%25B1%258F2023-03-13_21.40.31.png)
    
    正确的做法：相当于 2 个参数的函数，同时更新后，取更新后的结果。
    
3. **导数（derivative）***dəˈrɪvədɪv*：函数在某一点的导数等于该点处的切线斜率。正导数表示函数单调递增，负导数表示函数单调递减，而零导数表示函数在该点处取得极值。
在梯度下降算法中，偏导数用于确定参数更新的方向。
4. **学习率（learning rate）控制每次迭代时参数更新的步长。**较大的学习率表示更大的步长，较小的学习率表示更小的步长。选择合适的学习率很重要，因为它影响了梯度下降算法的收敛速度和稳定性。
    
    如果学习率过小，那么模型的收敛速度会变得非常缓慢，需要更多的迭代次数才能收敛到最优解，同时可能会陷入局部最优解。如果学习率过大，那么模型的收敛速度会变得非常快，但可能会导致模型在最优解附近震荡甚至无法收敛。
    
5. **同步更新（Simultaneously update w and b）ˌ***saɪməlˈteɪniəsli* :  在实现梯度下降时，应同时更新参数 w 和 b。这意味着在计算新值之前，不应更改任何一个参数。同步更新有助于正确实现梯度下降算法.

- 🔰**assignment（赋值）***əˈsaɪnmənt* 是将某个值或表达式赋给变量或数据结构的过程。例如，a = 5 就是将 5 赋给变量 a。
- 🔰**truth assertion（真值断言）***əˈsərʃ(ə)n* 是一种测试代码正确性的方法，即断言某个条件应该为真。例如，assert a == 5 表示 a 应该等于 5，否则会抛出异常。
- 🔰**重复直到收敛"repeat until convergence"**  *kənˈvərdʒ(ə)ns* ****是指在机器学习中使用迭代算法来求解优化问题的一种方法。具体来说，这个方法的步骤是：
    1. 初始化模型参数，比如权重和偏置。
    2. 在每一次迭代中，计算代价函数的梯度。
    3. 使用梯度下降算法更新模型参数。
    4. 重复执行步骤 2 和步骤 3 直到代价函数的值收敛或达到最大迭代次数。
    
    其中，"repeat until convergence" 表示重复执行步骤 2 和步骤 3 直到代价函数的值收敛。这种方法的目标是使代价函数最小化，以达到更好的模型拟合效果。
    
- 🔰Correct *kəˈrɛk(t)* 正确 、 Incorrent 错误

> **本节精炼总结：**
梯度下降、同步更新
w := w - α * (dJ/dw)
b := b - α * (dJ/db)
> 

## 渐变下降直觉：[Gradient descent intuition](https://www.coursera.org/learn/machine-learning/lecture/2EoN6/gradient-descent-intuition)

这个视频解释了梯度下降算法中的导数项及其作用。以下是关键知识点：

1. 梯度下降算法的目的是通过不断调整参数 w（和 b）来最小化代价函数 J(w)。
    
    ![截屏2023-04-12 22.44.36.png](1%201%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%20cd87c4e39a364b489ccfe7e841bae72d/%25E6%2588%25AA%25E5%25B1%258F2023-04-12_22.44.36.png)
    
2. 导数项（d/dw J(w)）表示代价函数 J(w) 相对于参数 w 的变化率。它告诉我们应该朝哪个方向更新参数 w 以便更快地找到代价函数的最小值。
3. 当导数为正数时，表示函数 J(w) 在当前点的斜率为正。这意味着我们需要减小 w 的值，以便向函数的最小值移动。因此，在梯度下降更新中，我们会从 w 中减去一个正数。
w 会变小→J(w)变小
4. 当导数为负数时，表示函数 J(w) 在当前点的斜率为负。这意味着我们需要增加 w 的值，以便向函数的最小值移动。因此，在梯度下降更新中，我们会从 w 中减去一个负数（相当于加上一个正数）。
w 会变大→J(w)变小
    
    ![1 导数为斜率  2.导数为整数时（随着 w 增大 J(w)也增大），w会变小（因为 learning rate 是正，导数也是正）](1%201%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%20cd87c4e39a364b489ccfe7e841bae72d/%25E6%2588%25AA%25E5%25B1%258F2023-03-13_22.38.47.png)
    
    1 导数为斜率  2.导数为整数时（随着 w 增大 J(w)也增大），w会变小（因为 learning rate 是正，导数也是正）
    
5. 通过以上示例，我们可以看到梯度下降算法通过考虑导数项来合理地调整参数 w，使我们更接近代价函数的最小值。

在接下来的视频中，将更深入地研究学习率 Alpha，了解其作用以及如何选择一个合适的 Alpha 值。

> **本节精炼总结：**
导数dJ/dw 为斜率。切线角度离水平线越大，斜率越大，单位△w引起的△J 的值越大，即导数越大梯度下降越快。
> 

## 学习率：[Learning rate](https://www.coursera.org/learn/machine-learning/lecture/OoP3Y/learning-rate)

这个视频深入讨论了梯度下降算法中学习率（α）的作用及如何选择合适的学习率。以下是关键知识点：

1. 学习率 α 对梯度下降算法的效率影响很大。如果选择不当，梯度下降算法可能无法正常工作。
2. 如果学习率过小，梯度下降算法会以非常缓慢的速度逼近最小值。虽然最终会找到最小值，但可能需要大量迭代。
3. 如果学习率过大，梯度下降算法可能会在寻找最小值的过程中来回“跳跃”**（overshoot 超过目标）**，甚至导致发散，无法收敛到最小值。
    
    ![截屏2023-03-14 19.38.31.png](1%201%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%20cd87c4e39a364b489ccfe7e841bae72d/%25E6%2588%25AA%25E5%25B1%258F2023-03-14_19.38.31.png)
    
4. **当参数已经位于局部最小值时，梯度下降算法会自动停止更新参数。**这是因为此时导数为零，参数更新的量为零。
    
    ![截屏2023-03-14 19.38.38.png](1%201%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%20cd87c4e39a364b489ccfe7e841bae72d/%25E6%2588%25AA%25E5%25B1%258F2023-03-14_19.38.38.png)
    
5. 在逼近局部最小值时，梯度下降算法会自动采取较小的步长。即使学习率 α 保持固定，导数值会随着接近最小值而变小，从而使更新步长变小。这是因为梯度下降算法的**步长（粉色框住的部分）是由学习率乘以导数得出的**。**如果导数变小，那么每次更新的步长就会减小，这也就意味着模型参数的调整速度变慢。**
    
    ![截屏2023-03-14 19.38.44.png](1%201%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%20cd87c4e39a364b489ccfe7e841bae72d/%25E6%2588%25AA%25E5%25B1%258F2023-03-14_19.38.44.png)
    

将梯度下降算法与线性回归模型的代价函数（均方误差）结合起来，就能得到线性回归算法。这是一种常用的学习算法，可以用于优化各种代价函数。

> **本节精炼总结：**
学习率过小训练速度慢、学习率过大无法收敛。
当达到局部最小值后，梯度下降会自动停止更新参数。
逼近局部最小值时，梯度下降会自动采取较小的步长（斜率越小，单位△w引起的△J越小 ）。
> 

## 线性回归的梯度下降：[Gradient descent for linear regression](https://www.coursera.org/learn/machine-learning/lecture/lgSMj/gradient-descent-for-linear-regression)

这个视频讲解了如何将梯度下降算法应用于线性回归模型的平方误差代价函数。以下是关键知识点：

1. **线性回归模型、平方误差代价函数和梯度下降算法可以结合在一起，训练线性回归模型以拟合训练数据。**
2. 对代价函数 J 关于 w 和 b 的偏导数可以用下列公式计算：
    - **关于 w 的偏导数**：(1/m) ∑(预测值 - 实际值) * x_i
    - **关于 b 的偏导数**：(1/m) ∑(预测值 - 实际值)
        
        ![截屏2023-03-14 20.16.12.png](1%201%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%20cd87c4e39a364b489ccfe7e841bae72d/%25E6%2588%25AA%25E5%25B1%258F2023-03-14_20.16.12.png)
        
        - 🔰直觉：函数值f 与实际的 y 之间差，维度和 y 一致。
            
            ![截屏2023-04-13 09.27.08.png](1%201%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%20cd87c4e39a364b489ccfe7e841bae72d/%25E6%2588%25AA%25E5%25B1%258F2023-04-13_09.27.08.png)
            
            将差值分配到各个 x 上，维度与 x 保持一致。相当于 np 的广播、Excel 将表格中所有数据都乘以一个数值，其中行 i 不固定，列j 固定。
            
            ![截屏2023-04-13 09.27.38.png](1%201%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%20cd87c4e39a364b489ccfe7e841bae72d/%25E6%2588%25AA%25E5%25B1%258F2023-04-13_09.27.38.png)
            
            合计，方向是 x 从 i-m，相当于 Excel 数据在i-m方向合计，列j 固定。
            
            ![截屏2023-04-13 09.31.11.png](1%201%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%20cd87c4e39a364b489ccfe7e841bae72d/%25E6%2588%25AA%25E5%25B1%258F2023-04-13_09.31.11.png)
            
    - 这些公式是通过微积分推导得到的，但了解这些推导并非必须，只需知道如何使用这些公式。
        
        ![截屏2023-04-13 09.20.20.png](1%201%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%20cd87c4e39a364b489ccfe7e841bae72d/%25E6%2588%25AA%25E5%25B1%258F2023-04-13_09.20.20.png)
        
3. **将偏导数公式应用于梯度下降算法，即可在每次迭代中更新参数 w 和 b。**在每一步中，需要同时更新 w 和 b。
    
    ![截屏2023-04-13 09.20.51.png](1%201%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%20cd87c4e39a364b489ccfe7e841bae72d/%25E6%2588%25AA%25E5%25B1%258F2023-04-13_09.20.51.png)
    
4. 平方误差代价函数在线性回归中具有一个特殊性质：它是一个凸函数。这意味着代价函数是一个碗状函数，仅有一个全局最小值，没有其他局部最小值。
5. 当在凸函数上实现梯度下降时，只要学习率选择得当，算法总是会收敛到全局最小值。
    
    ![截屏2023-04-13 09.21.18.png](1%201%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%20cd87c4e39a364b489ccfe7e841bae72d/%25E6%2588%25AA%25E5%25B1%258F2023-04-13_09.21.18.png)
    

通过这些关键知识点，您现在了解了如何为线性回归实现梯度下降算法，并且了解了这个算法在凸函数中的特殊性质。

> **本节精炼总结：**
线性回归模型 f(x)、平方误差代价函数 J(w,b) 和梯度下降算法 w := w - α * (dJ/dw)
b := b - α * (dJ/db) 可以结合在一起，训练线性回归模型以拟合训练数据。
> 
> 
> ![截屏2023-04-13 09.48.07.png](1%201%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%20cd87c4e39a364b489ccfe7e841bae72d/%25E6%2588%25AA%25E5%25B1%258F2023-04-13_09.48.07.png)
> 

## 运行梯度下降

这个视频演示了如何运行线性回归的梯度下降算法，并展示了算法在实际操作中的效果。以下是关键知识点：

1. 视频展示了一个线性回归模型的示例，包括数据图（左上）、代价函数的等高线图（右上）和代价函数的曲面图（下方）。
2. 视频演示了**使用梯度下降算法进行一步更新，从而改变参数 w 和 b 的值。每次更新后，代价函数的值都会减小，模型的拟合效果会逐步改善**。
    
    ![截屏2023-04-13 09.52.45.png](1%201%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%20cd87c4e39a364b489ccfe7e841bae72d/%25E6%2588%25AA%25E5%25B1%258F2023-04-13_09.52.45.png)
    
3. 经过多次迭代，梯度下降算法会使代价函数收敛到全局最小值，得到一个较好的拟合数据的直线模型。通过这个模型，可以预测房价等应用场景中的数据。
4. 视频中演示的梯度下降过程称为批量梯度下降（Batch Gradient Descent）。批量梯度下降是指在每次梯度下降更新中，我们查看所有训练样本，而不仅仅是训练数据的一个子集。
5. 除了批量梯度下降外，还有其他版本的梯度下降算法，它们在每次更新时查看训练数据的较小子集。但在本视频中，我们使用批量梯度下降进行线性回归。
    
    ![截屏2023-04-13 09.52.12.png](1%201%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%20cd87c4e39a364b489ccfe7e841bae72d/%25E6%2588%25AA%25E5%25B1%258F2023-04-13_09.52.12.png)
    

通过学习这些关键知识点，您现在了解了如何运行线性回归的梯度下降算法，并了解了在实际操作中，梯度下降算法如何改进模型的拟合效果。

## 实验室：[Optional lab: Gradient descent](https://www.coursera.org/learn/machine-learning/ungradedLab/lE1al/optional-lab-gradient-descent)

**批量梯度下降（Batch Gradient Descent）***bætʃ* 是一种使用所有训练样本更新参数的梯度下降算法。它的基本思想是在每一次迭代中，使用所有训练数据计算损失函数对参数的偏导数，并且根据该偏导数对所有参数进行更新。由于需要使用所有的训练数据，所以批量梯度下降算法的计算成本往往比随机梯度下降和小批量梯度下降要高。但是，**批量梯度下降算法的收敛速度相对来说更为稳定，可以更好地收敛到全局最优解。**

- 🔰问题：这张图是什么意思？
    
    ![截屏2023-03-16 23.08.47.png](1%201%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%20cd87c4e39a364b489ccfe7e841bae72d/%25E6%2588%25AA%25E5%25B1%258F2023-03-16_23.08.47.png)
    
    **这个图是一个等高线图（contour plot），用于展示损失函数在不同的参数值下的表现。**在这个例子中，图中的椭圆表示损失函数的等高线，即损失函数在这些椭圆上的点具有相同的值。图中心点处的椭圆表示损失函数的最小值。
    
    这个等高线图的横轴代表参数 w（权重），纵轴代表参数 b（偏置）。椭圆越靠近图中心，损失函数的值越小；椭圆越靠近图的边缘，损失函数的值越大。
    
    通过这个等高线图，你可以观察到梯度下降算法在参数空间中的搜索过程。在这个过程中，算法会根据梯度的方向逐步更新参数值，以最小化损失函数。从而在图中找到损失函数最小值的位置，即最优的参数值。
    
- 🔰问题：下面这张图中红色的是什么意思？
    
    ![截屏2023-03-16 23.11.28.png](1%201%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%20cd87c4e39a364b489ccfe7e841bae72d/%25E6%2588%25AA%25E5%25B1%258F2023-03-16_23.11.28.png)
    

这个图是上一张图的局部放大版，用于更详细地展示损失函数在参数空间中的表现。与之前的等高线图类似，椭圆表示损失函数的等高线。**这个图关注的是权重参数 w 和偏置参数 b 的一个较小的范围，分别由 w_range 和 b_range 指定。**在这个例子中，w 的范围是 [180, 220, 0.5]，b 的范围是 [80, 120, 0.5]。resolution 参数决定了图中等高线的密度。

红色的曲线（箭头）表示损失函数在指定范围内的等高线。contours 参数指定了要显示的等高线的损失值列表，这里是 [1, 5, 10, 20]。

图中的纵轴和横轴分别表示偏置参数 b 和权重参数 w。红色曲线和纵轴 100、横轴 200 的线表示在这个参数范围内，梯度梯度下降法在尝试寻找损失函数的最小值。纵轴 100 和横轴 200 的线表示参数空间的某个特定点，梯度下降算法会沿着损失函数的负梯度方向更新参数，以逐步接近损失函数的最小值。

- 🔰问题：这里的红色箭头和前面有一个实验中的彩色箭头是什么关系？
    
    ![截屏2023-04-13 09.58.12.png](1%201%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%20cd87c4e39a364b489ccfe7e841bae72d/%25E6%2588%25AA%25E5%25B1%258F2023-04-13_09.58.12.png)
    
    红色的箭头和前面提到的"plt_gradients(x_train,y_train, compute_cost, compute_gradient)plt.show()"这份代码里的第二张图，有很多彩色箭头的这个图。**两张图中的箭头是相反的意思，红色箭头代表了彩色箭头的反方向。**
    
    两个图中的箭头都代表梯度，**彩色箭头表示损失函数在各个点的梯度方向。梯度方向是函数值增加最快的方向。**而在 "plt_contour_wgrad" 函数中绘制的**红色箭头，实际上表示的是参数更新的方向，也就是梯度的反方向。**因为我们希望最小化损失函数，所以需要沿着梯度的反方向更新参数。这就是为什么红色箭头代表了彩色箭头的反方向。
    
- 🔰问题：粉色的线是什么意思？
    
    ![截屏2023-03-17 00.15.44.png](1%201%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%20cd87c4e39a364b489ccfe7e841bae72d/%25E6%2588%25AA%25E5%25B1%258F2023-03-17_00.15.44.png)
    
    **`plt_divergence(p_hist, J_hist, x_train, y_train)`** 函数绘制了两张图，分别展示了梯度下降过程中参数（权重和偏置）的更新情况和损失函数值的变化。
    
    这两张图都展示了当学习率过大时，损失函数的值（代价）可能会上升（即梯度下降方法可能不会收敛）。
    
    1. 第一张图（二维）：这张图的 x 轴表示权重 w 的值，y 轴表示损失函数值（代价）。图中的折线显示了当偏置 b 固定为 100 时，损失函数值随权重 w 的变化情况。从图中可以看出，当学习率过大时，损失函数值可能不会收敛，反而会逐渐上升。
    2. 第二张图（三维）：这张图的 x 轴表示权重 w 的值，y 轴表示偏置 b 的值，z 轴表示损失函数值（代价）。这个三维图展示了损失函数值随着权重 w 和偏置 b 的更新而变化的情况。从图中可以看出，当学习率过大时，损失函数值可能不会收敛，反而会逐渐上升。