# 1.2 监督学习：多输入变量的回归问题

復習: Done
日付: 2023年3月15日
最終更新日時: 2023年4月14日 10:43
種別: 授業

# 多特征线性回归模型： ****Multiple linear regression

## 多特征：[Multiple features](https://www.coursera.org/learn/machine-learning/lecture/gFuSx/multiple-features)

欢迎回来！在这周的课程中，我们将学习如何让线性回归更快速、更强大。本周结束后，你将完成这门课程的三分之二。现在我们来看一下如何将线性回归应用于不仅仅一个特征，而是多个特征。以下是关键知识点的总结：

- **多元线性回归（Multiple Linear Regression）：与单变量线性回归相比，多元线性回归模型考虑多个特征。**在本例中，我们考虑了房屋尺寸、卧室数量、楼层数和房屋年龄四个特征。通过这些特征，我们能更好地预测房价。
    
    ![截屏2023-04-13 20.22.33.png](1%202%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%A4%9A%E8%BE%93%E5%85%A5%E5%8F%98%E9%87%8F%E7%9A%84%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98%204dadce4d8a89447881480f0324437c1e/%25E6%2588%25AA%25E5%25B1%258F2023-04-13_20.22.33.png)
    
- 符号表示：为表示多个特征，我们引入新的符号表示。例如，使用 X_1、X_2、X_3 和 X_4 表示四个特征，使用小写字母 n 表示特征总数（在这个例子中，n 等于 4）。使用 X^i 表示第 i 个训练样本，其中 X^i 是一个包含 i 个训练样本所有特征的向量。
    
    ![x 的上标i为自身循环（同一个特征值，如房屋的面积），下标j为外层函数循环（多个特征值）](1%202%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%A4%9A%E8%BE%93%E5%85%A5%E5%8F%98%E9%87%8F%E7%9A%84%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98%204dadce4d8a89447881480f0324437c1e/%25E6%2588%25AA%25E5%25B1%258F2023-03-14_21.05.02.png)
    
    x 的上标i为自身循环（同一个特征值，如房屋的面积），下标j为外层函数循环（多个特征值）
    
- 多元线性回归模型：在原始线性回归模型中，模型是 fwb(x) = wx + b，其中 x 是一个特征（单个数字）。现在，我们需要考虑多个特征，所以我们将模型定义为 fwb(X) = w1x1 + w2x2 + w3x3 + w4x4 + b。这里，w1、w2、w3 和 w4 是权重，b 是偏置项。
    
    ![截屏2023-04-13 20.25.22.png](1%202%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%A4%9A%E8%BE%93%E5%85%A5%E5%8F%98%E9%87%8F%E7%9A%84%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98%204dadce4d8a89447881480f0324437c1e/%25E6%2588%25AA%25E5%25B1%258F2023-04-13_20.25.22.png)
    
- 向量表示：我们可以将**权重（w1、w2、w3 和 w4）和特征（x1、x2、x3 和 x4）表示为向量 W 和 X。**这使我们能够用简洁的方式表示模型：**f(X) = W • X + b。这里的点乘（•）表示两个向量的对应元素相乘后求和。**
    
    ![截屏2023-04-13 20.25.43.png](1%202%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%A4%9A%E8%BE%93%E5%85%A5%E5%8F%98%E9%87%8F%E7%9A%84%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98%204dadce4d8a89447881480f0324437c1e/%25E6%2588%25AA%25E5%25B1%258F2023-04-13_20.25.43.png)
    
    - 🔰直观理解：按Excel 表单理解如下图，W 固定列后与 X 相乘，最后求和。
        
        ![截屏2023-04-13 21.23.57.png](1%202%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%A4%9A%E8%BE%93%E5%85%A5%E5%8F%98%E9%87%8F%E7%9A%84%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98%204dadce4d8a89447881480f0324437c1e/%25E6%2588%25AA%25E5%25B1%258F2023-04-13_21.23.57.png)
        
- 向量化：为了简化实现多元线性回归以及其他许多学习算法，我们可以使用一种叫做向量化的技巧。向量化允许我们更紧凑地表示模型，并利用矩阵计算的优势提高计算效率。

以上就是本周关于多元线性回归的关键知识点。希望这些解释对您有所帮助。下一节课将介绍向量化，以便更简洁、高效地实现多元线性回归。

> **本节一句话总结：**
多元线性回归（Multiple Linear Regression）使用多个特征。
模型为 f(X)=W·X+b，点积代表对应向量相乘后求和。
每个 i 对应一个 y。
> 

## 向量化：[Vectorization](https://www.coursera.org/learn/machine-learning/lecture/ismjc/vectorization-part-1)

向量化是一种通过使用向量和矩阵来实现更高效代码的技术。**使用向量化，我们可以避免显式使用循环来操作数组中的元素，这可以加快代码的运行速度并减少实现的复杂性。**

在实现机器学习算法时，我们通常使用向量化操作来对整个数据集执行相同的计算。例如，我们可以使用向量化操作计算成本函数，执行梯度下降等。

![**点积（Dot Product）**是指两个向量相乘并求和的运算。](1%202%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%A4%9A%E8%BE%93%E5%85%A5%E5%8F%98%E9%87%8F%E7%9A%84%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98%204dadce4d8a89447881480f0324437c1e/%25E6%2588%25AA%25E5%25B1%258F2023-03-14_21.40.05.png)

**点积（Dot Product）**是指两个向量相乘并求和的运算。

> **本节一句话总结：**
向量化可以通过 NumPy计算，避免显示循环
> 

## 向量化 2：[Vectorization](https://www.coursera.org/learn/machine-learning/lecture/p2Nqv/vectorization-part-2)

- 向量化使代码更短、更易于编写和阅读。
- 向量化可以提高代码运行效率，因为它可以利用计算机中的并行硬件。
- 在 Python 中，NumPy 是最广泛使用的数值线性代数库。
- 向量化实现（如 NumPy 的 dot 函数）可以利用 CPU 或 GPU 的并行计算能力。
- 使用向量化避免了使用循环和逐个计算的低效率问题。

![截屏2023-03-14 20.54.51.png](1%202%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%A4%9A%E8%BE%93%E5%85%A5%E5%8F%98%E9%87%8F%E7%9A%84%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98%204dadce4d8a89447881480f0324437c1e/%25E6%2588%25AA%25E5%25B1%258F2023-03-14_20.54.51.png)

通过一个具体的例子（多元线性回归）来展示了如何利用向量化进行计算。在这个例子中，作者提到了用 NumPy 实现向量化的优势，例如在更新参数时可以大大减少计算时间。使用向量化实现的线性回归会比非向量化实现更高效。如果特征数目很大，向量化实现对学习算法的运行时间的影响将非常显著。

![截屏2023-03-14 20.55.01.png](1%202%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%A4%9A%E8%BE%93%E5%85%A5%E5%8F%98%E9%87%8F%E7%9A%84%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98%204dadce4d8a89447881480f0324437c1e/%25E6%2588%25AA%25E5%25B1%258F2023-03-14_20.55.01.png)

## 实验室：[Python, NumPy and vectorization](https://www.coursera.org/learn/machine-learning/ungradedLab/zadmO/optional-lab-python-numpy-and-vectorization)

当我们使用Python等高级编程语言时，这些向量化操作通常由内置库和函数提供，例如NumPy库。因此，理解向量化的概念以及如何使用它可以帮助我们更好地实现和优化机器学习算法。

- 代码实现：
    
    ![截屏2023-03-17 11.31.19.png](1%202%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%A4%9A%E8%BE%93%E5%85%A5%E5%8F%98%E9%87%8F%E7%9A%84%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98%204dadce4d8a89447881480f0324437c1e/%25E6%2588%25AA%25E5%25B1%258F2023-03-17_11.31.19.png)
    
    **`w_init shape: (4,)`**表示 **`w_init`** 是一个一维数组，它有4个元素。逗号和空白是为了明确表示这是一个一维数组。
    
    **`f_wb`** 的形状为空，因为它是一个标量（scalar）值，而不是一个多维数组。**`f_wb`**
     是一个预测值，是通过应用线性回归模型（权重矩阵、偏置项和输入特征）计算得到的单一数值。在 numpy 中，标量值没有形状（shape）信息，因此返回空的 shape。
    

## 多元线性回归的梯度下降算法**:** [Gradient descent for multiple linear regression](https://www.coursera.org/learn/machine-learning/lecture/ltMMp/gradient-descent-for-multiple-linear-regression)

- 多元线性回归：多元线性回归是在具有多个特征的情况下进行线性回归的推广。这使得我们可以使用更多的输入特征来预测目标值。
- 向量表示法：为了简化表示，我们使用向量表示法将参数 w_1 到 w_n 收集到一个向量 w 中，使得 w 是一个长度为 n 的向量。代价函数 J 可以定义为 J(w, b)，其中 w 是一个参数向量，b 是一个数值。
    
    ![截屏2023-03-14 20.55.58.png](1%202%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%A4%9A%E8%BE%93%E5%85%A5%E5%8F%98%E9%87%8F%E7%9A%84%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98%204dadce4d8a89447881480f0324437c1e/%25E6%2588%25AA%25E5%25B1%258F2023-03-14_20.55.58.png)
    
    - 🔰Excel 理解：J(w,b)相当于在
        
        ![截屏2023-04-13 21.33.16.png](1%202%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%A4%9A%E8%BE%93%E5%85%A5%E5%8F%98%E9%87%8F%E7%9A%84%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98%204dadce4d8a89447881480f0324437c1e/%25E6%2588%25AA%25E5%25B1%258F2023-04-13_21.33.16.png)
        
- 梯度下降法：在多元线性回归中**，我们使用梯度下降法来找到最佳参数 w 和 b，以最小化代价函数 J。**梯度下降法通过迭代地更新每个参数 w_j 和 b 以减小 J 的值来实现。
    
    ![截屏2023-03-14 22.03.41.png](1%202%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%A4%9A%E8%BE%93%E5%85%A5%E5%8F%98%E9%87%8F%E7%9A%84%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98%204dadce4d8a89447881480f0324437c1e/%25E6%2588%25AA%25E5%25B1%258F2023-03-14_22.03.41.png)
    
    - 🔰Excel 理解：更新梯度。**i取值1~m，累加后取平均相当于把 i 这个方向的值摞在一起后压扁。**
        
        ![截屏2023-04-13 21.39.21.png](1%202%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%A4%9A%E8%BE%93%E5%85%A5%E5%8F%98%E9%87%8F%E7%9A%84%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98%204dadce4d8a89447881480f0324437c1e/%25E6%2588%25AA%25E5%25B1%258F2023-04-13_21.39.21.png)
        
        ![截屏2023-04-13 21.39.30.png](1%202%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%A4%9A%E8%BE%93%E5%85%A5%E5%8F%98%E9%87%8F%E7%9A%84%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98%204dadce4d8a89447881480f0324437c1e/%25E6%2588%25AA%25E5%25B1%258F2023-04-13_21.39.30.png)
        
- 向量化：在实现梯度下降法时，我们可以使用向量化来提高计算效率。向量化是一种利用数组运算代替循环的技术，使得我们可以在硬件级别并行计算大量操作，从而大大加快算法的运行速度。
- 正规方程法（**Normal equation**）：正规方程法是求解线性回归中 w 和 b 的另一种方法。与梯度下降法不同，正规方程法不需要迭代，而是直接通过一次计算来求解 w 和 b。正规方程法的局限性在于，它**只适用于线性回归问题**，而且**在特征数量较大时计算速度较慢**。
    
    ![截屏2023-03-14 20.56.14.png](1%202%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%A4%9A%E8%BE%93%E5%85%A5%E5%8F%98%E9%87%8F%E7%9A%84%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98%204dadce4d8a89447881480f0324437c1e/%25E6%2588%25AA%25E5%25B1%258F2023-03-14_20.56.14.png)
    

> **本节一句话总结：**
求和符号 i取值1~m，相当于把 i 这个方向的值摞在一起。
多元线性回归的梯度下降，用向量化的方式表示W。
每个 i 对应一个 y，全部 y 对应一个 J和一个 W。 输入数据为全部 i 对应的 Y，条件为最小化J，找出一个W、b。
> 

## 实验室：**Multiple linear regression**

- 🔰先算一行的，按行累加。最后取平均值
    
    ![截屏2023-04-13 22.28.31.png](1%202%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%A4%9A%E8%BE%93%E5%85%A5%E5%8F%98%E9%87%8F%E7%9A%84%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98%204dadce4d8a89447881480f0324437c1e/%25E6%2588%25AA%25E5%25B1%258F2023-04-13_22.28.31.png)
    
- 🔰直接用 np 函数的写法：
    
    ![截屏2023-04-13 22.36.18.png](1%202%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%A4%9A%E8%BE%93%E5%85%A5%E5%8F%98%E9%87%8F%E7%9A%84%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98%204dadce4d8a89447881480f0324437c1e/%25E6%2588%25AA%25E5%25B1%258F2023-04-13_22.36.18.png)
    
    - 🔰问题：np.dot 和* 的区别是什么？
        
        ![截屏2023-04-13 22.43.59.png](1%202%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%A4%9A%E8%BE%93%E5%85%A5%E5%8F%98%E9%87%8F%E7%9A%84%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98%204dadce4d8a89447881480f0324437c1e/%25E6%2588%25AA%25E5%25B1%258F2023-04-13_22.43.59.png)
        
        ![截屏2023-04-13 22.48.43.png](1%202%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%A4%9A%E8%BE%93%E5%85%A5%E5%8F%98%E9%87%8F%E7%9A%84%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98%204dadce4d8a89447881480f0324437c1e/%25E6%2588%25AA%25E5%25B1%258F2023-04-13_22.48.43.png)
        

![截屏2023-03-17 20.26.35.png](1%202%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%A4%9A%E8%BE%93%E5%85%A5%E5%8F%98%E9%87%8F%E7%9A%84%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98%204dadce4d8a89447881480f0324437c1e/%25E6%2588%25AA%25E5%25B1%258F2023-03-17_20.26.35.png)

- 关键点解释
    
    **`gradient_descent`** 函数定义：这是执行批量梯度下降算法的主函数。它接受训练数据（**`X`** 和 **`y`**）、初始模型参数（**`w_in`** 和 **`b_in`**）、成本函数（**`cost_function`**）、梯度计算函数（**`gradient_function`**）、学习率（**`alpha`**）以及迭代次数（**`num_iters`**）作为输入。
    
    在函数内部，首先对初始参数 **`w_in`** 和 **`b_in`** 进行深度复制，以避免在函数内部修改全局参数。
    
    通过一个循环（共 **`num_iters`** 次迭代），在每次迭代中计算梯度（**`dj_db`** 和 **`dj_dw`**），然后用学习率 **`alpha`** 更新模型参数（**`w`** 和 **`b`**）。
    
    将每次迭代的成本存储在 **`J_history`** 列表中，以便稍后绘制成本随迭代次数变化的曲线。同时，每隔一定的迭代次数（总共打印 10 次），输出当前迭代次数和成本。
    
    函数返回更新后的模型参数（**`w`** 和 **`b`**）以及成本历史记录（**`J_history`**）。
    

**调用梯度下降函数**

![截屏2023-03-17 20.28.10.png](1%202%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%A4%9A%E8%BE%93%E5%85%A5%E5%8F%98%E9%87%8F%E7%9A%84%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98%204dadce4d8a89447881480f0324437c1e/%25E6%2588%25AA%25E5%25B1%258F2023-03-17_20.28.10.png)

- 过程解释
    1. 将初始权重 **`w`** 设为零向量，与 **`w_init`** 具有相同的形状，并将初始偏置 **`b`** 设为 0。
    2. 设置迭代次数（**`iterations`**）和学习率（**`alpha`**）。
    3. 使用 **`gradient_descent`** 函数运行梯度下降算法，并获取最终的模型参数 **`w_final`** 和 **`b_final`**，以及成本历史记录 **`J_hist`**。
    4. 输出找到的模型参数 **`w_final`** 和 **`b_final`**。
    5. 使用最终的模型参数对训练数据进行预测，并与实际目标值（**`y_train`**）进行比较。

**绘制成本与迭代**

![截屏2023-03-17 20.26.54.png](1%202%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%A4%9A%E8%BE%93%E5%85%A5%E5%8F%98%E9%87%8F%E7%9A%84%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98%204dadce4d8a89447881480f0324437c1e/%25E6%2588%25AA%25E5%25B1%258F2023-03-17_20.26.54.png)

- key word
    1. **`ax1`** 是第一个子图，显示了整个迭代过程中成本函数的变化。从图中可以看出成本函数随着迭代次数的增加而减小，这意味着梯度下降算法正在优化模型参数。
    2. **`ax2`** 是第二个子图，只显示从第 100 次迭代开始的成本函数变化。这个子图放大了尾部的成本变化情况，以便更清楚地观察在迭代过程后期成本函数的变化趋势。这有助于我们了解算法在后期是否已经收敛（即成本变化很小）。
    3. 在图1中，由于纵轴尺寸较大，成本函数下降的变化可能不太明显，看起来像一条横线。然而，实际上成本函数值仍在下降，只是变化幅度相对较小。图 2是图 1 的放大版，图 2通过放大横轴的一部分范围，使我们能更清楚地观察到成本函数值的下降趋势。

> **本节一句话总结：
`np.dot`**  对于 1 维数组来说，各项相乘后求和
* 各项相乘，和 Excel 中的各项相乘一样。
向量的计算，相当于 Excel中两张表的每个单元格，都做了相同的运算，如加减乘除。
> 

---

# 梯度下降的实践：Gradient descent in practice

## 特征缩放 1：[Feature scaling](https://www.coursera.org/learn/machine-learning/lecture/KMDV3/feature-scaling-part-1)

在这个课程中，主要讲解了特征缩放（Feature Scaling）对于提高梯度下降算法效率的重要性。以下是关键知识点：

- 不同特征值范围：当我们使用不同特征进行预测时，它们的取值范围可能差异很大，例如房屋面积和卧室数量。
    
    ![截屏2023-03-14 22.21.52.png](1%202%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%A4%9A%E8%BE%93%E5%85%A5%E5%8F%98%E9%87%8F%E7%9A%84%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98%204dadce4d8a89447881480f0324437c1e/%25E6%2588%25AA%25E5%25B1%258F2023-03-14_22.21.52.png)
    
- 参数大小关系：当特征值的范围较大时，我们需要选择相对较小的参数值；而特征值范围较小时，参数值可能会相对较大。
    
    ![截屏2023-03-14 22.22.08.png](1%202%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%A4%9A%E8%BE%93%E5%85%A5%E5%8F%98%E9%87%8F%E7%9A%84%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98%204dadce4d8a89447881480f0324437c1e/%25E6%2588%25AA%25E5%25B1%258F2023-03-14_22.22.08.png)
    
- **梯度下降的表现：当特征值范围相差很大时，损失函数的等高线可能呈椭圆形，导致梯度下降算法在寻找全局最小值时需要较长时间。**
- **特征缩放**：为了解决这个问题，可以对特征进行缩放，使得它们的取值范围相对相似。这样做可以使损**失函数的等高线更接近圆形，从而加速梯度下降算法**。
    
    ![截屏2023-03-14 22.22.18.png](1%202%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%A4%9A%E8%BE%93%E5%85%A5%E5%8F%98%E9%87%8F%E7%9A%84%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98%204dadce4d8a89447881480f0324437c1e/%25E6%2588%25AA%25E5%25B1%258F2023-03-14_22.22.18.png)
    

在下一个视频中，将介绍如何实际应用特征缩放。

> **本节一句话总结：**
当特征范围相差很大时，损失函数等高线可能呈现椭圆形，造成梯度下降算法性能下降。
特征缩放解决这个问题。
> 

## 特征缩放 2：[Feature scaling part 2](https://www.coursera.org/learn/machine-learning/lecture/akapu/feature-scaling-part-2)

在这个课程中，讲解了如何实现特征缩放，以便将取值范围不同的特征调整为相似的范围。以下是关键知识点：

- **缩放特征：可以通过除以最大值的方法来缩放特征。**例如，将 x1 的每个值除以 2000，将 x2 的每个值除以 5。
    
    ![每个x 都除以最大值](1%202%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%A4%9A%E8%BE%93%E5%85%A5%E5%8F%98%E9%87%8F%E7%9A%84%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98%204dadce4d8a89447881480f0324437c1e/%25E6%2588%25AA%25E5%25B1%258F2023-03-14_22.22.32.png)
    
    每个x 都除以最大值
    
- **均值归一化（Mean Normalization）**：将特征重新缩放，使它们的均值为0。为了实现这一点，需要减去每个特征的均值，然后除以最大值和最小值之间的差值。
    
    ![1.平均值 (2000-300)/2=850,不等于 600，那是所有实际x 的相加后取平局值？  2.每个 x都除以 max-min](1%202%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%A4%9A%E8%BE%93%E5%85%A5%E5%8F%98%E9%87%8F%E7%9A%84%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98%204dadce4d8a89447881480f0324437c1e/%25E6%2588%25AA%25E5%25B1%258F2023-03-14_22.22.43.png)
    
    1.平均值 (2000-300)/2=850,不等于 600，那是所有实际x 的相加后取平局值？  2.每个 x都除以 max-min
    
- **Z得分归一化（Z-score Normalization）**：这种方法需要计算每个特征的标准差。然后，对于每个特征值，减去其均值并除以其标准差。
    
    ![截屏2023-03-14 22.22.53.png](1%202%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%A4%9A%E8%BE%93%E5%85%A5%E5%8F%98%E9%87%8F%E7%9A%84%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98%204dadce4d8a89447881480f0324437c1e/%25E6%2588%25AA%25E5%25B1%258F2023-03-14_22.22.53.png)
    
- **特征缩放目标：**通常情况下，我们希望特征值范围在大约 -1 到 1 之间。这些值可以有一定的灵活性，例如 -3 到 3 或者 -0.3 到 0.3 也是可以接受的。
    
    ![截屏2023-03-14 22.23.02.png](1%202%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%A4%9A%E8%BE%93%E5%85%A5%E5%8F%98%E9%87%8F%E7%9A%84%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98%204dadce4d8a89447881480f0324437c1e/%25E6%2588%25AA%25E5%25B1%258F2023-03-14_22.23.02.png)
    
- 何时使用特征缩放：当特征的范围相差较大时，使用特征缩放可以帮助加速梯度下降算法。在大多数情况下，实施特征缩放是无害的，因此在有疑虑的情况下，建议实施特征缩放。

通过使用特征缩放，你通常能够使梯度下降算法运行得更快。在下一个视频中，将讨论如何判断梯度下降是否收敛，并在之后的视频中讨论如何为梯度下降选择一个好的学习率。

- 🔰关于 **均值归一化（Mean Normalization）** 的知识补充：
    
    **标准差（standard deviation）是衡量一组数据离散程度的常用指标，是方差的算术平方根。**标准差越大，说明数据的离散程度越大，数据点更分散；标准差越小，说明数据的离散程度越小，数据点更集中。标准差的计算公式为所有数据与均值之差的平方和除以样本大小的平方根。
    
    【【官方双语】但是什么是中心极限定理？-哔哩哔哩】 [https://b23.tv/eiDs54R](https://b23.tv/eiDs54R)
    
    ![4A7EF9A4-7A71-4CDF-BE02-C39527705F8D_1_105_c.jpeg](1%202%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%A4%9A%E8%BE%93%E5%85%A5%E5%8F%98%E9%87%8F%E7%9A%84%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98%204dadce4d8a89447881480f0324437c1e/4A7EF9A4-7A71-4CDF-BE02-C39527705F8D_1_105_c.jpeg)
    
    ![172EFCB8-4F51-42CF-B1A2-82FB135AA05E_1_105_c.jpeg](1%202%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%A4%9A%E8%BE%93%E5%85%A5%E5%8F%98%E9%87%8F%E7%9A%84%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98%204dadce4d8a89447881480f0324437c1e/172EFCB8-4F51-42CF-B1A2-82FB135AA05E_1_105_c.jpeg)
    
    ![3F6CD30F-A53D-436C-981B-EE5BE8258471_1_105_c.jpeg](1%202%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%A4%9A%E8%BE%93%E5%85%A5%E5%8F%98%E9%87%8F%E7%9A%84%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98%204dadce4d8a89447881480f0324437c1e/3F6CD30F-A53D-436C-981B-EE5BE8258471_1_105_c.jpeg)
    
    ![93DDE5EA-DE38-48AE-AF4E-71E419F53AAC_1_105_c.jpeg](1%202%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%A4%9A%E8%BE%93%E5%85%A5%E5%8F%98%E9%87%8F%E7%9A%84%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98%204dadce4d8a89447881480f0324437c1e/93DDE5EA-DE38-48AE-AF4E-71E419F53AAC_1_105_c.jpeg)
    
    ![0E62BA8E-DD7D-40A1-ACA9-7B310691114F_1_105_c.jpeg](1%202%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%A4%9A%E8%BE%93%E5%85%A5%E5%8F%98%E9%87%8F%E7%9A%84%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98%204dadce4d8a89447881480f0324437c1e/0E62BA8E-DD7D-40A1-ACA9-7B310691114F_1_105_c.jpeg)
    
    ![10A05DF2-253E-466E-876F-C651696CF1CB_1_105_c.jpeg](1%202%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%A4%9A%E8%BE%93%E5%85%A5%E5%8F%98%E9%87%8F%E7%9A%84%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98%204dadce4d8a89447881480f0324437c1e/10A05DF2-253E-466E-876F-C651696CF1CB_1_105_c.jpeg)
    

> **本节一句话总结：**
缩放特征：x/最大值
Mean Normalization：x-标准差/（最大-最小）
Z-score Normalization：x-标准差/标准差
> 

## 检查梯度下降是否收敛：[Checking gradient descent for convergence](https://www.coursera.org/learn/machine-learning/lecture/rOTkB/checking-gradient-descent-for-convergence)

在这个课程中，讲解了如何确定梯度下降是否收敛，即是否找到了接近成本函数全局最小值的参数。以下是关键知识点：

![截屏2023-03-17 20.58.08.png](1%202%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%A4%9A%E8%BE%93%E5%85%A5%E5%8F%98%E9%87%8F%E7%9A%84%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98%204dadce4d8a89447881480f0324437c1e/%25E6%2588%25AA%25E5%25B1%258F2023-03-17_20.58.08.png)

- **绘制学习曲线**：在每次梯度下降迭代后，绘制成本函数 J 的值。**水平轴表示梯度下降的迭代次数，垂直轴表示成本函数 J 的值。**
- **观察学习曲线**：当梯度下降运行正确时，**成本函数 J 应该在每次迭代后都减少**。如果 J 在某次迭代后增加，这意味着学习率 Alpha 可能选择不当（通常过大），或者代码中可能存在错误。
- 判断梯度下降收敛：观察学习曲线，**当曲线趋于平缓且不再降低时，可以认为梯度下降已经收敛。**
- **自动收敛测试**：**设定一个小的阈值（如 0.001 或 10^-3），如果成本函数 J 在一次迭代中减少的量小于这个阈值，可以认为已经收敛。**但是，选择合适的阈值可能比较困难，因此通常更倾向于观察学习曲线，而不是依赖自动收敛测试。
    
    ![截屏2023-03-17 20.58.19.png](1%202%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%A4%9A%E8%BE%93%E5%85%A5%E5%8F%98%E9%87%8F%E7%9A%84%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98%204dadce4d8a89447881480f0324437c1e/%25E6%2588%25AA%25E5%25B1%258F2023-03-17_20.58.19.png)
    
- 🔰“Declare convergence” *dəˈklɛr  kənˈvərdʒ(ə)ns* 是指在迭代过程中，当满足某种停止条件时，可以认为算法已经收敛。

> **本节一句话总结：**
检查梯度下降：绘制曲线，观察曲线趋势，J 应该越来越小。
设置收敛阈值
> 

## 学习率的选择：[Choosing the learning rate](https://www.coursera.org/learn/machine-learning/lecture/10ZVv/choosing-the-learning-rate)

在这个课程中，讲解了如何为模型选择合适的学习率。学习率过小会导致梯度下降运行速度非常慢，而学习率过大可能导致不收敛。以下是关键知识点：

- 当成本函数 J 在梯度下降的迭代过程中时而增加时而减少，这意味着梯度下降没有正常工作。这可能是代码中的错误，或者学习率太大。
- **学习率过大可能导致在参数空间中“跳过”最小值，从而无法收敛。**为了解决这个问题，可以**尝试使用较小的学习率**。
- 如果成本函**数 J 在每次迭代后持续增加，这也可能是由于学习率过大。降低学习率可以解决这个问题。**但是，这种情况也可能是代码错误的迹象。
    
    ![截屏2023-03-17 20.59.36.png](1%202%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%A4%9A%E8%BE%93%E5%85%A5%E5%8F%98%E9%87%8F%E7%9A%84%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98%204dadce4d8a89447881480f0324437c1e/%25E6%2588%25AA%25E5%25B1%258F2023-03-17_20.59.36.png)
    
- **一个调试梯度下降的技巧是使用足够小的学习率**，这样成本函数 J 应该在每次迭代后都减少。如果即使将学习率设定为很小的值，成本函数 J 仍然没有在每次迭代后减少，那么可能代码中存在错误。
- **选择合适的学习率通常需要尝试一系列值。可以从较小的值开始（例如 0.001），然后逐渐增大（例如 0.003, 0.01, 0.03 等），观察哪个学习率能够使成本函数 J 快速且稳定地下降。**
    
    ![截屏2023-03-17 20.59.44.png](1%202%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%A4%9A%E8%BE%93%E5%85%A5%E5%8F%98%E9%87%8F%E7%9A%84%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98%204dadce4d8a89447881480f0324437c1e/%25E6%2588%25AA%25E5%25B1%258F2023-03-17_20.59.44.png)
    
- 通过尝试不同的学习率，可以找到一个较大的合理值，从而提高梯度下降的效率。

通过本课程，你应该了解了如何选择合适的学习率，以及学习率对梯度下降算法的影响。在后续课程中，还将学习如何选择自定义特征，以便在数据中拟合曲线，而不仅仅是直线。

> **本节一句话总结：**
当成本函数在梯度下降的过程中，没有逐渐减小，请减少学习率试试。
合适的学习率需要尝试，一般从较小的值开始，0.001,0.01,0.1…
> 

## 实验室：[Feature scaling and learning rate](https://www.coursera.org/learn/machine-learning/ungradedLab/kIf25/optional-lab-feature-scaling-and-learning-rate)

- 👾**遇到的BUG**
    
    原因：当前的Python环境中没有安装**`scipy`**库。要解决这个问题，您需要先安装**`scipy`**库。请打开一个命令行窗口（或终端），然后运行以下命令来安装**`scipy`**库：
    
    ```jsx
    pip install scipy
    ```
    
    ![截屏2023-03-17 21.23.12.png](1%202%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%A4%9A%E8%BE%93%E5%85%A5%E5%8F%98%E9%87%8F%E7%9A%84%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98%204dadce4d8a89447881480f0324437c1e/%25E6%2588%25AA%25E5%25B1%258F2023-03-17_21.23.12.png)
    

### **问题：数字和之子线是什么意思？**

![截屏2023-03-17 21.38.48.png](1%202%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%A4%9A%E8%BE%93%E5%85%A5%E5%8F%98%E9%87%8F%E7%9A%84%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98%204dadce4d8a89447881480f0324437c1e/%25E6%2588%25AA%25E5%25B1%258F2023-03-17_21.38.48.png)

- 🔰1.37435e+06 这种数字的表示方式是什么意思？怎么看出来数字变大还是变小？
    
    这种数字表示方式叫做科学计数法（scientific notation）。科学计数法用于表示非常大或非常小的数字。科学计数法中的**`e`**表示指数（exponent），它后面的数字（在本例中为+06）表示10的幂。在这个例子中，**`1.37435 * 10^6`**表示1.37435乘以10的6次方（即1,000,000）。例如：
    
    - 3.5e+3 和 2.7e+4：因为e后的数字3和4不同，所以直接比较它们。4大于3，因此2.7e+4比3.5e+3大。
    - 4.2e+6 和 3.9e+6：因为e后的数字6和6相同，所以比较小数部分。4.2大于3.9，因此4.2e+6比3.9e+6大。
- 🔰5.5e-01 -8.8e-02 -1.0e+03 1.2e+03 怎么看出来数字变大还是变小？
    
    要比较这种表示法中的数字大小，您可以**首先小数部分符号相同的互相比较；在看指数（即`e`后面的数字），较大的指数意味着较大的数。如果指数相同，那么比较小数部分。**从小到大排序如下：
    
    1. -1.0e+03：指数为 3（e+3），尾数为 -1.0：-1000
    2. -8.8e-02：指数为 -2（e-2），尾数为 -8.8：-0.088
    3. 5.5e-01：指数为 -1（e-1），尾数为 5.5：0.55
    4. 1.2e+03：指数为 3（e+3），尾数为 1.2：1200
- 📌程序关键点解释
    
    当多个特征值的尺度差别很大时，模型参数更新的速度可能会不均匀。这可能导致模型学习速度较慢，同时也可能让模型难以收敛到一个较好的解。**导致梯度下降的路径在成本函数空间中呈现出锯齿状（类似于走“之”字形）**。
    
    当我们使用梯度下降算法时，**所有的参数更新（包括 𝑤 和 𝑏）都是由相同的学习率 𝛼 控制的。**而对于 𝑤 来说，计算梯度时会将共同的误差项乘以对应的特征值。如果特征值的尺度差异较大，那么参数更新的速度就会受到影响，某些特征可能会比其他特征更新得更快。
    
    举个例子，在这个问题中，𝑤0 是由房屋面积（一般大于1000平方英尺）乘以误差项来更新的，而 𝑤1 是由卧室数量（一般为2-4个）乘以误差项来更新的。**这导致𝑤0 更新得比 𝑤1 更快。**
    

### **房屋数据的例程：特征未缩放 vs 缩放后**

- 特征未缩放
    
    为解决上述问题进行特征缩放（Feature Scaling），通过将所有特征值调整到相似的尺度，可以使参数更新更加均匀，提高模型的学习速度和性能。
    
    ![截屏2023-03-17 22.20.22.png](1%202%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%A4%9A%E8%BE%93%E5%85%A5%E5%8F%98%E9%87%8F%E7%9A%84%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98%204dadce4d8a89447881480f0324437c1e/%25E6%2588%25AA%25E5%25B1%258F2023-03-17_22.20.22.png)
    
- 🪜**特征缩放**
    
    归一化公式为：**`X_norm = (X - mu) / sigma`**，其中 X 为原始数据矩阵，mu 为特征均值，sigma 为特征标准差。函数通过逐元素地将每一列的均值 mu 减去每个样本，并除以该列的标准差 sigma，来计算归一化后的矩阵 X_norm。
    
    ![截屏2023-03-17 22.34.15.png](1%202%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%A4%9A%E8%BE%93%E5%85%A5%E5%8F%98%E9%87%8F%E7%9A%84%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98%204dadce4d8a89447881480f0324437c1e/%25E6%2588%25AA%25E5%25B1%258F2023-03-17_22.34.15.png)
    
    - 🔰名词解释
        
        **`np.mean()`**和 **`np.std()`**分别用于计算矩阵的均值和标准差。
        
        **`np.mean(X, axis=0)`**：这个函数计算矩阵 X 沿着指定轴（在这里是 axis=0，即列方向）的均值。矩阵 X 的形状为 (m, n)，其中 m 是样本数量，n 是特征数量。当 axis=0 时，函数计算每个特征（列）的均值。输出结果是一个形状为 (n,) 的数组，其中每个元素是对应特征的均值。
        
- 🪜**特征标准化之前、进行中和之后的特征分布情况**。
    
    **第一个子图（unnormalized）显示原始数据的分布**。
    
    第二个子图（X - μ）显示**减去均值后的数据分布**。
    
    第三个子图（Z-score normalized）**显示 Z-score 标准化后的数据分布**。
    
    ![截屏2023-03-17 22.44.18.png](1%202%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%A4%9A%E8%BE%93%E5%85%A5%E5%8F%98%E9%87%8F%E7%9A%84%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98%204dadce4d8a89447881480f0324437c1e/%25E6%2588%25AA%25E5%25B1%258F2023-03-17_22.44.18.png)
    
    - 📌程序关键点解释
        1. 首先，计算训练数据 X_train 的均值（mu）和标准差（sigma）。
        2. 然后，将均值从原始数据中减去，得到 X_mean。
        3. 接着，将每个特征除以其对应的标准差，得到 Z-score 标准化后的数据 X_norm。
    - 🔰标准差 从直觉上来说是什么意思呢？为什么第三张图就会变成在一个大概正方形的小区域中显示了所有数据呢？是按某种比例缩放了？这个比例是标准差？
        
        标准差是一个衡量数据集中各数据点与均值之间差异的度量。直观地说，标准差越大，数据点与均值之间的差异就越大；标准差越小，数据点与均值之间的差异就越小。简而言之，**标准差度量了数据的离散程度。**
        
        对于第三张图（Z-score 标准化后的数据），数据变成在一个大概正方形的小区域中显示，是因为我们对原始数据进行了标准化。这个过程包括两个步骤：
        
        1. **减去每个特征的均值：这样做的目的是将数据的中心移到原点（0,0）。**
        2. 除以每个特征的标准差：**这个步骤可以认为是按比例缩放每个特征，使得它们的范围和变化程度大致相同。**这个比例确实是标准差。
        
        通过这种标准化方法，我们将每个特征的均值变为 0，标准差变为 1。这样，所有特征的范围和变化程度大致相同，使得梯度下降算法更容易找到合适的参数，提高学习的速度和效果。
        
- 特征在标准化之前和之后的分布图
    
    绘制了**特征在标准化之前和之后的分布图**。**每个子图展示了一个特征的分布情况，呈现出特征数据的形状和离散程度。**这里的子图有点类似于直方图，但显示的是概率密度函数，而非数据的频数。概率密度函数是连续的，呈现出数据分布的形状。
    
    ![截屏2023-03-17 22.51.40.png](1%202%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%A4%9A%E8%BE%93%E5%85%A5%E5%8F%98%E9%87%8F%E7%9A%84%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98%204dadce4d8a89447881480f0324437c1e/%25E6%2588%25AA%25E5%25B1%258F2023-03-17_22.51.40.png)
    
    - 🔰四张图片里为什么就绘制了直线和抛物线 2 种样式，为什么？那个参数让他这样绘制的？ 好像只引用了这两个变量X_train[:,i]，X_features[i]，就算直线可以理解，那曲线是哪里来的呢？
        
        **`norm_plot()`**函数是一个自定义函数，用于绘制特征值的分布图。这个函数的主要目的是展示每个特征的分布情况，以便更好地理解特征的分布。
        
        ![截屏2023-03-17 23.01.55.png](1%202%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%A4%9A%E8%BE%93%E5%85%A5%E5%8F%98%E9%87%8F%E7%9A%84%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98%204dadce4d8a89447881480f0324437c1e/%25E6%2588%25AA%25E5%25B1%258F2023-03-17_23.01.55.png)
        
        - 📌程序过程解释
            1. 函数接受两个参数：**`ax`** 和 **`data`**。**`ax`** 是一个绘图对象，它指定了我们将要在其上绘制图形的子图。**`data`** 是我们要可视化分布的一维数据（特征列）。
            2. 首先，我们计算数据范围（最大值减去最小值）的 20% 并将其存储在变量 **`scale`** 中。接下来，我们在数据范围的左右延伸 **`scale`** 的距离，并在此范围内创建 50 个等距离的点。这些点存储在变量 **`x`** 中。
            3. 使用 **`ax.hist()`** 函数，在子图 **`ax`** 上绘制 **`data`** 的直方图。直方图展示了数据的分布情况。颜色设置为“xkcd:azure”。
            4. 接下来，我们计算数据的均值（**`mu`**）和标准差（**`std`**）。这两个值将用于生成正态分布曲线。
            5. 使用 **`norm.pdf()`** 函数，计算在给定的均值 **`mu`** 和标准差 **`std`** 下，每个 **`bins`**（直方图的边界值）
            6. 接下来，我们创建一个与 **`ax`** 共享 x 轴的新坐标轴（**`axr`**），然后在这个新坐标轴上绘制正态分布曲线。我们使用 **`axr.plot()`** 函数，传入 **`bins`** 和 **`dist`**，并设置曲线的颜色为“orangered”，线宽为 2。
            7. 使用 **`axr.set_ylim()`** 函数，将新坐标轴的 y 范围设置为从 0 开始。
            8. 最后，我们使用 **`axr.axis('off')`** 关闭新坐标轴的显示，以便只显示原始直方图和叠加的正态分布曲线。
        - 🔰名词解释：**`norm.pdf`**
            
            **`dist = norm.pdf(bins, loc=mu, scale=std)`** 这行代码计算了一组值（bins）对应的正态分布概率密度函数（pdf，Probability Density Function）的值。
            
            **`norm.pdf`** 是 SciPy 库中的一个函数，用于计算正态分布的概率密度函数值。函数接收以下参数：
            
            1. **`bins`**：一组 x 值，用于计算这些 x 值在正态分布下的概率密度函数值。
            2. **`loc`**：正态分布的均值，用于确定分布的中心。
            3. **`scale`**：正态分布的标准差，用于确定分布的宽度。
            
            这行代码会返回一个与 **`bins`** 形状相同的数组 **`dist`**，数组中的每个元素表示 **`bins`** 中对应 x 值在正态分布下的概率密度函数值。这个数组将被用于绘制正态分布曲线。
            
        - 🔰名词解释：**概率密度函数（Probability Density Function，简称 PDF）**
            
            **是连续随机变量的一种描述方法。**它描述了某个**实数在随机变量的概率分布中的概率密度，可以用来了解某个取值范围内随机变量出现的相对概率。**
            
            对于连续随机变量而言，我们不能说某个具体的值出现的概率是多少，因为在连续的情况下，任意一个具体值的概率都是 0。然而，我们可以谈论在一个特定区间内的概率。概率密度函数的值表示的是随机变量在某个值附近出现的相对可能性。概率密度函数的值越大，表明该处的随机变量取值越可能出现；概率密度函数的值越小，表明该处的随机变量取值越不可能出现。
            
        - 🔰名词解释：连续随机变量 是什么意思？ 4 张图中的横轴不连续，纵轴也不连续啊
            
            连续随机变量是指在某个范围内可以取任意实数值的随机变量。与之相对的是离散随机变量，它只能在一组离散的值之间取值。对于连续随机变量，我们通常关心的是某个区间内的概率分布，而不是某个具体数值的概率。
            
            所以连续性是指横轴和纵轴上的取值原则上是连续的。虽然在这份数据中的样本值看起来是分散的，但这不影响横轴和纵轴上的连续性。我们可以通过绘制直方图和正态分布曲线来描述这些连续数据的分布特征。这些图形帮助我们了解数据的整体分布，从而更好地分析和处理数据。
            
    
    ### 样本值和预测值叠加分布图
    
    段代码首先根据归一化的特征值计算预测值**`yp`**。然后，它绘制了四张子图，每张子图对应一个特征。蓝色点代表目标值（实际房价），橘色点代表预测值（根据模型预测的房价）。这四张图的目的是比较模型预测的房价与实际房价在每个特征上的分布情况。
    
    ![截屏2023-03-17 23.49.54.png](1%202%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%A4%9A%E8%BE%93%E5%85%A5%E5%8F%98%E9%87%8F%E7%9A%84%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98%204dadce4d8a89447881480f0324437c1e/%25E6%2588%25AA%25E5%25B1%258F2023-03-17_23.49.54.png)
    
    - 🔰可以认为这一系列的计算，最终是为了求能让成本函数最小的和x-i 对应的w-i 和 b，然后再用确定下来的w-i 和 b、一个给定的x 来计算y.这个y就是推测值？
        
        ![截屏2023-03-17 23.56.42.png](1%202%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%A4%9A%E8%BE%93%E5%85%A5%E5%8F%98%E9%87%8F%E7%9A%84%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98%204dadce4d8a89447881480f0324437c1e/%25E6%2588%25AA%25E5%25B1%258F2023-03-17_23.56.42.png)
        
        是的，这个过程的目的就是通过训练数据找到一组最优的权重 w（w-i）和偏置 b，使得成本函数（误差）最小。找到这些参数后，我们可以用它们和一个给定的 x（特征值向量，例如 x_house）来计算预测值 y（房价），这个 y 就是我们根据模型预测的房价。
        
        在这个例子中，我们已经得到了最优的权重 w_norm（归一化后的权重）和偏置 b_norm。然后，我们将一个给定的房子的特征值 x_house 进行归一化处理，得到 x_house_norm。最后，我们用归一化的特征值 x_house_norm、权重 w_norm 和偏置 b_norm 来计算预测房价 x_house_predict。这个 x_house_predict 就是我们根据模型预测的房价。
        
    
    ### 特征在标准化之前和之后，等比例的成本函数等高线图
    
    这个函数 **`plt_equal_scale`** 用于绘制两张等比例的成本函数等高线图。在这些图中，每条线代表一个特定的成本函数值，沿着这条线，成本函数具有相同的值。这些线形成了一个轮廓图，它帮助我们了解成本函数在不同参数值下的表现。当梯度下降算法寻找最小成本时，它沿着等高线朝着最低点（全局最小值）移动。
    
    ![截屏2023-03-18 00.01.47.png](1%202%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%A4%9A%E8%BE%93%E5%85%A5%E5%8F%98%E9%87%8F%E7%9A%84%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98%204dadce4d8a89447881480f0324437c1e/%25E6%2588%25AA%25E5%25B1%258F2023-03-18_00.01.47.png)
    
    ![截屏2023-03-17 23.58.39.png](1%202%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%A4%9A%E8%BE%93%E5%85%A5%E5%8F%98%E9%87%8F%E7%9A%84%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98%204dadce4d8a89447881480f0324437c1e/%25E6%2588%25AA%25E5%25B1%258F2023-03-17_23.58.39.png)
    
    - 📌图表解释
        
        第一张图（左图）展示了未归一化特征的模型参数。在这张图上，横轴表示 w[0]（房屋面积），纵轴表示 w[1]（卧室数量）。这里，颜色越深的区域代表成本函数的值越低，而颜色越浅的区域代表成本函数的值越高。从图中可以看出，梯度下降的轨迹表现为一系列竖线，这是因为未归一化特征导致 w[0] 和 w[1] 更新速度不一致。
        
        第二张图（右图）展示了归一化特征的模型参数。与第一张图类似，横轴表示归一化后的 w[0]（房屋面积），纵轴表示归一化后的 w[1]（卧室数量）。在这张图中，我们看到了一系列的圆圈，这表明归一化后的特征使得 w[0] 和 w[1] 更新速度更加均衡，梯度下降能够更有效地找到成本函数的最小值。
        
        通过比较这两张图，我们可以看出特征归一化对于优化算法的影响。在未归一化的情况下，优化过程可能受到不同特征值尺度的影响，导致梯度下降收敛缓慢。而在归一化后的情况下，优化过程更加平滑且收敛速度更快。
        

> **本节一句话总结：**
详细列举了特征缩放、学习率、梯度下降的示例。
详细解释了绘制的离散特征值、等高线、曲线等应该怎么解读。
> 

## 特征工程：[Feature engineering](https://www.coursera.org/learn/machine-learning/lecture/dgZYR/feature-engineering)

在这个课程中，讲解了特征选择对学习算法性能的巨大影响。实际上，对于许多实际应用来说，选择或构建合适的特征是使算法表现良好的关键步骤。以下是关键知识点：

1. 特征选择：选择适当的特征对学习算法的性能有很大影响。在许多实际应用中，选择或构建合适的特征是关键。
2. **特征工程**：特征工程是使用你对问题的知**识或直觉来设计新特征的过程**，**通常通过转换或组合原始特征使学习算法能更容易地做出准确预测。**
3. 示例：预测房价。假设你有两个特征：x_1（房子所在土地的宽度或前沿宽度）和 x_2（房子所在土地的深度）。你可以构建一个模型：f(x) = w_1x_1 + w_2x_2 + b。但是，你也可以通过组合这些特征来创建一个更有效的模型。
4. **新特征**：你可以计算土地的面积（x_1 * x_2），并将其定义为新特征 x_3。然后，你可以构建一个包含这个新特征的模型：f(x) = w_1x_1 + w_2x_2 + w_3x_3 + b。这样，模型可以根据数据选择参数 w_1、w_2 和 w_3，以判断土地的宽度、深度或面积对预测房价的重要性。
    
    ![截屏2023-03-17 21.00.16.png](1%202%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%A4%9A%E8%BE%93%E5%85%A5%E5%8F%98%E9%87%8F%E7%9A%84%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98%204dadce4d8a89447881480f0324437c1e/%25E6%2588%25AA%25E5%25B1%258F2023-03-17_21.00.16.png)
    
5. 通过特征工程，你可以获得更好的模型。这通常取决于你对应用领域的洞察力。
6. 特征工程的一个变种是能够拟合非线性函数，而不仅仅是直线。在下一个视频中，将学习如何实现这一点。

通过本课程，你应该了解了特征工程的重要性，以及如何根据直觉和问题的知识选择和构建适当的特征。在后续课程中，将学习如何拟合非线性函数，而不仅仅是数据中的直线。

> **本节一句话总结：**
特征工程：为问题设计新特征的过程。 原始特征→组合为新特征
> 

## 多项式回归：[Polynomial regression](https://www.coursera.org/learn/machine-learning/lecture/OnGhN/polynomial-regression)

本课程介绍了如何将多元线性回归和特征工程的概念用于多项式回归，以便在数据中拟合非线性函数。以下是关键知识点：

- **多项式回归：多项式回归允许你拟合非线性函数，而不仅仅是直线。这可以通过使用原始特征的幂来实现，例如 x 的平方和立方等**。
    - 🔰名词解释：非线性关系—-是什么意思？
        
        描述因变量和自变量之间的非线性关系，是指通过建立一个模型来表示因变量（响应变量）随着一个或多个自变量（预测变量）变化时呈现的非线性关系。在非线性关系中，因变量并不是自变量的简单线性组合。换句话说，**当自变量发生变化时，因变量的变化不是均匀的或按固定比例进行的。**
        
- 拟合曲线：对于一些数据集，如房价预测，使用非线性函数（如二次、三次函数）可能比使用线性函数更好。这取决于数据的实际分布。
    
    ![Untitled](1%202%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%A4%9A%E8%BE%93%E5%85%A5%E5%8F%98%E9%87%8F%E7%9A%84%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98%204dadce4d8a89447881480f0324437c1e/Untitled.png)
    
- **特征缩放：当使用原始特征的平方和立方等幂作为特征时，特征缩放变得非常重要**。特征缩放可以将特征值缩放到相似的范围，有助于梯度下降的性能。
- 特征选择：在多项式回归中，可以选择不同的特征。例如，除了使用 x 的平方和立方，还可以考虑使用 x 的平方根。选择合适的特征可以帮助你建立更好的模型。
    
    ![Untitled](1%202%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%A4%9A%E8%BE%93%E5%85%A5%E5%8F%98%E9%87%8F%E7%9A%84%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98%204dadce4d8a89447881480f0324437c1e/Untitled%201.png)
    
- 特征评估：在后续课程中，将介绍如何选择不同的特征和模型，以及如何衡量它们的性能，以帮助你决定是否包含某些特征。
- Scikit-learn：这是一个非常流行的开源机器学习库，被世界顶级 AI、互联网和机器学习公司的许多从业者使用。掌握 Scikit-learn 可帮助你更高效地实现线性回归和多项式回归。

通过本课程，你应该了解了如何使用多项式回归拟合非线性函数，以及特征工程和特征缩放的重要性。在后续实践中，你将实现线性回归和多项式回归，并学习如何使用 Scikit-learn 工具包。此外，下一周的课程将涉及分类算法，用于预测类别而非数值。

> **本节一句话总结：**
多项式回归：通过原始特征的幂的方式，实现非线性（曲线）拟合。
即由 f(wb)=wx+b 的直线 → f(wb)= x的幂多项式
> 

## 实验室：[Feature engineering and Polynomial regression](https://www.coursera.org/learn/machine-learning/ungradedLab/Xat0X/optional-lab-feature-engineering-and-polynomial-regression)

## 实验室：[Linear regression with scikit-learn](https://www.coursera.org/learn/machine-learning/ungradedLab/uaIsm/optional-lab-linear-regression-with-scikit-learn)

```jsx
scaler = StandardScaler()
X_norm = scaler.fit_transform(X_train)
```

- 📌代码解释
    
    这段代码首先使用 scikit-learn 的 StandardScaler 对数据进行缩放（归一化）。StandardScaler 是一种标准化方法，它通过将数据的每个特征（列）减去其均值并除以其标准差，使得每个特征的均值变为 0，标准差变为 1。
    
    1. 首先创建一个 StandardScaler 对象 **`scaler`**。
    2. 使用 **`fit_transform`** 方法将原始数据 **`X_train`** 转换为缩放后的数据 **`X_norm`**。**`fit_transform`** 方法首先计算给定数据的均值和标准差（fit），然后用这些值来转换数据（transform）。
    3. 使用 **`np.ptp`** 函数计算原始数据 **`X_train`** 和缩放后的数据 **`X_norm`** 的每列（特征）的极差（Peak to Peak range，即最大值与最小值之差）。**`axis=0`** 表示沿着列方向进行计算。
    4. 打印原始数据 **`X_train`** 和缩放后的数据 **`X_norm`** 的每列的极差。
    
    这段代码的目的是展示数据在归一化处理前后的极差变化。归一化有助于提高梯度下降等优化算法的收敛速度，并有助于提高模型性能。
    

```jsx
sgdr = SGDRegressor(max_iter=1000)
sgdr.fit(X_norm, y_train)
print(sgdr)
print(f"number of iterations completed: {sgdr.n_iter_}, number of weight updates: {sgdr.t_}")
```

- 📌代码解释
    
    这段代码使用 scikit-learn 的 **`SGDRegressor`** 类实现随机梯度下降法（Stochastic Gradient Descent, SGD）来拟合线性回归模型。
    
    1. 首先创建一个 **`SGDRegressor`** 对象 **`sgdr`**。**`max_iter=1000`** 参数表示最大迭代次数为 1000 次。
    2. 使用 **`fit`** 方法将归一化后的数据 **`X_norm`** 和目标变量 **`y_train`** 拟合到模型。这个过程会执行随机梯度下降算法，不断更新权重和偏置（截距），直到满足收敛条件或达到最大迭代次数。
    3. 打印 **`sgdr`** 对象，显示 **`SGDRegressor`** 的配置信息。
    4. 打印已完成的迭代次数 **`sgdr.n_iter_`** 和权重更新次数 **`sgdr.t_`**。
    
    在这个示例中，**`SGDRegressor`** 用于解决线性回归问题。它使用随机梯度下降算法迭代地更新模型参数（权重和偏置），以最小化成本函数（均方误差）。通过迭代次数和权重更新次数，我们可以了解模型训练过程的收敛情况。
    

```python
b_norm = sgdr.intercept_
w_norm = sgdr.coef_
print(f"model parameters:w: {w_norm}, b:{b_norm}")
print( "model parameters from previous lab: w: [110.56 -21.27 -32.71 -37.97], b: 363.16") # make a prediction using sgdr.predict()

y_pred_sgd = sgdr.predict(X_norm)
# make a prediction using w,b. 
y_pred = np.dot(X_norm, w_norm) + b_norm  
print(f"prediction using np.dot() and sgdr.predict match: {(y_pred == y_pred_sgd).all()}")

print(f"Prediction on training set:\n{y_pred[:4]}" )
print(f"Target values \n{y_train[:4]}")
```

- 📌代码解释
    
    这段代码使用 **`SGDRegressor`** 对象 **`sgdr`** 获取模型参数，并使用模型参数进行预测。
    
    1. 使用 **`sgdr.intercept_`** 获取模型的截距（偏置）**`b_norm`**。
    2. 使用 **`sgdr.coef_`** 获取模型的权重（系数）**`w_norm`**。
    3. 打印当前模型的参数：权重 **`w_norm`** 和截距 **`b_norm`**。
    4. 打印之前实验中得到的模型参数：权重 **`[110.56 -21.27 -32.71 -37.97]`** 和截距 **`363.16`**。
    5. 使用 **`sgdr.predict()`** 方法计算归一化数据 **`X_norm`** 的预测值 **`y_pred_sgd`**。
    6. 使用权重 **`w_norm`** 和截距 **`b_norm`** 通过矩阵乘法计算预测值 **`y_pred`**。
    7. 检查通过 **`np.dot()`** 和 **`sgdr.predict()`** 计算的预测值是否相同。
    8. 打印训练集上的前 4 个预测值。
    9. 打印对应的目标值。
    
    这段代码展示了如何使用 **`SGDRegressor`** 模型的参数进行预测。同时，通过比较不同方法计算的预测值，验证了模型参数的正确性。最后，通过打印预测值和实际目标值，可以对模型的预测效果进行初步了解。
    
- 程序输出结果：
    
    ![截屏2023-03-19 14.26.18.png](1%202%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%A4%9A%E8%BE%93%E5%85%A5%E5%8F%98%E9%87%8F%E7%9A%84%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98%204dadce4d8a89447881480f0324437c1e/%25E6%2588%25AA%25E5%25B1%258F2023-03-19_14.26.18.png)
    

## 测试：[Gradient descent in practice](https://www.coursera.org/learn/machine-learning/exam/wAYb5/practice-quiz-gradient-descent-in-practice)

- True/False? With polynomial regression, the predicted values f_w,b(x) does not necessarily have to be a straight line (or linear) function of the input feature x.
    
    True. 在多项式回归中，预测值 f_w,b(x) 不一定是输入特征 x 的直线（或线性）函数。多项式回归允许建模非线性关系，因为它将输入特征 x 的幂次项（如 x^2、x^3 等）纳入模型。这些幂次项使得预测值 f_w,b(x) 可以表示更复杂的非线性关系。
    
- 看好问题再回答
    
    ![截屏2023-03-19 14.36.00.png](1%202%20%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%A4%9A%E8%BE%93%E5%85%A5%E5%8F%98%E9%87%8F%E7%9A%84%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98%204dadce4d8a89447881480f0324437c1e/%25E6%2588%25AA%25E5%25B1%258F2023-03-19_14.36.00.png)