# 2.2 高级神经网络：训练神经网络

復習: Done
日付: 2023年3月27日
最終更新日時: 2023年4月15日 16:54

---

# **神经网络培训**

## TensorFlow实现：**[TensorFlow implementation](https://www.coursera.org/learn/advanced-learning-algorithms/lecture/oL0HT/tensorflow-implementation)**

欢迎来到关于高级学习算法课程的第二周。在第一周，您学习了如何在神经网络中进行推理。**本周，我们将探讨神经网络的训练。**使用自己的数据并在其上训练自己的神经网络是非常有趣的。本周我们将了解如何实现这一目标。以下是关键知识点：

1. 神经网络训练：给定一个训练集，包括一组图片X和相应的标签Y，您需要训练神经网络的参数。在这个例子中，我们使用了一个具有两个隐藏层（分别有25个和15个单元）和一个输出层的神经网络。
2. TensorFlow代码概述：以下是在TensorFlow中训练神经网络的关键步骤：
    - 步骤1：指定模型。这与上周看到的内容相似，我们要求**TensorFlow将神经网络的三层顺序连接在一起。**
    - 步骤2：编译模型。**编译模型的关键在于指定要使用的损失函数。**在这个例子中，我们使用了二元交叉熵损失函数。
    - 步骤3：拟合模型。调用fit函数，告诉TensorFlow使用指定的数据集X和Y以及步骤2中指定的损失函数来拟合模型。此外，**还需要指定要运行的迭代次数（称为epochs）**。
    
    ![Untitled](2%202%20%E9%AB%98%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20811c5785a8c24f51bc083416b17b00e7/Untitled.png)
    

通过了解这些关键知识点，您将能够使用TensorFlow训练神经网络。更重要的是，理解这些代码背后的概念和原理可以帮助您在遇到问题时进行调试。接下来的视频将深入讲解TensorFlow实现的各个步骤。祝学习愉快！

> **本节一句话总结：**
指定模型→编译模型→拟合
> 

## 训练详情：**[Training Details](https://www.coursera.org/learn/advanced-learning-algorithms/lecture/35RQ3/training-details)**

这个课程的关键知识点包括：

- 使用TensorFlow训练神经网络的三个步骤：
    
    ![Untitled](2%202%20%E9%AB%98%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20811c5785a8c24f51bc083416b17b00e7/Untitled%201.png)
    
    - 1.**定义模型 Dense：指定输入、输出和模型参数，建立神经网络的结构。**
        
        ![Untitled](2%202%20%E9%AB%98%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20811c5785a8c24f51bc083416b17b00e7/Untitled%202.png)
        
    - 2.**编译模型 compile：选择适当的损失函数，如二元交叉熵损失函数。**
        
        ![Untitled](2%202%20%E9%AB%98%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20811c5785a8c24f51bc083416b17b00e7/Untitled%203.png)
        
    - 3. **拟合模型 fit：使用梯度下降或其他优化算法最小化损失函数。**
        
        ![Untitled](2%202%20%E9%AB%98%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20811c5785a8c24f51bc083416b17b00e7/Untitled%204.png)
        
- 损失函数和成本函数：
    - **损失函数是单个训练样本上预测与实际值之间的差异度量。**
    - **成本函数是整个训练集上损失函数的平均值。**
- 梯度下降算法：通过迭代更新参数来最小化成本函数。
- 反向传播算法：用于计算梯度下降中所需的偏导数。
- TensorFlow库的使用：利用现有的深度学习库，如TensorFlow或PyTorch，可以更简单地实现和训练神经网络。
    
    ![Untitled](2%202%20%E9%AB%98%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20811c5785a8c24f51bc083416b17b00e7/Untitled%205.png)
    

在接下来的课程中，您将学习如何使用不同的激活函数（而不是我们目前使用的Sigmoid激活函数）以改进神经网络的性能。这将使您的神经网络效果更好。

- 🔰问题：神经网络训练其实都是用在classificaion 分类是吗？
    
    **神经网络可以用于分类（classification）和回归（regression）两种类型的问题**。对于分类问题，通常在输出层使用Sigmoid或Softmax激活函数，以获得概率分布。而对于回归问题，输出层通常使用线性激活函数（无激活函数）或其他特定的激活函数，以产生连续的数值输出。
    
    神经网络在回归问题中的应用同样广泛。例如，预测房价、股票价格或者其他连续变量的问题都可以使用神经网络解决。在处理回归问题时，通常将输出层设计为一个或多个输出单元，这取决于你需要预测的目标变量的数量。**对于回归问题，损失函数通常选择均方误差（MSE，Mean Squared Error）或者平均绝对误差（MAE，Mean Absolute Error）**等。
    

> **本节一句话总结：**
指定模型：`**Dense(units=25,activation=’sigmoid’)**`
→编译模型：`**model.compile(loss=BiaryCrossentropy())**`
→拟合：`**model.fit(X,y,epochs=100)`**
损失函数（单个样本差异）、成本函数（整个训练集的损失函数平均值）
> 

---

# **激活功能**

## sigmoid激活的替代品：[Alternatives to the sigmoid activation](https://www.coursera.org/learn/advanced-learning-algorithms/lecture/04Maa/alternatives-to-the-sigmoid-activation)

在这节课中，关键知识点主要集中在**激活函数（activation functions）**的选择。之前，课程中使用了Sigmoid激活函数，但是，实际上有其他激活函数可以让神经网络变得更强大。

- **ReLU（Rectified Linear Unit）激活函数**：ReLU激活函数的数学表示为 g(z) = max(0, z)。当输入z大于等于0时，ReLU函数的输出等于z；当输入z小于0时，输出为0。这种激活函数的优点是它**可以让激活值取任意非负数**，从而使得神经网络更具表达能力。
    
    ![Untitled](2%202%20%E9%AB%98%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20811c5785a8c24f51bc083416b17b00e7/Untitled%206.png)
    
- **线性（Linear）激活函数**：线性激活函数的数学表示为 g(z) = z。在某些情况下，使用线性激活函数等**同于不使用激活函数**，因为激活值a等于输入值z。虽然在本课程中，我们将其称为线性激活函数，但在其他地方，人们可能会说没有使用激活函数。
    
    ![Untitled](2%202%20%E9%AB%98%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20811c5785a8c24f51bc083416b17b00e7/Untitled%207.png)
    
    - 🔰问题：relu的输出也是和 Sigmoid 一样是概率吗？
        
        **ReLU（修正线性单元）激活函数的输出不是概率。**ReLU函数将所有负数置为零，而正数保持不变。**ReLU的输出范围是[0, +∞)，而不是[0, 1]，**所以它不像Sigmoid函数那样输出概率。
        

除了Sigmoid激活函数，ReLU和线性激活函数是神经网络中最常用的激活函数。稍后在课程中还会介绍第四种激活函数——Softmax激活函数。不同类型的激活函数可用于构建功能丰富的强大神经网络。

在下一节课中，将讨论如何在Sigmoid、ReLU和线性激活函数之间进行选择以构建神经网络。

> **本节一句话总结：**
激活函数（Activation function）：Relu  `**g(z) = max(0, z)**`、Linear  `**g(z) = z**`
> 

## 选择激活函数：[Choosing activation functions](https://www.coursera.org/learn/advanced-learning-algorithms/lecture/aWivF/choosing-activation-functions)

本课程的关键知识点在于如何选择神经网络中不同神经元的激活函数。以下是为输出层和隐藏层选择激活函数的建议：

- **输出层激活函数的选择**：
    - **二元分类问题：使用 Sigmoid 激活函数**，因为神经网络可以学习预测 y 等于 1 的概率，就像逻辑回归一样。
    - **回归问题：使用线性激活函数**，特别是当预测值 y 可以是正数或负数时。
    - **只能取非负值的预测问题：使用 ReLU 激活函数**，因为 ReLU 只能取非负值。
    
    ![Untitled](2%202%20%E9%AB%98%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20811c5785a8c24f51bc083416b17b00e7/Untitled%208.png)
    
- **隐藏层激活函数的选择：ReLU 激活函数**是目前神经网络中最常用的选择。这主要是因为：
    - ReLU 计算速度较快，因为它只需要计算 max(0, z)，而 Sigmoid 需要进行指数运算和逆运算等，效率较低。
    - ReLU 函数只在一个部分平坦，而 Sigmoid 函数在两个部分平坦。使用梯度下降训练神经网络时，函数在很多地方平坦会导致梯度下降速度变慢。研究人员发现，使用 ReLU 激活函数可以使神经网络学习速度更快。
    
    ![Untitled](2%202%20%E9%AB%98%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20811c5785a8c24f51bc083416b17b00e7/Untitled%209.png)
    

综上所述，对于输出层，建议根据预测标签 y 的类型选择合适的激活函数。而对于隐藏层，建议使用 ReLU 作为默认激活函数。此外，研究文献中还提到了一些其他激活函数，如双曲正切（tanh）激活函数、LeakyReLU激活函数和Swish激活函数。尽管这些激活函数在某些情况下可能比 ReLU 效果更好，但本课程中介绍的激活函数对绝大多数应用来说已经足够。

![Untitled](2%202%20%E9%AB%98%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20811c5785a8c24f51bc083416b17b00e7/Untitled%2010.png)

课程还强调了激活函数的重要性。在下一节课中，将讨论为什么仅使用线性激活函数或完全不使用激活函数是行不通的，以及激活函数在神经网络中的重要作用。

> **本节一句话总结：**
隐藏层：ReLu（不要用 Linear）
输出层：根据预测标签 y 的类型选择，Sigmoid，Linear、ReLu
> 

## 为什么我们需要激活功能？：[Why do we need activation functions?](https://www.coursera.org/learn/advanced-learning-algorithms/lecture/mb9bw/why-do-we-need-activation-functions)

本课程的关键知识点在于解释为什么神经网络需要激活函数，以及仅使用线性激活函数会导致什么问题。

- 如果在**神经网络中的每个神经元都使用线性激活函数，那么神经网络将变得与线性回归模型没有任何区别，**因此无法捕捉到更复杂的模式。这意味着使用神经网络将失去意义，因为它无法拟合比线性回归更复杂的模型。
    
    ![Untitled](2%202%20%E9%AB%98%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20811c5785a8c24f51bc083416b17b00e7/Untitled%2011.png)
    
- 在一个简单的神经网络示例中，**当我们在所有神经元中使用线性激活函数时，输出 a2 只是输入 x 的线性函数。**这说明我们可以用线性回归模型代替包含一个隐藏层和一个输出层的神经网络。这个结论源于线性代数的一个事实：线性函数的线性函数本身是一个线性函数。因此，具有多个层的神经网络不能让神经网络计算更复杂的特征，也不能学习比线性函数更复杂的模式。
    
    ![Untitled](2%202%20%E9%AB%98%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20811c5785a8c24f51bc083416b17b00e7/Untitled%2012.png)
    
- 如果在具有多个层的神经网络中使用线性激活函数，那么该模型将完全等同于线性回归。**如果我们仅在隐藏层使用线性激活函数，而在输出层使用 logistic 激活函数，那么模型将等同于 logistic 回归。**因此，**建议在神经网络的隐藏层中不要使用线性激活函数，而使用 ReLU 等非线性激活函数。**
    
    ![Untitled](2%202%20%E9%AB%98%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20811c5785a8c24f51bc083416b17b00e7/Untitled%2013.png)
    

本课程中，您已经学习了如何构建用于二元分类问题（y 只取两个值）和回归问题的神经网络。在接下来的课程中，将介绍如何构建用于多分类问题的神经网络，即 y 可以取三个、四个、十个甚至更多的类别值。

> **本节一句话总结：**
所有层使用Linear激活，或仅在输出层使用 logistic，相当于没使用神经网络。
> 

## 实验室：ReLU激活

![截屏2023-03-29 10.33.48.png](2%202%20%E9%AB%98%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20811c5785a8c24f51bc083416b17b00e7/%25E6%2588%25AA%25E5%25B1%258F2023-03-29_10.33.48.png)

- 🔰问题：这是在做什么？
    
    目标是展示 ReLU（整流线性单元）**激活函数的非线性行为如何为神经网络提供拟合复杂非线性函数的能力。**在这个例子中，我们需要用三个线性段来匹配一个三段折线（即**目标函数**）。
    
    这三段折线及其在每个弯折处的改变。箭头表示在每个弯折处需要改变的斜率。通过调整权重（𝑤）和偏置（𝑏），我们可以控制神经网络中每个神经元的输出，使其在特定区间内具有所需的斜率。
    
- 🔰问题：目标函数是什么？
    
    在这个例子中，目标函数是指要拟合的目标折线，而不是分类问题中的损失函数。这个例子仅仅是为了演示如何使用ReLU激活函数使神经网络能够表示复杂的非线性函数。**目标函数在这里表示我们希望神经网络学会拟合的函数，即那个带有三个折点的折线。**
    

![截屏2023-03-29 10.17.58.png](2%202%20%E9%AB%98%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20811c5785a8c24f51bc083416b17b00e7/%25E6%2588%25AA%25E5%25B1%258F2023-03-29_10.17.58.png)

- 🔰问题：为什么调节了数字，右边图形会变化，有什么规律？调节角度的目的是什么？
    
    包含了三个单元。通过调整 w1、b1、w2 和 b2 的值，我们可以改变这三个单元的输出，从而使整个神经网络的输出更接近目标函数。
    
    当我们调整 w 和 b 的右边的图形会发生变化，因为这些参数决定了每个神经元的输出。具体来说，每个神经元的线性输出z由其输入、权重w和偏置b计算得出，然后通过激活函数（本例中为ReLU）得到激活输出a。因此，当我们调整w和b时，它们会影响神经元的输出，从而改变右边图形的形状。
    
    在这个示例中，**我们的目标是通过调整这些参数来使神经网络的输出与目标函数尽可能接近。为了达到这个目标，我们需要确保每个单元在特定区间内具有正确的斜率，同时在其他区间保持“关闭”状态。**这是通过使用ReLU激活函数实现的，因为它允许神经元在某个阈值之前保持关闭（即输出为0），然后在该阈值之后产生线性输出。
    
    调整角度的目的是为了使每个神经元在特定区间内具有正确的斜率，从而使整个神经网络的输出更接近目标函数。我们希望在每个折线段上找到合适的权重（w）和偏置（b）组合，以使得每个神经元在相应的区间内具有所需的斜率。在其他区间，我们希望神经元保持“关闭”状态，以防止它们干扰其他神经元的输出。
    
    通过调整 w 和 b 的值，并观察右侧图形的变化，我们可以更好地理解 ReLU 激活函数如何使神经网络能够表示复杂的非线性函数。这个例子展示了 ReLU 激活函数的 "关闭" 或 "禁用" 功能是如何使模型能够拼接线性段来模拟复杂的非线性函数的。
    

---

# **多类分类**

## 多类：**[Multiclass](https://www.coursera.org/learn/advanced-learning-algorithms/lecture/4u2wC/multiclass)**

这门课程的关键知识点是多类分类问题的概念以及与二元分类问题的区别。**多类分类问题指的是具有两个以上可能的输出标签的分类问题，而不仅仅是 0 或 1**。

- **多类分类（Multiclass Classification）**问题的示例：
    - 手写数字识别：除了区分 0 和 1，可能需要识别 10 个数字。
    - 病患分类：根据患者的病症，诊断他们可能患有三种或五种不同的疾病之一。
    - 制造部件的视觉缺陷检测：识别药片是否存在划痕、变色或破损等不同类型的缺陷。
    
    ![截屏2023-03-30 11.12.22.png](2%202%20%E9%AB%98%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20811c5785a8c24f51bc083416b17b00e7/%25E6%2588%25AA%25E5%25B1%258F2023-03-30_11.12.22.png)
    
- 与二元分类问题的区别：
    - 二元分类问题中，y 只能取 0 或 1，而在多类分类问题中，y 可以取多个可能的值。
    - 在二元分类问题中，可以使用逻辑回归模型估计给定特征 x 时 y 为 1 的概率。在**多类分类问题中，需要估计 y 等于每个类别的概率**。
    
    ![截屏2023-03-30 11.12.54.png](2%202%20%E9%AB%98%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20811c5785a8c24f51bc083416b17b00e7/%25E6%2588%25AA%25E5%25B1%258F2023-03-30_11.12.54.png)
    
- 多类分类问题的数据集：
    - 数据集可能包含多个类别，例如圆圈表示一类，叉号表示另一类，三角形表示第三类，方块表示第四类。
    - 算法需要学习一个将特征空间划分为四个而不是两个类别的决策边界。

在接下来的课程中，您将学习 softmax 回归算法，它是逻辑回归算法的推广。使用 softmax 回归，您将能够处理多类分类问题。然后，将 softmax 回归应用于神经网络，使其也能够进行多类分类问题的训练。

> **本节一句话总结：**
多类分类：输出有 2 个以上的标签，估计每个类别的概率。如手写数字识别。
> 

## Softmax 一种激活函数：[Softmax](https://www.coursera.org/learn/advanced-learning-algorithms/lecture/mzLuU/softmax)

这门课程的关键知识点是 softmax 回归算法，它是逻辑回归算法在多类分类问题中的推广。以下是关键概念和步骤：

- 逻辑回归与 softmax 回归比较：
    - 逻辑回归用于二分类问题，而 **softmax 回归用于多类分类问题**。
    - 逻辑回归中，y 可以是 0 或 1，而在 softmax 回归中，y 可以取 n 个可能的值。
- softmax 回归的计算：
    - 对于每个类别 j，计算 z_j = w_j·x + b_j。
    - 使用 softmax 函数计算 a_j：a_j = e^(z_j) / Σ(e^(z_k))，其中 k 从 1 到 n。
    - a_j 可解释为给定输入特征 x 时，y 等于 j 的概率。
    
    ![截屏2023-03-30 11.14.42.png](2%202%20%E9%AB%98%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20811c5785a8c24f51bc083416b17b00e7/%25E6%2588%25AA%25E5%25B1%258F2023-03-30_11.14.42.png)
    
- softmax 回归的损失函数：
    - 交叉熵损失（Crossentropy loss）：如果 y 等于 j，**损失为 -log(a_j)**。
    - 损失函数激励算法尽可能使 a_j 接近 1，以便在给定输入特征 x 时，y 等于 j 的概率较大。
    - **衡量了模型预测概率分布与真实概率分布之间的差异。**
        
        ![截屏2023-03-30 11.19.19.png](2%202%20%E9%AB%98%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20811c5785a8c24f51bc083416b17b00e7/%25E6%2588%25AA%25E5%25B1%258F2023-03-30_11.19.19.png)
        
- 成本函数：
    - **成本函数是整个训练集上损失的平均值**。

通过训练 softmax 回归模型，您可以构建多类分类算法。接下来的目标是将 softmax 回归模型应用于神经网络，从而为多类分类问题训练神经网络。

> **本节一句话总结：**
> 
> 
> Softmax 激活函数：用于多类问题，输出y可是多个概率。
> `**a_j**` ：给定输入特征 x 时，y 属于 j 的概率。
> Softmax的损失函数：`**-log(a_j)`**
> Softmax的成本函数：整个训练集损失函数的平均值
> 

## 具有Softmax输出的神经网络：**[Neural Network with Softmax output](https://www.coursera.org/learn/advanced-learning-algorithms/lecture/ZQPG3/neural-network-with-softmax-output)**

这门课程涉及到的主要知识点是使用神经网络进行多类分类。为了完成这个任务，我们需要将 Softmax 回归模型放入神经网络的输出层。以下是关键知识点的概述：

- **Softmax 输出层**：在进行手写数字分类时，我们需要将神经网络的**输出单元更改为 10 个，以便对 0 到 9 的所有数字进行分类**。在这种情况下，我们使用 Softmax 输出层。
- **Softmax 函数**：**Softmax 函数的一个独特之处是，每个激活值 a_i 取决于所有 Z 值**。这与我们之前见过的其他激活函数（如 Sigmoid 或 ReLU）不同。Softmax 函数可以计算出 y 属于 10 个可能类别的概率。
    
    ![截屏2023-03-30 11.24.49.png](2%202%20%E9%AB%98%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20811c5785a8c24f51bc083416b17b00e7/%25E6%2588%25AA%25E5%25B1%258F2023-03-30_11.24.49.png)
    
- **实现 Softmax 函数**：使用 **TensorFlow**，我们可以实现神经网络进行多类分类。首先，我们需要为神经网络定义一个具有 **10 个输出单元的输出层（ReLu），并使用 Softmax 激活函数**。然后，我们需要使用 **`SparseCategoricalCrossentropy`** 损失函数进行模型训练。请注意，在后续视频中，将介绍一种更优的实现方法，以便更准确地计算这些概率。
    
    ![截屏2023-03-30 11.42.05.png](2%202%20%E9%AB%98%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20811c5785a8c24f51bc083416b17b00e7/%25E6%2588%25AA%25E5%25B1%258F2023-03-30_11.42.05.png)
    
    - 🔰MNIST with Softmax
        
        MNIST with Softmax 是一种用于手写数字识别的模型，其中 Softmax 是一种用于多类分类的激活函数。MNIST 数据集是一个非常常用的手写数字数据集，包含了 60,000 张用于训练的图像和 10,000 张用于测试的图像。每个图像都是一个 28x28 的灰度图像，表示 0 到 9 中的一个数字。
        
        在 MNIST with Softmax 中，我们使用 Softmax 回归模型来对每个图像进行分类。具体地说，我们将每个图像的像素值展平为一个 1 维向量，并将其作为输入向量。然后，我们使用 Softmax 激活函数将输出转换为表示每个数字的概率分布。在训练过程中，我们使用交叉熵损失函数来衡量模型的预测与实际标签之间的差距，并使用随机梯度下降等优化算法来最小化损失。最终，我们使用测试集来评估模型的性能，以确定其对手写数字识别的准确性。
        

在下一个视频中，我们将详细了解如何使用推荐的代码训练 Softmax 神经网络。这将帮助您更好地理解如何使用神经网络进行多类分类。

> **本节一句话总结：**
手写数字识别用 TensorFlow 
指定模型：隐藏层 relu `**Dense(units=25,activation=’relu’)**`、输出层 softmax `**Dense(units=10,activation=’softmax’)**`
→编译模型：**`model.compile(loss= SparseCategoricalCrossentropy())`**
→拟合：`**fit**`
> 

## 改进了softmax的实施：**[Improved implementation of softmax](https://www.coursera.org/learn/advanced-learning-algorithms/lecture/Tyil1/improved-implementation-of-softmax)**

本课程的关键知识点包括：

- 数值稳定性问题：**计算机在进行浮点数计算时可能会遇到数值稳定性问题**。特别是当涉及到非常大或非常小的数字时，数值舍入误差可能导致计算结果不准确。
    
    ![截屏2023-03-30 11.43.24.png](2%202%20%E9%AB%98%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20811c5785a8c24f51bc083416b17b00e7/%25E6%2588%25AA%25E5%25B1%258F2023-03-30_11.43.24.png)
    
- 用于**softmax的优化实现**：通过将激活函数和损失函数组合在一起，可以改进原始的softmax实现。这种方法避免了计算中间值a，从而提高了数值稳定性。**要实现这一点，需要将输出层的激活函数更改为线性激活函数，并将损失函数设置为`from_logits=True`**。虽然这使代码更难阅读，但实际上，这样的实现更精确且数值稳定。
    
    ![截屏2023-03-30 11.43.42.png](2%202%20%E9%AB%98%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20811c5785a8c24f51bc083416b17b00e7/%25E6%2588%25AA%25E5%25B1%258F2023-03-30_11.43.42.png)
    
    ![截屏2023-03-30 11.48.33.png](2%202%20%E9%AB%98%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20811c5785a8c24f51bc083416b17b00e7/%25E6%2588%25AA%25E5%25B1%258F2023-03-30_11.48.33.png)
    
- 与logistic回归的对比：对于logistic回归，原始实现和优化实现在数值稳定性方面的差异并不显著。然而，在softmax回归中，使用优化实现可以大大降低数值舍入误差，从而提高计算的准确性。
- 输出概率的计算：优化实现的**输出层不再输出概率A_1至A_10，而是输出z_1至z_10**。**为了获取概率值，需要对输出层的值进行额外处理**，例如在logistic回归中使用logistic函数将输出值映射为概率。
    
    ![截屏2023-03-30 11.50.02.png](2%202%20%E9%AB%98%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20811c5785a8c24f51bc083416b17b00e7/%25E6%2588%25AA%25E5%25B1%258F2023-03-30_11.50.02.png)
    
    ![截屏2023-03-30 11.50.50.png](2%202%20%E9%AB%98%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20811c5785a8c24f51bc083416b17b00e7/%25E6%2588%25AA%25E5%25B1%258F2023-03-30_11.50.50.png)
    
    - 🔰问题：最后提到了logistic regression ，为什么隐藏层用了sigmoid 而不是和前面一样隐藏层用relu 呢？
    在提到logistic回归的示例时，课程中的目的是说明激活函数与损失函数组合在一起的概念。在实际应用中，隐藏层可以使用ReLU，也可以使用sigmoid或其他激活函数。选择哪种激活函数取决于任务需求、模型结构和实际表现。

在学习这门课程时，理解如何通过将激活函数和损失函数组合在一起来优化softmax实现是非常重要的。这种方法可以提高数值稳定性，从而使模型更准确地学习多类分类任务。

> **本节一句话总结：**
Softmax 改善：避免浮点数引起的误差
指定模型：隐藏层 relu `**Dense(units=25,activation=’relu’)**`、输出层 softmax `**Dense(units=10,activation=’linear’)**`
→编译模型：**`model.compile(loss= SparseCategoricalCrossentropy(from_logits=True))`**
→拟合：`**fit**`
→预测：`**logsts = model(X)**` ,将a_j 转为概率 **`tf.nn.softmax(logits)`** 。二元分类 **`tf.nn.sigmoid(logits)`**
> 

## 多标签分类：**[Classification with multiple outputs](https://www.coursera.org/learn/advanced-learning-algorithms/lecture/pjIk0/classification-with-multiple-outputs-optional)**

在这段课程中，关键知识点包括多标签分类问题和如何构建用于多标签分类的神经网络。以下是这些概念的简要总结：

- 多标签分类问题：**与多类分类问题不同，多标签分类问题允许为每个输入分配多个标签。**例如，在自动驾驶汽车的场景中，给定一张照片，您可能希望判断图片中是否存在汽车、公共汽车和行人。在这种情况下，每个输入图像都可能同时具有多个标签。
    
    ![截屏2023-03-30 11.59.22.png](2%202%20%E9%AB%98%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20811c5785a8c24f51bc083416b17b00e7/%25E6%2588%25AA%25E5%25B1%258F2023-03-30_11.59.22.png)
    
- 构建多标签分类的神经网络：有两种方法可以处理多标签分类问题。
    - **将问题拆分为多个独立的机器学习问题**：可以针对每个标签单独构建一个神经网络（例如，一个用于检测汽车，一个用于检测公共汽车，另一个用于检测行人）。
    - **同时检测所有标签**：构建一个神经网络，**其输出层有多个节点，每个节点对应一个标签。**在这种情况下，我们可以使用sigmoid激活函数作为输出层的激活函数，因为我们正在解决多个二分类问题。输出向量将包含多个概率值，分别表示每个标签存在的概率。
    
    ![截屏2023-03-30 11.59.49.png](2%202%20%E9%AB%98%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20811c5785a8c24f51bc083416b17b00e7/%25E6%2588%25AA%25E5%25B1%258F2023-03-30_11.59.49.png)
    

通过了解多标签分类问题及其与多类分类问题的区别，您可以根据实际应用场景选择正确的方法来解决您需要处理的问题。这个课程部分的重点是介绍多标签分类问题的概念，以便您在实际应用中能够区分多类和多标签分类问题，并选择最适合您的任务的方法。

> **本节一句话总结：**
多标签分类：为每个输入分配多个标签。如给定一张照片，判断是否存在汽车、公共汽车和行人。
（多类分类：为每个输入分配一个标签，输出有多个标签。）
> 

## Softmax实验室

softmax 函数将 𝐳 转换为概率分布，如下所述。应用 softmax 后，每个输出都将介于 0 和 1 之间，并且输出的总和为 1，因此它们可以被解释为概率。较大的输入将对应较大的输出概率。这个过程可以理解为将向量 𝐳 映射为一个概率分布向量，其中每个元素代表对应类别的概率，且所有元素之和为 1。这种方式可以使得模型的输出可以直接解释为概率，而不需要再进行额外的处理。

![截屏2023-03-28 21.07.59.png](2%202%20%E9%AB%98%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20811c5785a8c24f51bc083416b17b00e7/%25E6%2588%25AA%25E5%25B1%258F2023-03-28_21.07.59.png)

- 🔰问题：这种将函数𝐳 转换为概率分布的方式，和用sigmoid 把z转换为概率，感觉好像差不多？
    
    是的，softmax 和 sigmoid 都可以将输入的值映射为概率值。但它们的使用场景略有不同。Sigmoid 函数常用于二分类问题，将输出限制在 [0, 1] 的范围内。而 softmax 函数则常用于多分类问题，可以将多个输出映射到 [0, 1] 的范围内，并且这些输出的和为 1，表示概率分布。因此，在神经网络中，softmax 通常被用于具有多个输出类别的输出层。
    

![截屏2023-03-28 21.20.24.png](2%202%20%E9%AB%98%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20811c5785a8c24f51bc083416b17b00e7/%25E6%2588%25AA%25E5%25B1%258F2023-03-28_21.20.24.png)

## 多类实验室

**`np.unique(y_train)`**返回**`y_train`**中的所有唯一值，即该数据集中所有可能的标签类别。而**`y_train[:10]`**则是返回**`y_train`**的前10个样本的标签值，用于查看该数据集中的类别是否与**`np.unique(y_train)`**中的类别匹配。

![截屏2023-03-28 21.25.27.png](2%202%20%E9%AB%98%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20811c5785a8c24f51bc083416b17b00e7/%25E6%2588%25AA%25E5%25B1%258F2023-03-28_21.25.27.png)

![截屏2023-03-30 13.09.50.png](2%202%20%E9%AB%98%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20811c5785a8c24f51bc083416b17b00e7/%25E6%2588%25AA%25E5%25B1%258F2023-03-30_13.09.50.png)

![截屏2023-03-28 21.28.06.png](2%202%20%E9%AB%98%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20811c5785a8c24f51bc083416b17b00e7/%25E6%2588%25AA%25E5%25B1%258F2023-03-28_21.28.06.png)

![截屏2023-03-30 12.14.47.png](2%202%20%E9%AB%98%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20811c5785a8c24f51bc083416b17b00e7/%25E6%2588%25AA%25E5%25B1%258F2023-03-30_12.14.47.png)

- 🔰问题：为什么用 relu 函数，这两张图中间分别有一条之间？图例c0,c1,c2,c3。还有个颜色渐变的图例 0-20。 分别代表了什么意思？
    
    这个示例展示了一个多类分类问题，其中有四个类别（c0, c1, c2, c3）。示例中使用了一个包含两个隐藏层的神经网络。这**两张图分别表示神经网络第一层中的两个神经元（Unit 0 和 Unit 1）的功能。横轴x0和纵轴x1是输入特征**。
    
    这里，ReLU函数将输出限制在非负范围内。这两张图中的直线（contour lines）表示**ReLU函数中的拐点，也就是输出从0变为非0的位置**。这些拐点实际上是神经元尝试将数据点划分为不同类别的边界。
    
    - Unit 0 将类别 0 和 1 与类别 2 和 3 分开。在边界线左边的点（类别 0 和 1）将输出为0，而线右边的点将输出大于0的值。
    - Unit 1 将类别 0 和 2 与类别 1 和 3 分开。在边界线上方的点（类别 0 和 2）将输出为0，而线下方的点将输出大于0的值。
    
    图中的颜色渐变图例表示了神经元的输出值，范围从0到20。颜色越深，输出值越大。这里的ReLU函数使得输出值可以在较大范围内变化，而不仅限于0和1之间。
    
    通过观察这些图，我们可以了解ReLU激活函数如何在神经网络中帮助划分输入数据点，以便在下一层进一步处理。在本示例中，Unit 0 和 Unit 1 分别负责将输入数据点划分到不同的类别。在神经网络的下一层中，这些划分的信息将进一步组合，以实现更准确的多类分类。
    
- 🔰问题：输出是值什么呢？哦，是蓝色的这个坐标为 0 的地方吗？和横轴纵轴的 0.0 无关是吗？
    
    是的，您理解正确。在这里，“输出为0”是指神经元的输出值为0，而不是指横轴或纵轴上的0。
    
- 🔰问题：神经元的输出值，可以认为是和w_i,x_,b相关的一个函数z对吗？图中显示的是当x_i变化时，函数的结果是值的大小，用蓝色表现出来。因为用relu，g(z)= z，这么理解对吗？
    
    是的，您的理解是正确的。**神经元的输出值是由权重（w_i）、输入（x_i）和偏置（b）组合而成的函数z的结果。**然后，这个结果通过激活函数（这里是ReLU）进行处理。对于ReLU激活函数，g(z) = max(0, z)，即当z大于0时，g(z) = z；当z小于或等于0时，g(z) = 0。在这个例子中，图中用颜色表示了神经元输出值（经过ReLU激活函数处理后的值），蓝色表示输出值为0，其他颜色表示输出值大于0。当输入数据点（x_i）变化时，颜色变化反映了神经元输出值的变化。
    
- 🔰问题：那么，对于z 小于 0 的部分，可以认为不归当前神经元负责？
    
    是的，您的理解是正确的。**对于ReLU激活函数，当z小于0时，输出值为0。这意味着在这个区域内，当前神经元的激活被“关闭”，因此不对最终输出产生贡献。**在多层神经网络中，这种特性有助于将复杂的非线性函数拆分成多个线性部分，由不同的神经元负责。
    

**`y_train.reshape(-1,)`**是将**`y_train`**的形状进行变换，将其变成一个一维的数组。具体来说，其中的参数**`-1`**表示自动计算该维度的大小，即根据数组的总元素数和其他维度的大小来自动计算出该维度的大小。例如，如果**`y_train`**是一个形状为**`(60000, 1)`**的二维数组，那么通过**`y_train.reshape(-1,`**将其变换为一个形状为**`(60000,)`**的一维数组。这个操作常常用在将多维数组转换为一维数组的场合。

![截屏2023-03-28 21.28.55.png](2%202%20%E9%AB%98%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20811c5785a8c24f51bc083416b17b00e7/%25E6%2588%25AA%25E5%25B1%258F2023-03-28_21.28.55.png)

- 🔰问题：这里的 4 张图，为什么图例c0-c3的布局和上一组的 2 张不一样了呢？横轴和纵轴的地方堆积了很多c1,c2。c0只在0,0的位置有一个。是因为在layer 1 中，unit0和unit1都等于0的区域刚好是c0吗？就算c0已经分好类了？
    
    您的理解是正确的。**在第二层（输出层）的图中，横轴和纵轴表示来自上一层（第一层）的输出值，分别为 𝑎[1]0 和 𝑎[1]1。**在第一层的图中，可以看到，蓝**色（类别0）的点主要位于ReLU激活函数输出为0的区域，**即𝑎[1]0=0和𝑎[1]1=0的区域。
    
    到了第二层，我们关注的是如何基于第一层的输出将不同类别的点区分开。由于类别0（蓝色）的点在第一层的输出中主要集中在(0,0)附近，因此在第二层的图中，我们可以看到蓝色点主要聚集在原点附近。**而其他类别（绿色、橙色和紫色）的点在第一层的输出中分布在不同的区域，这也反映在第二层的图中。**
    
    通过这种方式，神经网络可以根据第一层的输出对各个类别进行有效区分。这也说明了为什么在第二层的图中，类别0（蓝色）的点主要位于原点附近，而其他类别的点分布在不同区域。
    
- 🔰问题：第一层的结果图交叠在一起确实是可以大概划分 4 个区域，如果已经可以判断，要为什么有第二层呢？
    
    您的疑问是合理的。在这个具体的例子中，第一层确实已经可以在很大程度上区分四个类别。然而，这个例子是为了帮助您理解神经网络中各层之间的关系以及神经元如何协同工作。在实际应用中，数据集往往更为复杂，仅依靠一层神经元可能无法有效地区分不同类别。
    
    在多层神经网络中，每一层都负责提取输入数据的不同特征。第一层通常负责捕捉一些低级别的特征，如边缘、线条等。随着神经网络层数的增加，所提取的特征会越来越抽象。因此，在许多情况下，我们需要多层神经网络才能有效地对输入数据进行分类。
    
    在这个例子中，第二层的目的是进一步整合第一层提取的特征，以便更准确地进行分类。尽管在这种简化的场景中，第一层似乎已经足够区分四个类别，但在实际应用中，多层神经网络通常能够更好地处理复杂数据。
    
    - 
- 🔰问题： 怎么知道用了softmax呢？
    
    当使用参数**`linear`**作为输出层的激活函数时，实际上并没有进行softmax操作，因此需要在损失函数中设置**`from_logits=True`**来告诉模型要在softmax函数中使用原始输出而不是已经经过softmax转换后的输出。这样做的原因是在softmax函数的数值计算过程中，如果使用原始输出而非已经转换后的输出，可以避免数值不稳定性。因此，在损失函数中设置**`from_logits=True`**是判断是否使用softmax回归的一个关键标志。
    
- 🔰问题：在上面说的这种情况下，predict 时，需要用过tf.nn.softmax 来转换结果，对吗？
    
    是的，如果输出层使用的是线性激活函数而不是softmax激活函数，那么在预测时需要使用**`tf.nn.softmax`**来将输出转换为概率分布。具体来说，可以使用以下代码进行转换：
    
    ```python
    model = ... # 定义模型
    output = model.predict(x_test) # 预测结果
    probs = tf.nn.softmax(output) # 使用softmax函数将输出转换为概率分布
    ```
    

---

# **其他神经网络概念**

## 高级优化：**[Advanced Optimization](https://www.coursera.org/learn/advanced-learning-algorithms/lecture/5Qt9E/advanced-optimization)**

在这段课程中，关键知识点是介绍一种比梯度下降更好的优化算法——Adam算法。以下是关于Adam算法的简要总结：

- 梯度下降的局限性：在某些情况下，**梯度下降可能需要更大或更小的学习率α来更快地达到最优解。**如果梯度下降沿着大致相同的方向连续移动，我们可能希望增加学习率。相反，如果参数在两个方向之间来回震荡，我们可能希望减小学习率。
    
    ![截屏2023-04-04 21.35.34.png](2%202%20%E9%AB%98%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20811c5785a8c24f51bc083416b17b00e7/%25E6%2588%25AA%25E5%25B1%258F2023-04-04_21.35.34.png)
    
- **Adam算法（Adaptive Moment Estimation）：这是一种自适应优化算法，可以根据参数的变化自动调整学习率。**Adam算法为模型的每个参数使用不同的学习率。如果参数似乎始终沿着大致相同的方向移动，那么我们可以加快该方向上的学习率。相反，如果参数在两个方向之间来回震荡，我们可以减小相应参数的学习率。
- 如何在代码中实现Adam算法：在编译模型时，只需在编译函数中添加一个额外的参数，指定要使用的优化器为 `**tf.keras.optimizers.Adam**`。尽管Adam算法会自动调整学习率，但在实际应用中，仍建议尝试几个不同的初始学习率，找到能带来最快学习性能的值。
    
    ![截屏2023-04-04 21.35.56.png](2%202%20%E9%AB%98%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20811c5785a8c24f51bc083416b17b00e7/%25E6%2588%25AA%25E5%25B1%258F2023-04-04_21.35.56.png)
    

总之，Adam算法通常比梯度下降更快，已成为训练神经网络的事实标准。大多数实践者如今会选择使用Adam算法而不是梯度下降算法。通过使用Adam算法，您的学习算法可能会更快地收敛。在接下来的课程中，您将学习到关于神经网络的更多高级概念，包括一些替代层类型。

> **本节一句话总结：**
Adam 算法：根据参数的变化自动调整学习率
→编译模型：**`model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3), loss= SparseCategoricalCrossentropy(from_logits=True))`**
> 

## 附加层类型：**[Additional Layer Types](https://www.coursera.org/learn/advanced-learning-algorithms/lecture/L0aFK/additional-layer-types)**

什么是卷积层？为什么使用卷积层可以加快计算速度并减少训练数据的需求？请用具体例子解释卷积层在分类 EKG 信号方面的应用。

- **卷积层是神经网络中的一种类型的层，它的特点是每个神经元只看一小部分输入**。相比之下，稠密层的每个神经元会看到上一层的全部神经元输出。
    
    ![截屏2023-04-04 21.41.47.png](2%202%20%E9%AB%98%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20811c5785a8c24f51bc083416b17b00e7/%25E6%2588%25AA%25E5%25B1%258F2023-04-04_21.41.47.png)
    
- 使用卷积层可以**加速计算速度**，因为每个神经元只看一小部分输入，而不是全部输入。此外，卷积层可以减少训练数据的需求或减少过拟合的发生。
- 以分类 EKG 信号为例，卷积神经网络（CNN）可以将 EKG 信号作为一维输入，每个隐藏单元只看一小部分输入。例如，第一个隐藏单元可以只看前20个数据点，第二个隐藏单元可以只看第11到第30个数据点。这种方式可以加速计算速度，减少训练数据的需求，并在分类任务上达到很好的效果。
    
    ![截屏2023-04-04 21.47.51.png](2%202%20%E9%AB%98%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20811c5785a8c24f51bc083416b17b00e7/%25E6%2588%25AA%25E5%25B1%258F2023-04-04_21.47.51.png)
    

> **本节一句话总结：**
卷积层：每个神经元只看到一小部分输入。加速运算速度。
> 

---

# **反向传播（可选）**

## 什么是导数：**[What is a derivative?](https://www.coursera.org/learn/advanced-learning-algorithms/lecture/i9Dqr/what-is-a-derivative-optional)**

这段视频中讨论的关键知识点是**反向传播算法和导数的定义和应用**。

- 首先，视频中介绍了通过在TensorFlow中指定神经网络架构来计算输出y作为输入x的函数，并指定成本函数，然后TensorFlow会自动使用反向传播来计算导数并使用梯度下降或Adam来训练神经网络的参数。反向传播算法是神经网络学习中的关键算法。
- 视频中还讨论了导数的定义，并给出了导数的例子。导数的定义是函数值的微小变化与自变量的微小变化之比。这里还介绍了一个近似求导的方法，其中用一个微小量Epsilon代替微小的变化，然后计算函数值的变化与Epsilon之比，这个比率就是导数。
    
    ![截屏2023-04-04 21.54.23.png](2%202%20%E9%AB%98%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20811c5785a8c24f51bc083416b17b00e7/%25E6%2588%25AA%25E5%25B1%258F2023-04-04_21.54.23.png)
    
    ![截屏2023-04-04 21.55.02.png](2%202%20%E9%AB%98%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20811c5785a8c24f51bc083416b17b00e7/%25E6%2588%25AA%25E5%25B1%258F2023-04-04_21.55.02.png)
    
- 视频中使用的例子是一个简化的成本函数，即J(w) = w^2，其中w是参数。视频中计算了J(w)对于w的导数，即如果w增加微小量Epsilon，J(w)将增加大约k倍的Epsilon，这意味着J(w)对w的导数是k。
    
    ![截屏2023-04-04 21.58.37.png](2%202%20%E9%AB%98%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20811c5785a8c24f51bc083416b17b00e7/%25E6%2588%25AA%25E5%25B1%258F2023-04-04_21.58.37.png)
    
    - w微小变化引起 J(w) k 倍微小变化
    
    ![截屏2023-04-05 10.43.19.png](2%202%20%E9%AB%98%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20811c5785a8c24f51bc083416b17b00e7/%25E6%2588%25AA%25E5%25B1%258F2023-04-05_10.43.19.png)
    
    ![截屏2023-04-04 21.59.30.png](2%202%20%E9%AB%98%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20811c5785a8c24f51bc083416b17b00e7/%25E6%2588%25AA%25E5%25B1%258F2023-04-04_21.59.30.png)
    
- 最后，视频中还介绍了梯度下降算法和如何使用导数来更新参数。
    
    ![截屏2023-04-04 22.02.06.png](2%202%20%E9%AB%98%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20811c5785a8c24f51bc083416b17b00e7/%25E6%2588%25AA%25E5%25B1%258F2023-04-04_22.02.06.png)
    

这些结果的推导使用了微积分的基础概念。了解这些基础概念可以帮助我们更好地理解反向传播算法。

参考视频：【【官方双语】深度学习之反向传播算法 上/下 Part 3 ver 0.9 beta-哔哩哔哩】 [https://b23.tv/0G6IyDe](https://b23.tv/0G6IyDe)

导数相当于敏感值，如 w 的单位变化对结果影响有多大。值越大影响越大，单位变化引起结果的变化越大。

![8291CDDF-DA2F-4378-8576-312D6E1926B9_1_102_o.jpeg](2%202%20%E9%AB%98%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20811c5785a8c24f51bc083416b17b00e7/8291CDDF-DA2F-4378-8576-312D6E1926B9_1_102_o.jpeg)

![AF951499-B1D2-44C6-9F82-6B21ED52B123_1_102_o.jpeg](2%202%20%E9%AB%98%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20811c5785a8c24f51bc083416b17b00e7/AF951499-B1D2-44C6-9F82-6B21ED52B123_1_102_o.jpeg)

> **本节一句话总结：**
dJ(w)/dw： w 微小变化引起 J 等比例变化。权重影响
> 

## 计算图：**[Computation graph](https://www.coursera.org/learn/advanced-learning-algorithms/lecture/rhcTZ/computation-graph-optional)**

作者以计算图为基础，手动地执行了一个**小型神经网络**的前向传播和反向传播算法。以下是关键知识点的中文讲解：

- 计算图：**计算图将神经网络计算的所有步骤分解为一系列相互连接的节点**。每个节点表示一个计算步骤，例如加法、乘法等。
- **不是带有 x 和 y 轴的图，而是计算机科学中另一种意义上的图，即由节点组成并通过边或箭头连接的图。**这个计算图展示了如何计算神经网络的输出 a 的前向传播步骤。然后还要计算损失函数 J 的值。
    
    ![截屏2023-04-04 22.21.19.png](2%202%20%E9%AB%98%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20811c5785a8c24f51bc083416b17b00e7/%25E6%2588%25AA%25E5%25B1%258F2023-04-04_22.21.19.png)
    
- 前向传播：**前向传播是从左到右进行计算的过程**，从输入参数（如权重和偏置）开始，沿着计算图的节点逐步**计算输出和代价函数 J**。
- 反向传播：**反向传播是从右到左进行计算的过程**，用于计算**代价函数 J 关于网络中每个参数（如权重和偏置）的梯度（导数）**。在计算图中，反向传播算法从损失函数开始，反向遍历计算图，并依次计算每个节点对于损失函数的梯度。通过链式法则将这些梯度连接起来，最终得到每个参数的梯度，从而实现神经网络的训练。
    
    ![截屏2023-04-04 22.33.41.png](2%202%20%E9%AB%98%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20811c5785a8c24f51bc083416b17b00e7/%25E6%2588%25AA%25E5%25B1%258F2023-04-04_22.33.41.png)
    
    ![截屏2023-04-04 22.35.16.png](2%202%20%E9%AB%98%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20811c5785a8c24f51bc083416b17b00e7/%25E6%2588%25AA%25E5%25B1%258F2023-04-04_22.35.16.png)
    
    - 首先计算**最终输出节点的导数，然后根据这个导数计算前一层节点的导数**，以此类推，一直计算到第一层节点的导数为止。在计算每个节点的导数时，我们使用链式法则将每一层的导数相乘。具体来说，对于节点J和其输入X和Y，其导数可以通过链式法则计算为dJ/dX = dJ/dZ * dZ/dX，其中Z是J的输入，dJ/dZ是J对Z的导数，dZ/dX是Z对X的导数。
    - 🔰问题：反向传播是由确定的J 计算 wb 的过程吗？ x为变量？
        
        反向传播是一种计算神经网络参数的方法，其中输入是损失函数J对于网络输出的导数，输出是每个参数的梯度。在这个过程中，**损失函数J是确定的，而参数w和b是需要优化的变量。输入x是输入层的变量，它是固定的，并且在训练期间不发生变化。**
        向前传播是指在神经网络中从输入到输出的计算过程，**其中输入值x和神经网络的权重w和偏置b固定，计算出网络的输出和损失函数J的值**。
        
    
    ![截屏2023-04-04 22.39.47.png](2%202%20%E9%AB%98%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20811c5785a8c24f51bc083416b17b00e7/%25E6%2588%25AA%25E5%25B1%258F2023-04-04_22.39.47.png)
    
- 链式法则：在计算梯度时，我们使用链式法则。链式法则是微积分中的一个重要概念，用于计算复合函数的导数。在反向传播中，链式法则使我们能够有效地计算代价函数 J 关于每个参数的梯度。
- 反向传播的高效性：反向传播算法的一个优点是它可以高效地计算梯度。**对于具有 n 个节点和 p 个参数的计算图，反向传播可以在大约 n + p 步中计算所有参数的梯度，**而不是 n * p 步。这使得反向传播在处理大型神经网络时具有很高的计算效率。

通过这个小型神经网络示例，你可以了解到计算图、前向传播和反向传播在深度学习算法中的关键作用。

- 🔰问题：关于n 个节点和 p 个参数的计算图-------- 作者的例子中，节点是4，参数是 2 吗？
    
    是的，作者的例子中有4个节点（分别对应c、a、d和j的计算）和2个参数（分别是权重w和偏置b）。所以在这个例子中，n=4，p=2。
    
- 🔰问题：上一节课把导数的计算变成了乘法、这节把计算图的整个过程变成了加减乘除的简单运算、是这样吗？
    
    是的，**计算图通过将复杂的导数计算分解为更简单的基本操作（如加法、减法、乘法和除法），使得整个计算过程更加直观和易于理解。**同时，这种表示方法也使得计算过程更加高效，因为我们**可以在计算过程中重复使用已经计算过的导数值**，从而避免了不必要的重复计算。因此，计算图使得我们可以更有效地计算梯度，从而优化神经网络的权重和偏置。
    

> **本节一句话总结：**
单个神经元的计算图：将神经网络的计算分解为很多节点，每个节点代表加减乘除
向前传播：w、b、x → J(wb)
向后传播：J(wb)、x → wb
> 

## 更大的神经网络示例：**[Larger neural network example](https://www.coursera.org/learn/advanced-learning-algorithms/lecture/qqczh/larger-neural-network-example-optional)**

这个视频的主要目的是向大家展示如何在更大的神经网络示例中使用计算图。

- 首先，我们有一个具有**单个隐藏层的神经网络，隐藏层包含一个输出为a1的隐藏单元，它输入到输出层以输出最终预测a2**。为了简化数学计算，我们继续使用一个输入x=1，y=5的单个训练示例。网络的参数如下，我们在整个过程中使用ReLU激活函数g(z)=max(0, z)。
- 向前传播：通过分步计算，我们可以将这个过程表示为计算图。在计算图中，我们首先计算w1乘以x，接着计算z1（即t1+b1），然后应用激活函数得到a1。这个过程持续到输出层，最后我们计算代价函数j。
    
    ![截屏2023-04-04 23.08.28.png](2%202%20%E9%AB%98%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20811c5785a8c24f51bc083416b17b00e7/%25E6%2588%25AA%25E5%25B1%258F2023-04-04_23.08.28.png)
    
    ![3CEBAE14-86BE-482B-8064-D0451EBDF16B_1_102_o.jpeg](2%202%20%E9%AB%98%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20811c5785a8c24f51bc083416b17b00e7/3CEBAE14-86BE-482B-8064-D0451EBDF16B_1_102_o.jpeg)
    
    ![3412802F-3439-41F5-A31B-76485BF500EC_1_102_o.jpeg](2%202%20%E9%AB%98%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20811c5785a8c24f51bc083416b17b00e7/3412802F-3439-41F5-A31B-76485BF500EC_1_102_o.jpeg)
    
- 反向传播的过程就是根据计算图从右向左计算各个参数的梯度。我们首先计算代价函数j关于a2的导数。通过链式法则，我们可以逐步计算出j关于w1、b1、w2和b2的导数。
- 在实际神经网络中，**计算图使得我们能够在N+P（而非N×P）个步骤内计算所有导数**，这在节点和参数数量很大的情况下**显著提高了计算效率**。
    
    ![截屏2023-04-04 23.11.16.png](2%202%20%E9%AB%98%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20811c5785a8c24f51bc083416b17b00e7/%25E6%2588%25AA%25E5%25B1%258F2023-04-04_23.11.16.png)
    

通过自动微分（autodiff）算法，现代编程框架（如TensorFlow和PyTorch）可以自动为我们计算梯度，使得我们不再需要手动计算导数。这降低了使用神经网络所需的数学门槛，使得更多人可以方便地应用这些算法。

> **本节一句话总结：**
多层神经元的计算图：让 N+P 步骤内计算所有导数，提高了计算效率
> 

## 可选实验室：积分 [Derivatives](https://www.coursera.org/learn/advanced-learning-algorithms/ungradedLab/1GrQk/optional-lab-derivatives)

非常简单的积分变化的例子，

![截屏2023-04-05 09.50.10.png](2%202%20%E9%AB%98%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20811c5785a8c24f51bc083416b17b00e7/%25E6%2588%25AA%25E5%25B1%258F2023-04-05_09.50.10.png)

![截屏2023-04-05 09.56.16.png](2%202%20%E9%AB%98%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20811c5785a8c24f51bc083416b17b00e7/%25E6%2588%25AA%25E5%25B1%258F2023-04-05_09.56.16.png)

- 🔰问题：这是什么？
    
    这个函数是**`sympy`**库中的**`symbols、`**函数，用于将输入的字符串转换为符号（Symbol）实例。它使得用户可以轻松地创建代数表达式的符号变量
    
    ![截屏2023-04-05 09.57.23.png](2%202%20%E9%AB%98%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20811c5785a8c24f51bc083416b17b00e7/%25E6%2588%25AA%25E5%25B1%258F2023-04-05_09.57.23.png)
    
    ![截屏2023-04-05 10.03.39.png](2%202%20%E9%AB%98%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20811c5785a8c24f51bc083416b17b00e7/%25E6%2588%25AA%25E5%25B1%258F2023-04-05_10.03.39.png)
    
    以下 2 句等效
    
    ![截屏2023-04-05 10.12.57.png](2%202%20%E9%AB%98%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20811c5785a8c24f51bc083416b17b00e7/%25E6%2588%25AA%25E5%25B1%258F2023-04-05_10.12.57.png)
    
    ![截屏2023-04-05 10.13.10.png](2%202%20%E9%AB%98%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20811c5785a8c24f51bc083416b17b00e7/%25E6%2588%25AA%25E5%25B1%258F2023-04-05_10.13.10.png)
    
    ![截屏2023-04-05 10.40.42.png](2%202%20%E9%AB%98%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20811c5785a8c24f51bc083416b17b00e7/%25E6%2588%25AA%25E5%25B1%258F2023-04-05_10.40.42.png)
    

## 可选实验室：[Back propagation](https://www.coursera.org/learn/advanced-learning-algorithms/ungradedLab/4kwdI/optional-lab-back-propagation)

![截屏2023-04-05 10.53.45.png](2%202%20%E9%AB%98%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20811c5785a8c24f51bc083416b17b00e7/%25E6%2588%25AA%25E5%25B1%258F2023-04-05_10.53.45.png)

![截屏2023-04-05 11.13.48.png](2%202%20%E9%AB%98%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20811c5785a8c24f51bc083416b17b00e7/%25E6%2588%25AA%25E5%25B1%258F2023-04-05_11.13.48.png)

![截屏2023-04-05 10.52.44.png](2%202%20%E9%AB%98%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20811c5785a8c24f51bc083416b17b00e7/%25E6%2588%25AA%25E5%25B1%258F2023-04-05_10.52.44.png)

![截屏2023-04-05 11.08.48.png](2%202%20%E9%AB%98%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20811c5785a8c24f51bc083416b17b00e7/%25E6%2588%25AA%25E5%25B1%258F2023-04-05_11.08.48.png)

## **实践实验室：[神经网络培训](https://www.coursera.org/learn/advanced-learning-algorithms/programming/Qf8Cg/practice-lab-neural-networks-for-multiclass-classification)**

![截屏2023-04-05 11.35.21.png](2%202%20%E9%AB%98%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20811c5785a8c24f51bc083416b17b00e7/%25E6%2588%25AA%25E5%25B1%258F2023-04-05_11.35.21.png)

![截屏2023-04-05 11.31.57.png](2%202%20%E9%AB%98%E7%BA%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%20811c5785a8c24f51bc083416b17b00e7/%25E6%2588%25AA%25E5%25B1%258F2023-04-05_11.31.57.png)