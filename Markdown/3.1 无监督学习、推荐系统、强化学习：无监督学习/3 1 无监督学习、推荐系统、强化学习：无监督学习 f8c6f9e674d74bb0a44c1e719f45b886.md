# 3.1 无监督学习、推荐系统、强化学习：无监督学习

復習: Done
日付: 2023年4月9日
最終更新日時: 2023年4月16日 9:41

## 欢迎：[Welcome!](https://www.coursera.org/learn/unsupervised-learning-recommenders-reinforcement-learning/lecture/SEpVK/welcome)

在这门课程中，你将学习无监督学习、推荐系统和强化学习。以下是每周的关键知识点：

第一周 - **无监督学习**：

1. 聚类算法：聚类算法是一种将数据分组到不同簇的方法。它有助于发现数据中的模式和结构。常见的聚类算法包括 K-means、DBSCAN 和层次聚类。
2. 异常检测：异常检测用于识别不符合数据集正常行为的数据点。它在欺诈检测、入侵检测和系统健康监控等领域有广泛应用。常见的异常检测方法包括基于统计学的方法、基于距离的方法和基于密度的方法。

第二周 - **推荐系统**：Recommender Systems

1. 协同过滤：推荐系统的一种常见方法，根据用户之间的相似性或项目之间的相似性来进行推荐。协同过滤有两种主要类型：用户-用户协同过滤和项目-项目协同过滤。
2. 矩阵分解：这是另一种推荐系统的方法，它将用户-项目评分矩阵分解为两个低维矩阵（用户矩阵和项目矩阵），以揭示潜在特征。常见的矩阵分解技术包括奇异值分解（SVD）和非负矩阵分解（NMF）。

第三周 - **强化学习**：Reinforcement Learning

1. 马尔可夫决策过程（MDP）：强化学习的基本框架，包括状态、动作、转移概率、奖励和折扣因子。通过 MDP，智能体可以根据与环境的交互来学习如何做出决策。
2. 值函数和策略：值函数表示从给定状态开始采取特定策略的预期回报。策略是智能体在每个状态下选择动作的规则。强化学习的目标是找到最优策略，使值函数最大化。
3. Q-learning 和 Deep Q-Network（DQN）：Q-learning 是一种基于值迭代的强化学习算法，用于估计动作-值函数（Q值）。Deep Q-Network 将深度学习和 Q-learning 结合，使用神经网络作为函数逼近器，以处理高维和连续的状态空间。

---

# 聚类：**Clustering**

## 聚类：[What is clustering?](https://www.coursera.org/learn/unsupervised-learning-recommenders-reinforcement-learning/lecture/zoCuG/what-is-clustering)

*ˈkləstər* 

在这段内容中，你学习了聚类的概念和一些应用。以下是关键知识点：

- **聚类 Clustering**：聚类是一种无监督学习方法，**它将相似或相关的数据点自动分组到一起。**与监督学习（如二分类）相比，**无监督学习没有目标标签y**，因此我们需要让算法找到数据中的有趣结构或模式。
    
    ![截屏2023-04-09 15.01.45.png](3%201%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%20f8c6f9e674d74bb0a44c1e719f45b886/%25E6%2588%25AA%25E5%25B1%258F2023-04-09_15.01.45.png)
    
    ![截屏2023-04-09 14.53.56.png](3%201%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%20f8c6f9e674d74bb0a44c1e719f45b886/%25E6%2588%25AA%25E5%25B1%258F2023-04-09_14.53.56.png)
    
- 聚类算法：聚类算法寻找数据中一种特定类型的结构，即将数据点分组到相似的簇中。例如，在给定的数据集中，聚类算法可能会发现两个相似的数据点集合。
- 聚类应用：以下是聚类的一些应用实例：
    - 新闻文章分组：将相似的新闻文章自动分组在一起。
    - 市场细分：对客户进行分组，以便更好地了解他们的需求和特点。
    - DNA数据分析：根据个体的基因表达数据对他们进行分组，以发现具有相似特征的群体。
    - 天文数据分析：通过聚类分析天文数据，将天体分组以识别宇宙中的相互关联的结构。
    
    ![截屏2023-04-09 14.54.03.png](3%201%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%20f8c6f9e674d74bb0a44c1e719f45b886/%25E6%2588%25AA%25E5%25B1%258F2023-04-09_14.54.03.png)
    

接下来，你将学习一种最常用的聚类算法——K-means算法。在后续的学习过程中，你将深入了解K-means算法的工作原理和实现方法。

> **一句话总结：**
Clustering 聚类算法：相似的数据点自动划分组。和 supervised learning 相比、只有 x、没有目标答案 y。
应用领域：新闻文章分组、市场细分、DNA 分析等
> 

## K-means聚类直觉：[K-means intuition](https://www.coursera.org/learn/unsupervised-learning-recommenders-reinforcement-learning/lecture/xS8nN/k-means-intuition)

在这段内容中，你学习了K-means聚类算法的工作原理。以下是关键知识点：

- **K-means聚类**：K-means聚类算法是一种无监督学习方法，**用于将相似的数据点分组到一起。**在本例中，我们要求算法找到两个簇（群集）。
- 随机初始化：K-means首先随机猜测簇的中心（称为簇质心 Cluster centroides）。这只是一个初始猜测，可能并不是最佳选择，但它是一个开始。
- K-means的两个关键步骤：K-means算法反复执行两个步骤，直到收敛。这些步骤是：
    - **将点分配给簇质心**：算法检查每个数据点，判断它离哪个簇质心更近，并将其分配给最近的簇质心。
        
        ![截屏2023-04-09 15.13.19.png](3%201%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%20f8c6f9e674d74bb0a44c1e719f45b886/%25E6%2588%25AA%25E5%25B1%258F2023-04-09_15.13.19.png)
        
    - **移动簇质心**：接着，算法计算每个簇中所有数据点的平均位置，并将簇质心移动到这个平均位置。
        
        ![截屏2023-04-09 15.14.02.png](3%201%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%20f8c6f9e674d74bb0a44c1e719f45b886/%25E6%2588%25AA%25E5%25B1%258F2023-04-09_15.14.02.png)
        
        ![截屏2023-04-09 15.14.33.png](3%201%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%20f8c6f9e674d74bb0a44c1e719f45b886/%25E6%2588%25AA%25E5%25B1%258F2023-04-09_15.14.33.png)
        
    - **重复**执行这两个步骤，**直到簇质心的位置和数据点的分配不再发生变化**。此时，K-means算法收敛，找到了合适的簇。
        
        ![截屏2023-04-09 15.15.49.png](3%201%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%20f8c6f9e674d74bb0a44c1e719f45b886/%25E6%2588%25AA%25E5%25B1%258F2023-04-09_15.15.49.png)
        
        ![截屏2023-04-09 15.16.38.png](3%201%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%20f8c6f9e674d74bb0a44c1e719f45b886/%25E6%2588%25AA%25E5%25B1%258F2023-04-09_15.16.38.png)
        
        ![截屏2023-04-09 15.16.46.png](3%201%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%20f8c6f9e674d74bb0a44c1e719f45b886/%25E6%2588%25AA%25E5%25B1%258F2023-04-09_15.16.46.png)
        

在本例中，K-means算法成功地将数据点分为两个簇。接下来，你将学习如何将这个过程形式化，并编写执行这个过程的算法。在后续的学习过程中，你将了解K-means算法的数学公式以及如何实现它。

> **一句话总结：**
K-means：将相似的点分组在一起。
步骤：「将点分配给质心→移动质心到平均值」，重复直到无变化
> 

## K-means聚类算法：[K-means algorithm](https://www.coursera.org/learn/unsupervised-learning-recommenders-reinforcement-learning/lecture/GwgDo/k-means-algorithm)

在上一个视频中，你看到了 k-means 算法的运行示例。现在，我们将详细介绍 K-means 算法，以便你能够自己实现它。

- K-means 算法
    - 随机初始化 K 个簇质心（cluster centroids *ˈsenˌtroid*），记为 Mu 1, Mu 2, ..., Mu k。在我们的示例中，K 等于 2。如果红色十字是簇质心 1，蓝色十字是簇质心 2，则红色十字表示 Mu 1 的位置，蓝色十字表示 Mu 2 的位置。
    - 将重复执行以下两个步骤：
        - **将点分配给簇质心。**对于所有 m 个训练样本，我们将**设置 c^i 等于最接近训练样本 x^i 的簇质心的索引**。具体来说，我们将计算 x^i 和 Mu k 之间的距离。我们希望找到使这个距离最小化的 k 值，因为这对应于距离训练样本 x^i 最近的簇质心 Mu k。计算出最小距离的 k 值后，将其设置为 c^i。
            
            ![截屏2023-04-09 15.27.31.png](3%201%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%20f8c6f9e674d74bb0a44c1e719f45b886/%25E6%2588%25AA%25E5%25B1%258F2023-04-09_15.27.31.png)
            
        - 移动簇质心。对于每个簇 k（k 从 1 到大 K），我们将更新簇质心的位置为分配给该簇的所有点的平均值（或均值）。具体来说，我们将计算所有红点（例如）在水平轴上的位置的平均值，并计算垂直轴上的值的平均值。计算出这两个平均值后，找到新的 Mu 1 位置，即红色簇质心的新位置。对蓝色簇质心也执行相同的操作。
            
            ![截屏2023-04-09 15.33.17.png](3%201%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%20f8c6f9e674d74bb0a44c1e719f45b886/%25E6%2588%25AA%25E5%25B1%258F2023-04-09_15.33.17.png)
            
- 注意事项
    - 需要注意的是，如果一个簇没有分配到任何训练样本，那么在第二步中更新 Mu k 时会出现问题。在这种情况下，最常见的做法是消除该簇，这样就只剩下 K-1 个簇。或者，如果你确实需要 K 个簇，另一种选择是随机重新初始化该簇质心，希望下一轮能分配到一些点。
    - 尽管我们主要讨论了在簇之间有明显分界的情况下的 K-means，但事实上 **K-means 也经常应用于簇之间没有明显分界的数据集**。例如，如果你想确定 T 恤的小、中、大尺寸，你可以收集可能购买你 T 恤的人的身高和体重数据。你会发现在这种情况下，身高和体重数据可能没有明显的簇分界，但是仍然可以使用 K-means 算法来确定 T 恤的尺寸。在这种情况下，你可以将 K 设为 3，以便找到三个最常见的身高和体重组合。然后，你可以根据这三个簇的中心来设计小、中、大尺寸的 T 恤。
        
        ![截屏2023-04-09 15.40.35.png](3%201%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%20f8c6f9e674d74bb0a44c1e719f45b886/%25E6%2588%25AA%25E5%25B1%258F2023-04-09_15.40.35.png)
        
        尽管数据集可能没有明显的分界线，K-means 仍然可以有效地找到数据的集群结构。这可以帮助你更好地了解你的客户群，并确定最适合他们需求的产品尺寸。总之，K-means 算法可以应用于多种不同类型的数据集，包括那些没有明显簇分界的数据集。通过使用 K-means 算法，你可以在数据中找到有趣的结构和模式，从而帮助你更好地理解你的客户需求和优化你的产品设计。
        

> **一句话总结：**
K-means 算法：随机质心 mu_k 、样本x_i与mu_k距离、距离近分配为c_i、移动质心到平均值
K-means 也可用于没有明显分界线的数据集。
> 

## 优化：[Optimization objective](https://www.coursera.org/learn/unsupervised-learning-recommenders-reinforcement-learning/lecture/f5G5k/optimization-objective)

*ˌɑptəməˈzeɪʃ(ə)n*

本课程关注 K-means 算法的成本函数。在监督学习算法中，我们通常使用梯度下降或其他方法优化成本函数。事实证明，K-means 算法也在优化一个特定的成本函数。本课程将详细解释这个成本函数及其优化过程。

- **成本函数：**
    
    成本函数 J 是 K-means 算法试图最小化的目标。它表示为 J(C1,...,Cm, μ1,...,μK)，其中 C1,...,Cm 是数据点到聚类中心的分配，μ1,...,μK 是聚类中心的位置。**成本函数的值是所有数据点到其所分配的聚类中心的平均平方距离。**
    
    ![截屏2023-04-09 16.07.40.png](3%201%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%20f8c6f9e674d74bb0a44c1e719f45b886/%25E6%2588%25AA%25E5%25B1%258F2023-04-09_16.07.40.png)
    
    ![截屏2023-04-09 16.07.13.png](3%201%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%20f8c6f9e674d74bb0a44c1e719f45b886/%25E6%2588%25AA%25E5%25B1%258F2023-04-09_16.07.13.png)
    
    - 🔰问题：c_10是x_10被分配给质心的值。比如离质心红色近则c_10为红色。uk是红色。那么uc_10又是什么呢？
        
        在这里，c_10 是一个值，表示第 10 个样本被分配到哪个簇。假设我们有 K 个簇，那么 **c_10 的值将在 1 到 K 之间**。例如，如果 c_10 等于 2，那么这意味着第 10 个样本被分配到了第 2 个簇。
        
        μ_k 是第 k 个簇的质心的位置。例如，如果我们有红色、蓝色和绿色的簇，我们可以将 μ_1、μ_2 和 μ_3 分别表示红色、蓝色和绿色簇的质心位置。
        
        现在，μ_c10 是一个稍微复杂的表示法。它表示的是第 10 个样本被分配到的簇的质心的位置。换句话说，**μ_c10 代表着根据 c_10 的值选取相应簇的质心位置**。例如，如果 c_10 等于 2（表示第 10 个样本被分配到了第 2 个簇），那么 μ_c10 就等于 μ_2，即第 2 个簇（蓝色簇）的质心位置。
        
    - 🔰问题：那么c_10的值是 1-K 之间。u_k和 u_c10 都是质心的位置，如向量(x1,x2)？
        
        是的，您理解得非常正确。**c_10 的值是在 1 到 K 之间的整数，表示第 10 个样本被分配到哪个簇。而 μ_k 和 μ_c10 都是质心的位置，表示为向量 (x1, x2)。**这里的向量 (x1, x2) 是一个二维示例，实际情况下，质心向量的维度取决于数据集的特征数量。
        
        μ_k 是第 k 个簇的质心位置，k 的值在 1 到 K 之间。对于每一个簇，都有一个对应的质心。在 K-means 算法中，我们试图找到最佳的质心位置，使得数据点到其所属簇的质心的距离之和最小。
        
        μ_c10 是一个根据 c_10 值选择的质心位置。这意味着，如果 c_10 的值为 2，那么 μ_c10 就等于 μ_2，即第 2 个簇的质心位置。这样的表示法让我们可以方便地表示数据点到其所属簇质心的距离，从而计算代价函数。
        
- K-means 算法的步骤：
    - 分配数据点到最近的聚类中心：该步骤通过**更新 C1,...,Cm 来尽可能降低成本函数 J，同时保持 μ1,...,μK 固定。**
        
        ![截屏2023-04-09 16.12.19.png](3%201%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%20f8c6f9e674d74bb0a44c1e719f45b886/%25E6%2588%25AA%25E5%25B1%258F2023-04-09_16.12.19.png)
        
    - 移动聚类中心：在该步骤中，**保持 C1,...,Cm 固定，但更新 μ1,...,μK 以尽可能降低成本函数或失真。**
        
        ![截屏2023-04-09 16.13.54.png](3%201%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%20f8c6f9e674d74bb0a44c1e719f45b886/%25E6%2588%25AA%25E5%25B1%258F2023-04-09_16.13.54.png)
        
        - 🔰问题：图上有三个点分别是x1=1,2,11。其中1和 11是uk吗？那 2 是x_i吗？
            
            **假设我们只有一个簇，其中有两个点：x1 = 1 和 x2 = 11。**（所以图中 1 和 11这两个点是 example，黑色的×是假定的质心μ_k）为了找到最佳的质心位置（μ_k），我们需要计算这两个点的平均值。这个平均值是 (1 + 11) / 2 = 6。因此，我们将质心 μ_k 移动到位置 6。 移动前的成本函数为 41，移动后成本函数是 25。
            
- 优化过程 ：**K-means 算法通过最小化成本函数 J 来优化聚类中心的位置。**每个步骤都会调整数据点的分配和聚类中心的位置以减小 J。如果 J 的值不再减小或减小速度非常慢，那么算法可能已经收敛，可以停止迭代。

> **一句话总结：**
K-means 成本函数： J(C1,...,Cm, μ1,...,μK)，其中 C1,...,Cm 是数据点到聚类中心的分配，μ1,...,μK 质心
K-means 降低成本函数：通过最小化成本函数J 来优化质心μK 位置。
步骤：将点分配给质心 **C1,...,Cm** （μ1,...,μK不变）→移动质心到平均值 **μ1,...,μK** （C1,...,Cm不变）
> 

## 初始化 K-means：[Initializing K-means](https://www.coursera.org/learn/unsupervised-learning-recommenders-reinforcement-learning/lecture/lw9LD/initializing-k-means)

在这个视频中，讲述了K-means聚类算法中的两个关键知识点：**如何随机初始化聚类中心，以及如何通过多次尝试随机初始化以找到更好的聚类结果。**

![截屏2023-04-09 16.42.53.png](3%201%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%20f8c6f9e674d74bb0a44c1e719f45b886/%25E6%2588%25AA%25E5%25B1%258F2023-04-09_16.42.53.png)

- 随机初始化聚类中心：在K-means算法的第一步中，我们需要随机选择K个初始聚类中心。通常的方法是从训练集中随机挑选K个样本作为初始聚类中心。这与之前的视频中的随机选择聚类中心的方法略有不同，但这种方法更为常用。
    
    ![截屏2023-04-09 16.45.21.png](3%201%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%20f8c6f9e674d74bb0a44c1e719f45b886/%25E6%2588%25AA%25E5%25B1%258F2023-04-09_16.45.21.png)
    
    ![截屏2023-04-09 16.48.48.png](3%201%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%20f8c6f9e674d74bb0a44c1e719f45b886/%25E6%2588%25AA%25E5%25B1%258F2023-04-09_16.48.48.png)
    
- 多次随机初始化：由于K-means算法容易陷入局部最优解，为了找到更好的聚类结果，可以多次运行K-means算法，每次使用不同的随机初始化方法。运行K-means算法后，计算每次迭代得到的聚类结果的代价函数J，最后选择具有最小代价函数值的聚类结果。通常情况下，**运行50到1000次随机初始化是比较常见的做法**。
    
    ![截屏2023-04-09 16.50.44.png](3%201%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%20f8c6f9e674d74bb0a44c1e719f45b886/%25E6%2588%25AA%25E5%25B1%258F2023-04-09_16.50.44.png)
    

这两个关键知识点可以帮助我们在实际应用中更好地运用K-means算法，通过多次尝试随机初始化以提高聚类结果的质量。在下一个视频中，将讨论如何选择聚类中心的数量，即如何确定K值。

> **一句话总结：**
随机初始化质心
多次随机初始化找到J 最小的解 （避免局部最优解）、50-1000 次
> 

## 选择簇数：[Choosing the number of clusters](https://www.coursera.org/learn/unsupervised-learning-recommenders-reinforcement-learning/lecture/LK4Zn/choosing-the-number-of-clusters)

在这个视频中，讨论了如何为K-means算法选择合适数量的聚类中心（即k值）。以下是关键知识点的详细解释：

1. 选择k值：对于许多聚类问题，**正确的k值是模糊的。**对于同一个数据集，不同的人可能会看到不同数量的聚类，因为聚类作为无监督学习算法没有给定的“正确答案”。
    
    ![截屏2023-04-09 16.55.11.png](3%201%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%20f8c6f9e674d74bb0a44c1e719f45b886/%25E6%2588%25AA%25E5%25B1%258F2023-04-09_16.55.11.png)
    
2. 拐点方法（Elbow Method *ˈɛlˌboʊ*）：尽管在学术文献中有一些用于自动选择k值的技巧，但其中的一个方法叫做“拐点方法”。在这个方法中，你需要运行K-means算法，使用不同的k值，并将成本函数（或者称为失真函数，Distortion Function）J与聚类数量绘制成图。如果你发现成本函数随着聚类数量的增加而显著减少，那么你**可以选择使成本函数显著减少的k值**。然而，拐点方法并不总是能给出一个明确的拐点，因此在实际应用中可能并不总是有用。
    
    ![截屏2023-04-09 17.00.27.png](3%201%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%20f8c6f9e674d74bb0a44c1e719f45b886/%25E6%2588%25AA%25E5%25B1%258F2023-04-09_17.00.27.png)
    
3. **基于下游任务选择k值**：实际上，通常会将K-means算法生成的聚类用于后续任务。在这种情况下，可以根据K-means在下游任务中的表现来评估k值。例如，在T恤尺寸的例子中，你可以尝试使用k=3和k=5生成聚类，然后根据成本效益分析（如成本、运输、生产等）来决定哪个k值更合适。同样，在图像压缩的例子中，可以根据压缩图像的质量和大小来选择合适的k值。
    
    ![截屏2023-04-09 17.24.36.png](3%201%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%20f8c6f9e674d74bb0a44c1e719f45b886/%25E6%2588%25AA%25E5%25B1%258F2023-04-09_17.24.36.png)
    
    - 🔰问题：这里画了两张人的图像、这是干嘛呢？
        
        在提到的视频中，使用 K-means 聚类算法进行图像压缩是一个有趣的例子。这里展示了图像质量和压缩程度之间的权衡。**当你使用 K-means 对图像进行压缩时，实际上是在降低图像的颜色数量。这样可以减少存储每个像素所需的位数，从而节省空间。**
        
        在这个例子中，K 表示用于表示图像中的颜色的聚类数量。较小的 K 值会导致更高的压缩率，但可能会损害图像质量。较大的 K 值则会导致较低的压缩率，但图像质量可能更好。这种权衡使你可以根据期望的图像质量和压缩程度来选择合适的 K 值。
        

总之，在实际应用中选择k值通常需要在模型性能和具体任务需求之间进行权衡。在某些情况下，可以尝试多个k值并根据实际情况选择最合适的k值。

> **一句话总结：**
选择K-means 的 k 的个数：没有准确答案、用拐点 Elbow Method 方法、根据下游任务
> 

## 实验室：[k-means](https://www.coursera.org/learn/unsupervised-learning-recommenders-reinforcement-learning/programming/hxhn9/k-means)

**`np.linalg.norm`** 是一个 NumPy 函**数，用于计算向量或矩阵的范数**（通常为 L2 范数，即欧几里得距离）。它不是为结果创建新向量的函数，而是计算输入向量或矩阵的大小（长度）。

在 K-means 算法中，我们经常需要计算数据点与聚类中心之间的距离。当我们使用 L2 范数时，这相当于计算两点之间的欧几里得距离。**`np.linalg.norm`** 函数可以接受一个向量（一维数组）或矩阵（二维数组）作为输入，并返回它们的范数。当输入是一个矩阵时，可以通过指定 **`axis`** 参数来沿某个轴计算范数。

例如，假设我们有两个点 A 和 B，它们的坐标分别为 **`(x1, y1)`** 和 **`(x2, y2)`**。我们可以使用 **`np.linalg.norm`** 计算它们之间的欧几里得距离：

```python
import numpy as np

A = np.array([x1, y1])
B = np.array([x2, y2])

#在这个示例中，A - B 是两点之间的差向量，
#np.linalg.norm(A - B) 计算了这个差向量的长度，即 A 和 B 之间的距离。
distance = np.linalg.norm(A - B)
```

![截屏2023-04-09 21.09.29.png](3%201%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%20f8c6f9e674d74bb0a44c1e719f45b886/%25E6%2588%25AA%25E5%25B1%258F2023-04-09_21.09.29.png)

![截屏2023-04-09 21.22.59.png](3%201%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%20f8c6f9e674d74bb0a44c1e719f45b886/%25E6%2588%25AA%25E5%25B1%258F2023-04-09_21.22.59.png)

- 🔰问题：1.points = X[idx == k]中 X[idx == k]中在做什么？如果不相等怎么办？ points 是m,n的向量吗？ 2. mean 函数是什么意思？
    
    这个 **`compute_centroids`** 函数的目的是根据每个数据点分配给的聚类中心来计算新的聚类中心。以下是两个问题的解答：
    
    1. **`points = X[idx == k]`** 这一行代码是在筛选出所有分配给聚类中心 **`k`** 的数据点。`**idx` 是一个一维数组，其中包含每个数据点在 `X` 中所分配的聚类中心的索引。`idx == k`** 会产生一个布尔数组，其中每个值表示对应的数据点是否被分配给聚类中心 **`k`**。然后，通过将这个布尔数组应用于 **`X`**，我们获得了一个新的数组 **`points`**，其中仅包含分配给聚类中心 **`k`** 的数据点。因此，**`points`** 是一个形状为 **`(m', n)`** 的数组，其中 **`m'`** 是分配给聚类中心 **`k`** 的数据点的数量。
    2. **`np.mean`** 函数用于计算输入数组沿指定轴的均值。在这里，我们使用 **`np.mean(points, axis=0)`** 来计算 **`points`** 数组中每个特征的均值。**`axis=0`** 表示我们沿第一个轴（数据点所在的轴）计算均值。这将为每个特征计算一个均值，从而得到一个形状为 **`(n,)`** 的数组。这个数组表示新的聚类中心 **`k`** 的坐标。
- 🔰问题：我不太明白这几个向量的形状是怎么来的？
    
    首先，让我们回顾一下这个问题中涉及的向量及其形状。
    
    1. **`X`**：这是一个形状为 **`(m, n)`** 的二维数组，**其中包含了 `m` 个数据点，每个数据点具有 `n` 个特征。**
    2. **`idx`**：这是一个形状为 **`(m,)`** 的一维数组，其中包含每个数据点在 **`X`** 中所分配的聚类中心的索引。数组的长度与数据点的数量相同。
    3. **`points`**：这是一个形状为 **`(m', n)`** 的二维数组，其中仅包含分配给特定聚类中心（如 **`k`**）的数据点。**`m'`** 是分配给聚类中心 **`k`** 的数据点的数量，这个数量可能小于 **`m`**（即总数据点的数量）。
    4. **`centroids`**：这是一个形状为 **`(K, n)`** 的**二维数组，其中包含了 `K` 个聚类中心的坐标，每个聚类中心具有 `n` 个特征。**
    
    现在让我们回顾一下这些向量是如何计算的。
    
    1. 对于 **`points`**：我们首先通过 **`idx == k`** 创建一个布尔数组，其中每个元素表示对应的数据点是否被分配给聚类中心 **`k`**。然后，将这个布尔数组应用于 **`X`** 以获得一个新的数组 **`points`**，其中仅包含分配给聚类中心 **`k`** 的数据点。因此，**`points`** 的形状是 **`(m', n)`**，其中 **`m'`** 是分配给聚类中心 **`k`** 的数据点的数量。
    2. 对于 **`centroids`**：我们需要为每个聚类中心 **`k`** 计算其新的坐标。我们通过计算分配给聚类中心 **`k`** 的所有数据点的均值来完成这个任务。使用 **`np.mean(points, axis=0)`**，我们可以沿第一个轴（即数据点所在的轴）计算 **`points`** 中每个特征的均值。这将为每个特征计算一个均值，从而得到一个形状为 **`(n,)`** 的数组。这个数组表示新的聚类中心 **`k`** 的坐标。然后，我们将这个坐标数组分配给 **`centroids[k]`**，从而最终构建一个形状为 **`(K, n)`** 的二维数组。

---

# 异常检测：**Anomaly detection**

## 异常检测算法：[Finding unusual events](https://www.coursera.org/learn/unsupervised-learning-recommenders-reinforcement-learning/lecture/1FML2/finding-unusual-events)

在这个视频中，主讲人介绍了异常检测算法，它是一种无监督学习方法。**异常检测算法通过观察一组未标记的正常事件数据来学习，从而检测到异常或不寻常的事件。**在这个例子中，异常检测被用于检测可能存在问题的飞机发动机。

![截屏2023-04-09 22.20.57.png](3%201%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%20f8c6f9e674d74bb0a44c1e719f45b886/%25E6%2588%25AA%25E5%25B1%258F2023-04-09_22.20.57.png)

异常检测算法的关键知识点如下：

- 特征：算法需要一组特征来描述数据。在这个例子中，x1 表示发动机产生的热量，x2 表示发动机的振动强度。
- 密度估计（Density estimation*ˈdɛnsədi ˌɛstəˈmeɪʃən*）：异常检测的常见方法是使用密度估计。首先，根据训练集中的 m 个样本建立一个模型，表示特征 x1 和 x2 的概率分布。然后，**算法将尝试找出具有高概率和低概率的特征值**。
    
    ![截屏2023-04-09 22.24.20.png](3%201%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%20f8c6f9e674d74bb0a44c1e719f45b886/%25E6%2588%25AA%25E5%25B1%258F2023-04-09_22.24.20.png)
    
- 概率阈值（Epsilon）：在模型中，通过计算新测试样本的概率值，**可以判断它是否可能是异常。**如果概率值小于阈值（一个较小的数值），则认为该样本可能是异常；如果概率值大于或等于阈值，则认为该样本正常。
    
    [https://www.notion.so](https://www.notion.so)
    
- 异常检测的应用：异常检测广泛应用于多个领域，如**欺诈检测、制造业和计算机集群与数据中心的监控。**
在欺诈检测中，可以计算用户活动的特征，然后根据异常检测算法筛选可能存在欺诈行为的用户。在制造业中，可以用于检测飞机发动机、印刷电路板、智能手机等产品的异常。在计算机集群和数据中心监控中，可以监测计算机性能参数，如内存使用、磁盘访问次数等，以检测潜在的硬件故障或黑客攻击。

> **一句话总结：**
异常检测：未标记数据检测不寻常事件。
密度估计（找高概率和低概率的特征值）→ 概率阈值 Epsilon 判断异常
应用：欺诈检测、制造业、计算机集群与数据中心的监控。
> 

## 高斯分布：[Gaussian (normal) distribution](https://www.coursera.org/learn/unsupervised-learning-recommenders-reinforcement-learning/lecture/1pURx/gaussian-normal-distribution)

在这段文字中，关键知识点涉及高斯分布（或正态分布）以及如何将其应用于异常检测。下面，我们详细解释这些关键概念。

- 高斯分布（正态分布）：
**高斯分布（也称为正态分布或钟形曲线）是一种描述随机变量的概率分布，其概率密度函数呈钟形。**它有两个参数：**均值（μ）和标准差（σ）**（标准差的平方称为方差（σ²））。概率密度函数的公式为：p(x) = (1 / √(2πσ²)) * e^(-(x - μ)² / (2σ²))
    
    ![截屏2023-04-09 22.38.35.png](3%201%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%20f8c6f9e674d74bb0a44c1e719f45b886/%25E6%2588%25AA%25E5%25B1%258F2023-04-09_22.38.35.png)
    
- 如何改变μ和σ会影响高斯分布：（面积保持一致）
    - **当μ变化时，**分布的中心会改变，即**高斯分布的峰值将移动到新的μ值处**。
    - **当σ变化时**，分布的形状会改变。**较小的σ值会产生较尖的钟形曲线，而较大的σ值会产生较宽的钟形曲线。**
    
    ![截屏2023-04-09 22.52.58.png](3%201%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%20f8c6f9e674d74bb0a44c1e719f45b886/%25E6%2588%25AA%25E5%25B1%258F2023-04-09_22.52.58.png)
    
    - 🔰标准正态分布
        
        ![3F6CD30F-A53D-436C-981B-EE5BE8258471_1_105_c.jpeg](3%201%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%20f8c6f9e674d74bb0a44c1e719f45b886/3F6CD30F-A53D-436C-981B-EE5BE8258471_1_105_c.jpeg)
        
        ![172EFCB8-4F51-42CF-B1A2-82FB135AA05E_1_105_c.jpeg](3%201%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%20f8c6f9e674d74bb0a44c1e719f45b886/172EFCB8-4F51-42CF-B1A2-82FB135AA05E_1_105_c.jpeg)
        
        ![C200B979-DEAB-42B3-9E3B-ADE8B0FFF0D9_1_105_c.jpeg](3%201%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%20f8c6f9e674d74bb0a44c1e719f45b886/C200B979-DEAB-42B3-9E3B-ADE8B0FFF0D9_1_105_c.jpeg)
        
        ![18314649-E4C3-4E38-8ECD-A0453FBC7826_1_105_c.jpeg](3%201%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%20f8c6f9e674d74bb0a44c1e719f45b886/18314649-E4C3-4E38-8ECD-A0453FBC7826_1_105_c.jpeg)
        
        ![0C16483D-F998-4CE5-B5F7-A0DDF33730C0_1_105_c.jpeg](3%201%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%20f8c6f9e674d74bb0a44c1e719f45b886/0C16483D-F998-4CE5-B5F7-A0DDF33730C0_1_105_c.jpeg)
        
- 异常检测算法：
    - 首先，**根据训练数据集（m个样本）估计高斯分布的参数μ和σ²**。μ是所有训练样本的平均值，σ²是所有训练样本与μ之差的平方的平均值。
    - 使用估计的参数μ和σ²来构建一个高斯分布模型。然后，对于新的观测值x，计算其在该**高斯分布下的概率p(x)**。
    - 如果**p(x)低于某个预先设定的阈值**，那么可以认为**观测值x是异常的**，因为它与训练数据集中的样本相比较不同。
        
        ![截屏2023-04-09 22.57.15.png](3%201%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%20f8c6f9e674d74bb0a44c1e719f45b886/%25E6%2588%25AA%25E5%25B1%258F2023-04-09_22.57.15.png)
        
- 多特征异常检测：
    
    在具有多个特征的异常检测问题中，可以**为每个特征分别估计高斯分布的参数，然后计算每个特征的概率值。**在实际应用中，可能需要考虑特征之间的关联性，并使用多元高斯分布来建立更复杂的异常检测模型。
    

总之，关键知识点包括高斯分布的概念、公式、参数的影响以及如何将高斯分布应用于异常检测。通过理解这些概念，您将能够使用高斯分布解决实际的异常检测问题。

> **一句话总结：**
高斯分布：表述随机变量的概率分布。均值（μ 中心）和标准差（σ）
异常检测：根据数据集计算高斯分布参数，确定概率 P(x)，低于阈值→异常。
> 
> 
> ![截屏2023-04-16 09.22.50.png](3%201%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%20f8c6f9e674d74bb0a44c1e719f45b886/%25E6%2588%25AA%25E5%25B1%258F2023-04-16_09.22.50.png)
> 

## 异常检测算法：[Anomaly detection algorithm](https://www.coursera.org/learn/unsupervised-learning-recommenders-reinforcement-learning/lecture/nZcu2/anomaly-detection-algorithm)

在这段视频中，我们将学习如何构建异常检测算法。**异常检测算法的目的是根据一组训练数据，找出具有异常特征的样本。**以下是关键知识点的详细解释：

- **特征选择**：首先，我们需要选择可能表明异常样本的特征（例如，飞机发动机的热量和振动）。
- 参数拟合：我们要为**每个特征拟合参数 mu (μ) 和 sigma squared (σ²)，**其中 mu 表示特征的平均值，而 sigma squared 表示特征的方差。这些参数可以通过计算训练集中所有样本的特征平均值和方差得到。
- 概率密度函数：**对于每个特征，我们使用高斯分布（也称为正态分布）**来建立概率密度函数。我们假设每个特征之间是统计独立的，即使实际上它们可能不是，这种算法通常也能正常工作。
- 计算概率：**对于一个新的样本（测试样本），我们需要计算其特征向量的概率 p(x)，**这可以通过计算每个特征的概率密度函数并将它们相乘来实现。换句话说，我们计算 p(x) = p(x1) * p(x2) * ... * p(xn)，其中 p(xj) 是第 j 个特征的概率密度函数。
    
    ![截屏2023-04-09 23.07.56.png](3%201%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%20f8c6f9e674d74bb0a44c1e719f45b886/%25E6%2588%25AA%25E5%25B1%258F2023-04-09_23.07.56.png)
    
- 异常检测：最后，我们需要**确定一个阈值 epsilon (ε)，并检查计算出的 p(x) 是否小于这个阈值。如果 p(x) 小于 epsilon，**则我们认为这个样本是异常的。阈值的选择依赖于问题背景和对异常的容忍度。
`**μ = np.sum(X,axic=0)/m`
`σ = (1/m)np.sum(np.square(X-μ),axic=0)`**
    
    ![截屏2023-04-09 23.11.16.png](3%201%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%20f8c6f9e674d74bb0a44c1e719f45b886/%25E6%2588%25AA%25E5%25B1%258F2023-04-09_23.11.16.png)
    
- 通过这个过程，异常检测算法可以判断新样本是否具有异常特征。当一个样本的特征与训练集中的样本相比显著偏离平均水平时，算法会将其标记为异常。
    
    ![截屏2023-04-09 23.14.17.png](3%201%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%20f8c6f9e674d74bb0a44c1e719f45b886/%25E6%2588%25AA%25E5%25B1%258F2023-04-09_23.14.17.png)
    

在实际应用中，我们需要选择合适的特征、参数和阈值以获得最佳的异常检测效果。为了评估和优化异常检测系统的性能，我们还需要关注评价指标（如精确度、召回率等）和方法（如交叉验证等）。

> **一句话总结：**
异常检测算法：根据一组训练数据，找出异常特征样本
新样本的概率密度 p(x) = p(x1) * p(x2) * ... * p(xn)，其中 p(xj) 是第 j 个特征的概率密度函数
`**μ = np.sum(X,axic=0)/m`  
`σ = (1/m)np.sum(np.square(X-μ),axic=0)`**
求和符号：内部数据 在i方向上求和（内部数据运算本身和求和无关）。
> 

## 开发和评估异常检测系统：[Developing and evaluating an anomaly detection system](https://www.coursera.org/learn/unsupervised-learning-recommenders-reinforcement-learning/lecture/WyzeY/developing-and-evaluating-an-anomaly-detection-system)

在这段文字中，作者讨论了如何开发异常检测系统的实际技巧，并强调了在系统开发过程中进行评估的重要性。以下是关键知识点的详细解释：

1. **实数评估（real number evaluation）**：在开发学习算法时，通过计算一个数值来判断算法是否变得更好或更差。这样可以快速决定是否保留对算法的某个修改，从而更好地调整算法。
2. **使用标签数据**：虽然异常检测主要关注无标签数据，但在实际应用中，有一小部分标签数据（如已知的异常样本）是非常有帮助的。这些标签数据可以用于创建交叉验证集和测试集，从而评估和改进算法。
    
    ![截屏2023-04-09 23.34.33.png](3%201%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%20f8c6f9e674d74bb0a44c1e719f45b886/%25E6%2588%25AA%25E5%25B1%258F2023-04-09_23.34.33.png)
    
3. **分割数据集**：将数据集分为训练集、交叉验证集和测试集。训练集用于拟合高斯分布，交叉验证集用于调整参数（如ε）和特征选择，而测试集用于评估算法的最终性能。
    
    ![截屏2023-04-09 23.39.16.png](3%201%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%20f8c6f9e674d74bb0a44c1e719f45b886/%25E6%2588%25AA%25E5%25B1%258F2023-04-09_23.39.16.png)
    
    - 🔰问题：原始数据中 20 个异常数据。这里是training set 是已知其中没有异常吗？ 那在cv 和test 中怎么能预测正确呢？
        
        在这个示例中，我们假设训练集中主要包含正常的引擎（6,000个），即使其中可能有少量的异常引擎。而在交叉验证集（2,000个正常引擎和10个已知异常引擎）和测试集（2,000个正常引擎和10个已知异常引擎）中，我们有一些已知的异常数据。
        
        当我们使用无监督学习算法（如异常检测算法）时，**我们的目标是让算法学会从训练数据中捕捉正常数据的模式。**在这个例子中，我们用训练集训练算法来拟合正态分布，以便捕捉正常引擎的特征。由于训练集中的数据主要是正常的，算法将主要学会识别正常引擎的特征。
        
        然后，在交叉验证集和测试集中，我们可以评估算法在检测已知异常引擎方面的性能。我们可以调整参数（例如 epsilon 阈值）以在尽可能准确地检测异常的同时，避免将过多的正常引擎误判为异常。通过这种方法，我们可以找到最佳的参数设置，以在实际应用中准确地检测异常。
        
    - 🔰问题：那前面的相比、后来去掉了test set 是为什么呢？
        
        在某些情况下，如果异常数据非常稀少，可能没有足够的数据来创建一个独立的测试集。在这种情况下，**可以考虑将所有剩余的数据（包括正常数据和已知的异常数据）放入交叉验证集中，而不是创建一个独立的测试集。**
        
        这种方法的优点是，您可以利用所有可用的已知异常数据来调整算法参数（例如 epsilon 阈值）和特征选择，从而在有限的数据集上获得尽可能好的性能。
        
4. **处理高度偏斜的数据分布**：在异常检测场景中，正样本（异常值）的数量通常远小于负样本（正常值）。**在这种情况下，使用诸如真阳性（True Positive）、假阳性（False Positive）、假阴性（False Negative）和真阴性（True Negative）的评估指标，以及精确度（Precision）、召回率（Recall）和F1分数（F1 Score）**等评估方法，可能比单纯使用分类准确率更为合适。
    
    ![截屏2023-04-09 23.47.59.png](3%201%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%20f8c6f9e674d74bb0a44c1e719f45b886/%25E6%2588%25AA%25E5%25B1%258F2023-04-09_23.47.59.png)
    
5. 异常检测与监督学习的比较：虽然有标签数据，但仍选择使用无监督学习算法进行异常检测。在某些情况下，异常检测可能比监督学习更合适。这取决于数据的特点、异常类型和实际应用场景。

总之，在开发异常检测系统时，使用一小部分已知的异常样本进行评估和调整算法是非常有帮助的。通过实数评估、合理分割数据集以及使用适当的评估指标，可以更好地优化算法，提高异常检测的性能。

> **一句话总结：**
评估异常：实数估计、使用标签数据、分割数据集
> 

## 异常检测与监督学习：[Anomaly detection vs. supervised learning](https://www.coursera.org/learn/unsupervised-learning-recommenders-reinforcement-learning/lecture/eLO9Y/anomaly-detection-vs-supervised-learning)

在这段文字中，作者讨论了在具有少量正例和大量负例的情况下，何时应该选择异常检测，何时应该使用监督学习。以下是关键知识点的详细解释：

1. 异常检测：**异常检测算法更适用于具有非常少量正例（例如0-20个）和相对较多的负例的情况。**异常检测关注的是识别那些与正常（负例）数据显著不同的样本。这种方法在未来可能出现新类型异常的情况下表现较好，因为它主要关注正常数据，并将与之显著不同的数据标记为异常。
2. 监督学习：如果具有较多的正例和负例，那么监督学习可能更适用。**监督学习关注的是识别那些与已知正例类似的样本。**这种方法在未来的正例可能与训练集中的正例类似的情况下表现较好。
    
    ![截屏2023-04-10 19.37.18.png](3%201%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%20f8c6f9e674d74bb0a44c1e719f45b886/%25E6%2588%25AA%25E5%25B1%258F2023-04-10_19.37.18.png)
    
3. 应用示例：
- 异常检测：**金融欺诈检测**（因为可能出现新的欺诈类型）、**制造业中检测新的、以前未见过的缺陷、数据中心机器监控**（以检测是否被黑客攻击）、**安全相关应用**（因为黑客可能发现新的攻击方式）。
- 监督学习：**电子邮件垃圾邮件检测**（因为垃圾邮件类型相对固定）、**制造业中检测已知的缺陷、天气预报**（因为天气类型有限且相对稳定）、**根据病人症状判断是否患有特定已知疾病。**
    
    ![截屏2023-04-10 19.37.57.png](3%201%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%20f8c6f9e674d74bb0a44c1e719f45b886/%25E6%2588%25AA%25E5%25B1%258F2023-04-10_19.37.57.png)
    

> **一句话总结：**
异常检测：更适用于少量未知异常。
监督学习适用于已知的异常。
> 

## 异常检测的特征选择：[Choosing what features to use](https://www.coursera.org/learn/unsupervised-learning-recommenders-reinforcement-learning/lecture/7MOXj/choosing-what-features-to-use)

在这段视频中，讨论了用于异常检测的特征选择和调整技巧。以下是关键知识点的总结：

选择特征：在异常检测中，选择恰当的特征非常重要。相较于有监督学习，异常检测算法更难从无标签数据中学习。因此，选择合适的特征对异常检测来说至关重要。

1. **高斯特征：确保你给异常检测算法的特征大致是高斯分布**。如果特征不是高斯分布，可以尝试对其进行转换，使其更接近高斯分布。例如，可以计算特征X的对数或平方根等。
    
    ![截屏2023-04-10 19.48.07.png](3%201%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%20f8c6f9e674d74bb0a44c1e719f45b886/%25E6%2588%25AA%25E5%25B1%258F2023-04-10_19.48.07.png)
    
2. 用直方图查看特征分布：通过绘制特征的直方图，可以查看特征的分布是否类似于高斯分布。如果发现特征分布不是高斯分布，可以尝试应用不同的变换，例如对数、平方根或指数变换等。
    
    ![截屏2023-04-10 19.51.54.png](3%201%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%20f8c6f9e674d74bb0a44c1e719f45b886/%25E6%2588%25AA%25E5%25B1%258F2023-04-10_19.51.54.png)
    
3. 错误分析：训练异常检测算法后，如果在交叉验证集上表现不佳，可以进行错误分析。这意味着你可以查看算法在哪些地方表现不佳，然后尝试提出改进方法。例如，可以尝试找到新的特征，以帮助算法区分正常和异常样本。
4. 创建新特征：**当算法无法检测到交叉验证集中的某些异常时，可以尝试结合现有特征创建新的特征，以提高算法的性能**。
    
    ![截屏2023-04-10 19.55.27.png](3%201%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%20f8c6f9e674d74bb0a44c1e719f45b886/%25E6%2588%25AA%25E5%25B1%258F2023-04-10_19.55.27.png)
    
    ![截屏2023-04-10 19.59.16.png](3%201%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%20f8c6f9e674d74bb0a44c1e719f45b886/%25E6%2588%25AA%25E5%25B1%258F2023-04-10_19.59.16.png)
    

总之，为了获得最佳的异常检测性能，特征选择和调整非常重要。确保你的特征具有高斯分布，并尝试进行错误分析以优化算法。此外，在开发过程中，训练模型并查看算法无法检测到的交叉验证集中的异常，以引导新特征的创建，从而提高异常检测算法的性能。

> **一句话总结：**
异常检测的特征选择：尽量符合高斯、不符合的进行转换、可创建新特征提高算法性能
> 

## 实验室：[Anomaly Detection](https://www.coursera.org/learn/unsupervised-learning-recommenders-reinforcement-learning/programming/Rqh9M/anomaly-detection)

![截屏2023-04-10 20.30.22.png](3%201%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%20f8c6f9e674d74bb0a44c1e719f45b886/%25E6%2588%25AA%25E5%25B1%258F2023-04-10_20.30.22.png)

![截屏2023-04-10 20.29.55.png](3%201%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%20f8c6f9e674d74bb0a44c1e719f45b886/%25E6%2588%25AA%25E5%25B1%258F2023-04-10_20.29.55.png)

![截屏2023-04-11 10.18.13.png](3%201%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%20f8c6f9e674d74bb0a44c1e719f45b886/%25E6%2588%25AA%25E5%25B1%258F2023-04-11_10.18.13.png)

![截屏2023-04-10 20.53.30.png](3%201%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%20f8c6f9e674d74bb0a44c1e719f45b886/%25E6%2588%25AA%25E5%25B1%258F2023-04-10_20.53.30.png)

![截屏2023-04-10 20.53.56.png](3%201%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%20f8c6f9e674d74bb0a44c1e719f45b886/%25E6%2588%25AA%25E5%25B1%258F2023-04-10_20.53.56.png)

- 🔰问题：相当于把概率密度等分为1000个测试项，分别计算每种测试项下的 epsilon 值和 F1 ，保存了最大值？
    
    是的，你的理解是正确的。在 **`select_threshold`** 函数中，我们将 **`p_val`** 的范围划分为 1000 个间隔，然后在这些间隔内测试不同的 epsilon 值。对于每个 epsilon 值，我们计算对应的 F1 分数。我们希望找到一个使 F1 分数最大化的 epsilon 值，因为这意味着在该阈值下，模型在识别异常数据和避免误判正常数据方面的表现最好。
    
    函数会遍历所有可能的 epsilon 值，并在每次迭代中检查当前 epsilon 对应的 F1 分数是否比之前找到的最佳 F1 分数更高。如果当前 F1 分数更高，我们将更新最佳 F1 分数和最佳 epsilon 值。在遍历完所有 epsilon 值后，我们得到了使 F1 分数最大化的最佳 epsilon 值。
    

🔰问题：y_val 是一个1 维数组吗？如果是数组，y_val == 0 是什么意思呢？

是的，**`y_val`** 是一个一维数组，它包含验证集中每个数据点的标签。在这种情况下，我们关心的是异常值检测，所以标签通常是二元的，其中 0 表示正常数据，1 表示异常数据。

当我们在代码中使用 **`y_val == 0`** 时，这会返回一个布尔数组，其形状与 **`y_val`** 相同，数组中的每个元素表示 **`y_val`** 中对应位置的元素是否等于 0。类似地，**`y_val == 1`** 会返回一个表示 **`y_val`** 中哪些元素等于 1 的布尔数组。

例如，**如果 `y_val` 是一个形状为 `(5,)` 的数组，值为 `[0, 1, 0, 1, 1]`，那么 `y_val == 0` 将返回一个布尔数组 `[True, False, True, False, False]`，而 `y_val == 1` 将返回一个布尔数组 `[False, True, False, True, True]`。**这些布尔数组可以用来计算真阳性、假阳性和假阴性的数量。

🔰问题：np.sum((predictions == 1) & (y_val == 0)) 中(predictions == 1) & (y_val == 0) 是 2 个数组做比较、又返回了一个如形状为(5,)的数组，然后计算其中真的总数吗？

是的，您的理解是正确的。**`(predictions == 1) & (y_val == 0)`** 这部分代码是在对两个布尔数组进行逐元素逻辑与操作。结果是一个新的布尔数组，其中的每个元素表示 **`predictions`** 和 **`y_val`** 中对应位置的元素是否同时满足指定条件（即 **`predictions`** 中的元素等于 1 且 **`y_val`** 中的元素等于 0）。

之后，**`np.sum()`** 函数会将布尔数组中的 **`True`** 值视为 1，**`False`** 值视为 0，并计算数组中所有元素的和。这就给出了满足条件的元素数量，也就是在这种情况下的假阳性（false positives）数量。

![截屏2023-04-10 21.10.45.png](3%201%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%20f8c6f9e674d74bb0a44c1e719f45b886/%25E6%2588%25AA%25E5%25B1%258F2023-04-10_21.10.45.png)