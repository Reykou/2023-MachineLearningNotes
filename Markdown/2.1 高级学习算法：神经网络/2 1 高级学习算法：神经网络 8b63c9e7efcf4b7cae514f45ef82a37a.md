# 2.1 高级学习算法：神经网络

復習: Done
日付: 2023年3月21日
最終更新日時: 2023年4月14日 23:27

# 神经网络直觉：**Neural networks intuition**

## 欢迎 ：[Welcome!](https://www.coursera.org/learn/advanced-learning-algorithms/lecture/3wO3h/welcome)

在这门课程中，您将学习关于神经网络（也称为深度学习算法）和决策树。以下是关键知识点概述：

1. **神经网络**：神经网络是一种强大且广泛使用的机器学习算法，它们可以**进行复杂的模式识别和预测**。**神经网络的基本组成单位是神经元（或称为节点）。**
2. **决策树**：决策树是另一种强大且广泛使用的机器学习算法，**它通过逐层划分数据来进行预测。**虽然它们没有像神经网络那样受到媒体的关注，但它们在实际应用中具有很高的价值。

课程将分为四个星期的内容：

- 第1周：您将学习神经网络的基本知识和如何进行推理（预测）。推理是指使用预先训练好的神经网络进行预测的过程。
- 第2周：您将学习如何训练自己的神经网络。这一周将详细讲解如何使用带有标签的训练集（X和Y）来训练神经网络的参数。
- 第3周：本周将分享关于构建机器学习系统的实用建议，以及一些高效快速地构建实际系统的技巧。
- 第4周：您将学习关于决策树的知识。决策树是一种简单且强大的机器学习算法，它们在实际应用中非常实用。

在学习这门课程的过程中，您将了解到如何在实际场景中构建和应用这些机器学习算法，以及如何更系统地做出更好的决策，以提高您的机器学习应用的效率和效果。

[data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%2730%27%20height=%2730%27/%3e](data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%2730%27%20height=%2730%27/%3e)

## 神经元和大脑：[Neurons and the brain](https://www.coursera.org/learn/advanced-learning-algorithms/lecture/couAA/neurons-and-the-brain)

在这部分课程中，您将了解神经网络的起源、发展和基本原理。以下是关键知识点概述：

- 神经网络的起源：神经网络最初是为了模拟人脑（或生物脑）的学习和思维过程。虽然现代神经网络与人脑的实际学习过程相去甚远，但一些生物学上的启示仍然存在于我们今天对人工神经网络的思考中。
    
    ![Untitled](2%201%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%208b63c9e7efcf4b7cae514f45ef82a37a/Untitled.png)
    
    ![Untitled](2%201%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%208b63c9e7efcf4b7cae514f45ef82a37a/Untitled%201.png)
    
- 神经网络的发展：神经网络研究始于20世纪50年代，之后几经兴衰。自2005年起，神经网络再次兴起并与深度学习紧密相连。现代神经网络已在多个领域产生了革命性的影响，如语音识别、计算机视觉、自然语言处理等。
    
    ![Untitled](2%201%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%208b63c9e7efcf4b7cae514f45ef82a37a/Untitled%202.png)
    
- 为什么神经网络现在变得如此重要：随着互联网、移动设备和数据数字化的发展，我们拥有的数据量不断增长。传统的机器学习算法（如线性回归和逻辑回归）无法充分利用这些大量数据。神经网络在处理大数据时表现出更好的性能，因此在众多领域得到了广泛应用。
    
    ![Untitled](2%201%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%208b63c9e7efcf4b7cae514f45ef82a37a/Untitled%203.png)
    
- 生物神经元与人工神经元：生物神经元通过电脉冲进行信息传递。人工神经网络中的神经元是基于生物神经元的简化数学模型。一个神经元接收一个或多个输入（数字），进行计算，并输出另一个数字。在构建神经网络时，我们通常会模拟许多这样的神经元。

尽管神经网络的起源受到生物学的启示，但现代深度学习研究已转向采用工程原理来构建更有效的算法。在学习神经网络和深度学习时，请不要过于关注生物学的动机。

## 需求预测：[Demand Prediction](https://www.coursera.org/learn/advanced-learning-algorithms/lecture/MsbrF/demand-prediction)

在本课程中，关键知识点包括神经网络的基本概念、组成以及构建神经网络的过程。以下是一些主要概念：

- **神经网络**：神经网络是一种模仿人类大脑的计算模型，**用于解决复杂的模式识别问题。它们通常由多个层组成，包括输入层、隐藏层和输出层。**
    
    ![Untitled](2%201%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%208b63c9e7efcf4b7cae514f45ef82a37a/Untitled%204.png)
    
- **输入层**：神经网络的第一层，负责接收原始数据（如图像、文本或音频）并将其传递给下一层。
    
    ![截屏2023-04-14 16.40.46.png](2%201%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%208b63c9e7efcf4b7cae514f45ef82a37a/%25E6%2588%25AA%25E5%25B1%258F2023-04-14_16.40.46.png)
    
- **输出层**：神经网络的最后一层，负责根据前面层的处理结果生成预测或分类输出。
- **神经元**：神经网络中的基本计算单元，负责接收输入、进行计算并产生输出。**神经元之间通过权重连接，以传递和调整信号**。
    
    ![Untitled](2%201%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%208b63c9e7efcf4b7cae514f45ef82a37a/Untitled%205.png)
    
- **向量：神经网络中用于表示输入、激活值和输出的一维数组。**
- **隐藏层：在输入层和输出层之间的层。隐藏层可以有一个或多个，每个隐藏层包含多个神经元。**隐藏层负责对输入数据进行处理和转换，以提取有意义的特征。
    
    ![Untitled](2%201%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%208b63c9e7efcf4b7cae514f45ef82a37a/Untitled%206.png)
    
- 激活函数：用于在神经元中引入非线性的函数。常用的激活函数包括Sigmoid、ReLU和Tanh等。
- 神经网络架构：神经网络的层数、每层神经元的数量以及它们之间的连接方式。选择合适的架构对于提高神经网络的性能至关重要。

通过本课程，你将了解如何构建和训练神经网络，以解决各种复杂问题。在后续课程中，你还将学习如何选择合适的网络架构以及如何应用神经网络解决实际问题。

> **本节一句话总结：**
神经网络：用于复杂的模式识别问题。由输入层、隐藏层和输出层组成。
神经元：负责接收输入、进行计算并产生结果。神经元之间通过权重连接。
大多说神经网络的输入、激活、输出是一维数组。
> 

## 图像识别：[Example: Recognizing Images](https://www.coursera.org/learn/advanced-learning-algorithms/lecture/RCpEW/example-recognizing-images)

在这段课程中，教授解释了如何将神经网络应用于计算机视觉应用，特别是面部识别。以下是关键知识点的概述：

- 图像表示：一张1000x1000像素的图片在计算机中表示为一个1000x1000矩阵，每个元素对应一个像素的亮度值（0-255）。
- **将图像展开为特征向量：将像素亮度值矩阵展开为一个一维向量，包含一百万个像素亮度值。**
    
    ![Untitled](2%201%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%208b63c9e7efcf4b7cae514f45ef82a37a/Untitled%207.png)
    
- **构建神经网络**：为了实现面部识别，我们可以构建一个神经网络，输入是一个包含一百万个像素亮度值的特征向量，输出是图像中人物的身份。
- **隐藏层学习特征：神经网络的隐藏层可以自动学习特征。**在第一层隐藏层，神经元可能学习检测图像中的短边缘和线条；在第二层隐藏层，神经元可能学习检测面部的各个部分（如眼睛、鼻子等）；在第三层隐藏层，神经元可能学习检测更完整的面部形状。
    
    ![Untitled](2%201%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%208b63c9e7efcf4b7cae514f45ef82a37a/Untitled%208.png)
    
    - 自动特征学习：神经网络能够自动从数据中学习这些特征，而无需手动指定。
- **应用于不同数据集**：通过训练不同的数据集，神经网络可以自动学习检测不同的特征，例如面部识别或汽车检测等。
    
    ![Untitled](2%201%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%208b63c9e7efcf4b7cae514f45ef82a37a/Untitled%209.png)
    

接下来的课程将更深入地讲解神经网络的具体数学原理和实现细节，包括如何构建一个或多个神经网络层以及如何实现这些层。

### 大量疑问

- 🔰问题：不同的层，是训练的人设置的吗？还是机器自己设置的，如果是它设置的，那是怎么区分并确定出不同的层呢？
    
    在神经网络的结构中，**隐藏层的数量和每层神经元的数量通常是由人工事先设定的**。当您设计神经网络时，您需要决定隐藏层数量以及每层神经元的数量。然后，神经网络通过学习过程自动学习每个隐藏层中神经元的权重和激活函数。
    
    虽然隐藏层数量和每层神经元的数量是人为设定的，但**神经网络在训练过程中会自动学习从输入数据中提取有用特征的能力。在训练期间，神经网络通过调整权重和偏置来最小化预测误差。**这个过程中，不同层的神经元自然地学会检测不同层次的特征，因为每一层都从前一层的输出中提取信息。
    
- 🔰问题：相当和input x接触的层先得到一个小结果a1，这个结果a1是人工设定目标，通过如梯度下降等算法得到收敛的wb。然后向量a1再重复前面的过程，得到另一个小目标a2...以此类推，最终得到我们想要的结果。每一层的wb 也就确定了，简单的这样理解对吗？
    
    您的理解基本上是正确的，但有一点需要澄清。**在神经网络中，我们并不是事先为每一层设定一个明确的目标。我们为整个网络设定一个目标（比如分类或回归任务），然后通过训练过程，每个隐藏层都学会提取对最终目标有用的特征。**
    
    以下是神经网络训练过程的简化概述：
    
    1. 输入数据（X）传递给第一隐藏层。
    2. 第一隐藏层根据权重（W）和偏置（b）计算激活值（a1）。
    3. 激活值（a1）传递给下一层（可能是另一个隐藏层或输出层），该层使用其权重和偏置计算自己的激活值（如 a2）。
    4. 重复这个过程，直到输出层生成最终预测。
    5. 根据预测和实际目标计算损失（误差）。
    6. 使用优化算法（如梯度下降）更新网络中所有层的权重和偏置，以最小化损失。
    7. 重复这个过程，直到网络达到一定的收敛标准或训练轮数。
    
    在这个过程中，我们并没有为每个隐藏层设定具体的目标。相反，网络自动学习在每个层次上提取什么样的特征能够最小化整个网络的预测误差。通过训练，每一层的权重（W）和偏置（b）都逐渐得到优化，使得整个网络能够完成我们设定的任务。
    
- 🔰问题：output 层最终一起计算损失，使用优化算法更新了所有层的权重和偏差，所以对于整体的结果来说，得到了最佳的收敛（如果算法正确，或存在收敛的最小值的话）。那么，整体的收敛并不代表每一层都是最优（取到最小值）是吗？我们只是达到了最终效果的最小。那可以通过调整某一层的数据，进而达到优化最终结果的目的吗？
    
    您是对的，**整体收敛不代表每一层都是最优的。**神经网络中的每一层都是在一起协同工作来最小化整个网络的损失。实际上，很难衡量每个隐藏层是否达到了最优，因为它们的表现与其他层的权重和偏置紧密相关。当我们训练神经网络时，我们关心的是整个网络的损失，而不是每一层的单独表现。
    
    在实践中，为了优化神经网络的性能，我们可以调整网络结构和训练参数。这包括**改变隐藏层数量、每层神经元数量、激活函数、学习率等。这些调整可以影响每一层的权重和偏置更新，从而影响整个网络的性能。**同时，还可以使用正则化技术（如L1、L2正则化或dropout）来降低过拟合的风险，从而提高模型在未知数据上的泛化能力。
    
    因此，**我们通常不会直接调整某一层的数据，而是通过调整网络结构、训练参数和正则化技术来优化整个网络的性能。**这些调整将间接地影响每一层的权重和偏置，从而改进整个网络的预测效果。
    
- 🔰问题：关于根据预测和实际目标计算损失（误差）。----- 如果一开始用人像（实际目标为是否是人）去训练，自动学习后具有泛化能力。在不同数据集上，如汽车图像（否是汽车），也可以得到相当好的效果是吗？
    
    **实际上，**当您使用人像数据集（目标为是否是人）训练神经网络时，网络将学会识别人脸的特征。**当您将训练好的模型应用于汽车图像（目标为是否是汽车）时，它可能无法取得良好的效果，**因为模型学到的特征主要是针对人脸的。
    
    如果您想让模型在识别汽车图像上表现良好，您需要使用汽车数据集重新训练模型。通过使用汽车数据集，模型将自动学习提取与汽车相关的特征，从而在汽车图像上获得更好的泛化能力。
    
    在实际应用中，如果您希望模型能够处理多个不同的任务（例如，识别人脸和汽车），您可以采用以下策略之一：
    
    1. 为每个任务单独训练一个模型。这意味着您需要为识别人脸训练一个模型，为识别汽车训练另一个模型。这种方法可能需要较多的计算资源和存储空间。
    2. 使用多任务学习。在这种情况下，您可以设计一个共享的神经网络结构，同时学习多个任务。这样，网络可以在一个任务中学到的特征共享
- 🔰问题：那么当您使用人像数据集（目标为是否是人）训练神经网络时，网络将学会识别人脸的特征。这个模型，泛化到怎样的其他数据集上，推测表现会好呢？
    
    当您使用人像数据集（目标为是否是人）训练神经网络时，网络将学会识别人脸的特征。这个模型在其他与人脸识别相关的数据集上可能表现较好。例如：
    
    1. **不同人种的人脸识别**：如果训练数据集主要包含某一种族的人脸，那么将训练好的模型应用于其他种族的人脸识别任务时，它可能仍然具有一定的泛化能力。
    2. **表情识别**：虽然模型主要针对人脸识别任务进行训练，但它可能在识别不同表情的任务上表现较好，因为模型已经学会了识别面部特征。
    3. **年龄和性别识别**：训练好的模型可能在识别人物年龄和性别方面具有一定泛化能力，因为这些任务与面部特征相关。
    
    然而，对于与人脸识别无关的任务，例如汽车识别、动物识别或者物体识别等，训练好的模型可能无法取得良好的效果，因为它学到的特征主要是针对人脸的。在这种情况下，您需要为特定任务训练一个新的神经网络，以便网络能够学习适用于该任务的特征。
    
- 🔰问题：和人脸无关的任务效果不佳。那GPT 为什么有很强的泛化能力呢？最近有好几家公司都是基于 GPT 做了图像自动生成，这是为什么呢？怎么就从文字跨越到图像了呢？
    
    GPT（即生成预训练Transformer）具有很强的泛化能力，因为它采用了大量的文本数据进行训练，学习了丰富的知识和各种任务的模式。GPT 是一个基于Transformer结构的语言模型，它试图预测文本中的下一个单词。通过对大量文本数据的训练，GPT 学习了语言结构、语法、知识和推理等多种能力。
    
    然而，GPT 本身并不直接处理图像生成任务。但是，基于 GPT 结构的模型可以通过融合图像和文本的表示来处理多模态任务。一个典型的例子是 OpenAI 的 DALL-E，它是一个基于 GPT 结构的多模态模型，能够根据文本描述生成图像。**DALL-E 的成功在于它的训练方法，将图像和文本编码成统一的表示形式，让模型能够在两种模态之间建立联系。**
    
    通过将文本和图像数据结合起来进行训练，模型能够学习在两种模态之间进行转换和生成。这使得基于 GPT 结构的模型能够在不同任务之间进行泛化，包括图像生成任务。然而，需要注意的是，这些多模态模型是基于 GPT 结构进行调整和扩展的，而不是直接使用原始的 GPT 模型来处理图像任务。
    
- 🔰具体解释：“将图像和文本编码成统一的表示形式，让模型能够在两种模态之间建立联系”
    
    将图像和文本编码成统一的表示形式是指，我们需要在训练过程中将不同类型的数据（如图像和文本）表示为相同的数据结构。这样，神经网络可以在这些统一的表示之间建立联系和映射。
    
    以 DALL-E 为例，它将文本描述和图像数据结合在一起。**文本通过 GPT 结构进行编码，而图像则通过卷积神经网络（CNN）进行编码。**在这个过程中，**文本和图像都被转换为向量表示，这些向量可以在模型内部进行操作和组合。**通过这种方式，模型可以学习在文本描述和图像之间建立联系，从而实现根据文本生成图像的功能。
    
    对于声音、动作等其他类型的数据，也可以采用类似的方法将它们与文本编码成统一的表示形式。例如，声音可以通过语音编码器（如 WaveNet）编码成向量表示，而动作数据可以通过时间序列模型（如 LSTM 或 Transformer）进行编码。通过将这些编码与 GPT 的文本编码相结合，我们可以设计出多模态模型，以实现音乐生成、控制机器人、自动驾驶和工厂自动化等任务。
    
    需要注意的是，虽然基于 GPT 的多模态模型具有很强的泛化能力和潜力，但要针对特定任务（如音乐生成、自动驾驶等）实现高效和准确的性能，可能还需要针对任务特点进行模型的调整和优化。所以，从理论上讲，您的理解是正确的，但在实际应用中，需要对模型和训练方法进行适当的调整，以实现特定任务的高性能。
    
- 🔰问题：要针对特定任务（如音乐生成、自动驾驶等）实现高效和准确的性能，可能还需要针对任务特点进行模型的调整和优化------- 作为刚刚学习机器学习的人，这里似乎还离我很遥远？只有训练GPT模型公司内部的人才能做到吧？GPT 开发的接口已经是训练后的结果了把？我能做些什么积累经验之类的吗？或者GPT相关软件工程师的工作内容是这个吗？
    
    要积累经验，您可以尝试以下步骤：
    
    1. 学习基础知识：深入学习机器学习、深度学习和自然语言处理等领域的基本概念和方法。
    2. 实践项目：在实际项目中应用所学知识，参与开源项目或者自己设计项目。例如，您可以使用预训练的 GPT 模型来构建聊天机器人、文本生成器或自动摘要工具等。
    3. 阅读论文和博客：关注最新的研究进展和技术动态，了解业界的最佳实践和创新思路。
    4. 参加比赛和研讨会：参加机器学习和人工智能相关的比赛、研讨会和在线课程，与同行交流，提升自己的技能。
    
    GPT 相关软件工程师的工作内容可能包括：
    
    1. 使用预训练的 GPT 模型开发应用程序，如聊天机器人、智能推荐系统等。
    2. 对预训练的 GPT 模型进行微调，以适应特定任务或行业需求。
    3. 负责模型的部署、监控和维护，确保应用程序的稳定运行。
    4. 与数据科学家

> **本节一句话总结：**
图像识别：将图像展开为特征向量，构建神经网络。
隐藏层会自动学习特征，训练后的神经网络可以应用于不同数据集。
> 

---

# **Neural network model**

## 神经网络模型：[Neural network layer](https://www.coursera.org/learn/advanced-learning-algorithms/lecture/z5sks/neural-network-layer)

本视频讲解了如何构建一个神经网络的隐藏层。以下是关键知识点：

- 隐藏层：神经网络的基本构成单位是神经元层。**隐藏层是输入层和输出层之间的神经元层，负责从输入特征中提取有用信息**。
- 激活函数：**神经元使用激活函数（如逻辑函数或 Sigmoid 函数）来处理其输入。**激活函数将原始输入转换为激活值，用于传递给下一层。
- 计算：每个神经元接收输入值并计算其激活值。**激活值是通过将权重（w）与输入值（x）进行点积运算、加上偏置值（b），并将结果传递给激活函数得到的**。激活值随后作为下一层神经元的输入。
    
    ![Untitled](2%201%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%208b63c9e7efcf4b7cae514f45ef82a37a/Untitled%2010.png)
    
- 符号约定：**使用上标方括号表示与特定层相关的数量**。例如，a^[1] 表示与第一层相关的激活值，w^[1] 和 b^[1] 表示与第一层相关的权重和偏置值。
    
    ![Untitled](2%201%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%208b63c9e7efcf4b7cae514f45ef82a37a/Untitled%2011.png)
    
- 多层神经网络：将多个神经元层叠加在一起，可以构建更复杂、更大的神经网络。每一层的输出都会成为下一层的输入。
- **二元预测：在神经网络的最后输出中，可以选择是否应用阈值（如 0.5）以得到二元预测（0 或 1）。**
    
    ![Untitled](2%201%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%208b63c9e7efcf4b7cae514f45ef82a37a/Untitled%2012.png)
    

通过理解这些关键知识点，您可以开始构建更复杂、更大的神经网络。随着对更多示例的学习，构建神经网络的概念以及如何将多个神经元层组合在一起将变得更加清晰。

> **本节一句话总结：**
每个神经元接收输入 X 指并计算激活值 a。激活值 g(w·x+b)
二元预测
> 

## 更复杂的神经网络：[More complex neural networks](https://www.coursera.org/learn/advanced-learning-algorithms/lecture/a5AfY/more-complex-neural-networks)

这个课程中，你学到了一个更复杂的神经网络示例。这个神经网络包括一个输入层（Layer 0）和四个层，其中三个是隐藏层（Layer 1, 2, 3），一个是输出层（Layer 4）。以下是课程中的关键知识点：

- **神经网络的层数**：神经网络的层数包括所有隐藏层和输出层，**但不包括输入层**。
    
    ![Untitled](2%201%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%208b63c9e7efcf4b7cae514f45ef82a37a/Untitled%2013.png)
    
- 计算过程：**神经网络的每一层都需要进行计算，以将输入向量转换为输出向量。**在这个例子中，我们仔细观察了第三个隐藏层（Layer 3）的计算过程。隐藏层通过使用权重矩阵、激活函数（如 Sigmoid 函数）和偏置向量将输入向量 a_2 转换为输出向量 a_3。
    
    ![Untitled](2%201%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%208b63c9e7efcf4b7cae514f45ef82a37a/Untitled%2014.png)
    
- 激活函数：激活函数（如 Sigmoid 函数）用于计算神经元的输出。激活函数的作用是将神经元的输入经过非线性变换后输出。激活函数不仅限于 Sigmoid 函数，还有其他激活函数，如 ReLU、tanh 等。
- **符号表示**：神经网络中的符号表示对于理解神经网络的工作原理非常重要。**上标方括号表示层数（如 [2] 表示第二层），下标表示神经元的索引（如 w_1 表示第一个神经元的权重矩阵）。**
    
    ![Untitled](2%201%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%208b63c9e7efcf4b7cae514f45ef82a37a/Untitled%2015.png)
    
    ![Untitled](2%201%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%208b63c9e7efcf4b7cae514f45ef82a37a/Untitled%2016.png)
    
- 推理算法：神经网络的推理算法是指给定输入向量 X，如何计算出神经网络的输出。根据前面学到的知识，我们可以逐层计算激活值，直到计算出输出层的激活值。

通过这个课程，你应该对神经网络的基本概念和计算过程有了更深入的了解。

> **本节一句话总结：**
神经元的层数：包括隐藏层和输出层，不包括输入层。
神经网络的每一层都需要进行计算 a_2_1=g(w_2_1·a_1+b_2_1)，以将输入向量转换为输出向量。
符号表示：上标表示层数，下标表示神经元索引
> 

## 推理：做出预测（前向传播）：[Inference: making predictions (forward propagation)](https://www.coursera.org/learn/advanced-learning-algorithms/lecture/vYsrR/inference-making-predictions-forward-propagation)

在这门课程中，你学习了如何使用前向传播算法（forward propagation）来让神经网络进行推理（inference）或预测。以下是课程中的关键知识点：

- **前向传播算法**：前向传播算法是一种计算神经网络输出的方法。**从输入层开始，逐层计算每一层的激活值，直到输出层。**算法从左到右进行计算，因此称为前向传播。
- 计算过程：在这个例子中，输入是一个8x8的图像，神经网络包含两个隐藏层（第一个隐藏层有25个神经元，第二个隐藏层有15个神经元）以及一个输出层。**前向传播算法首先计算第一隐藏层的激活值 a1，然后计算第二隐藏层的激活值 a2，最后计算输出层的激活值 a3**。
    
    ![Untitled](2%201%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%208b63c9e7efcf4b7cae514f45ef82a37a/Untitled%2017.png)
    
    ![Untitled](2%201%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%208b63c9e7efcf4b7cae514f45ef82a37a/Untitled%2018.png)
    
- 神经网络架构：在这个例子中，**神经网络的隐藏层数量逐渐减少，这是一种典型的神经网络架构选择。**
- 二值分类：**通过将输出层的激活值 a3 与阈值（例如0.5）进行比较**，可以将神经网络的输出转换为二值分类标签（是数字1还是数字0）。
    
    ![截屏2023-04-14 19.53.38.png](2%201%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%208b63c9e7efcf4b7cae514f45ef82a37a/%25E6%2588%25AA%25E5%25B1%258F2023-04-14_19.53.38.png)
    

通过这个课程，你应该对前向传播算法以及如何使用神经网络进行推理有了更深入的了解。在下一节课中，你将学习如何在 TensorFlow 中实现这些算法。

> **本节一句话总结：**
前向传播算法：从输入层开始，逐层计算每一层的激活值，直到输出层。
神经网络架构：隐藏层逐层减少
二值分类：将输出层的激活值与阈值比较→二值分类标签
> 

## 实验室：[Neurons and Layers](https://www.coursera.org/learn/advanced-learning-algorithms/ungradedLab/hH4VM/neurons-and-layers)

- 代码
    
    ```python
    import numpy as np
    import matplotlib.pyplot as plt
    import tensorflow as tf
    from tensorflow.keras.layers import Dense, Input
    from tensorflow.keras import Sequential
    from tensorflow.keras.losses import MeanSquaredError, BinaryCrossentropy
    from tensorflow.keras.activations import sigmoid
    from lab_utils_common import dlc
    from lab_neurons_utils import plt_prob_1d, sigmoidnp, plt_linear, plt_logistic
    plt.style.use('./deeplearning.mplstyle')
    import logging
    logging.getLogger("tensorflow").setLevel(logging.ERROR)
    tf.autograph.set_verbosity(0) X_train = np.array([[1.0], [2.0]], dtype=np.float32)           #(size in 1000 square feet)
    Y_train = np.array([[300.0], [500.0]], dtype=np.float32)       #(price in 1000s of dollars)
    
    fig, ax = plt.subplots(1,1)
    ax.scatter(X_train, Y_train, marker='x', c='r', label="Data Points")
    ax.legend( fontsize='xx-large')
    ax.set_ylabel('Price (in 1000s of dollars)', fontsize='xx-large')
    ax.set_xlabel('Size (1000 sqft)', fontsize='xx-large')
    plt.show() linear_layer = tf.keras.layers.Dense(units=1, activation = 'linear', ) a1 = linear_layer(X_train[0].reshape(1,1))
    print(a1) w, b= linear_layer.get_weights()
    print(f"w = {w}, b={b}") set_w = np.array([[200]])
    set_b = np.array([100])
    
    # set_weights takes a list of numpy arrays
    linear_layer.set_weights([set_w, set_b])
    print(linear_layer.get_weights())  a1 = linear_layer(X_train[0].reshape(1,1))
    print(a1)
    alin = np.dot(set_w,X_train[0].reshape(1,1)) + set_b
    print(alin)  prediction_tf = linear_layer(X_train)
    prediction_np = np.dot( X_train, set_w) + set_b plt_linear(X_train, Y_train, prediction_tf, prediction_np)
    ```
    
    这份代码中使用了 TensorFlow（一个开源深度学习框架）的 Keras API 来实现一个简单的线性回归模型。下面是代码中一些主要的知识点：
    
    - 导入相关库：代码首先导入所需的库，包括 NumPy（用于数值计算）、Matplotlib（用于绘图）、TensorFlow（用于构建和训练神经网络）等。
    - 准备数据：代码创建了一组简单的训练数据，包括房屋面积（X_train）和房价（Y_train）。然后，使用 Matplotlib 绘制了这些数据点的散点图。
    - 创建线性层：使用 **`tf.keras.layers.Dense`** 类创建了一个线性层，表示线性回归模型。**`units=1`** 表示该层只有一个输出单元，**`activation='linear'`** 表示激活函数是线性函数。
    - 获取和设置权重：首先，使用 **`linear_layer.get_weights()`** 获取线性层的权重（w）和偏置（b）。然后，使用 **`linear_layer.set_weights()`** 设置新的权重（set_w）和偏置（set_b）。
    - 进行预测：使用 **`linear_layer(X_train)`** 对输入数据进行预测。此时，预测结果存储在 **`prediction_tf`** 变量中。同时，使用 NumPy 计算预测结果，存储在 **`prediction_np`** 变量中。
    - 绘制预测结果：调用 **`plt_linear()`** 函数绘制原始数据点、TensorFlow 预测结果（prediction_tf）和 NumPy 预测结果（prediction_np）。
    
    代码中绘制了两张看似一模一样的图，主要是为了验证 TensorFlow 预测结果（使用 **`linear_layer`**）与直接使用 NumPy 计算预测结果的一致性。这可以确保 TensorFlow 实现的线性回归模型是正确的。通过对比，我们可以发现 TensorFlow 和 NumPy 得到了相同的预测结果，证明 TensorFlow 实现了正确的线性回归模型。在实际场景中，这种对比可能不是必需的，但在这个示例中，它有助于展示和验证 TensorFlow 和 NumPy 计算结果的一致性。
    
    ![截屏2023-03-23 21.38.52.png](2%201%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%208b63c9e7efcf4b7cae514f45ef82a37a/%25E6%2588%25AA%25E5%25B1%258F2023-03-23_21.38.52.png)
    
    - 🔰问题：set_w = np.array([[200]])、set_b = np.array([100])   这个值是哪里来的呢？随便设置的吗？还是假定已经求解过了的值，只是随便用一下这个主要是看最终结果是吗？
    
    是的，这里的 **`set_w`**和 **`set_b`** 值是随意设置的。这段代码的主要目的是演示如何使用 TensorFlow 设置和获取层的权重（**`w`**）和偏置（**`b`**）。在这个示例中，**`set_w`**和**`set_b`**
    只是用来展示如何在 TensorFlow 中操作这些参数，实际上在训练过程中，这些值会通过优化算法（如梯度下降）自动更新以最小化损失函数。
    
- 代码 2

```python

model = Sequential(
    [
        tf.keras.layers.Dense(1, input_dim=1,  activation = 'sigmoid', name='L1')
    ]
) model.summary() logistic_layer = model.get_layer('L1')
w,b = logistic_layer.get_weights()
print(w,b)
print(w.shape,b.shape) set_w = np.array([[2]])
set_b = np.array([-4.5])
# set_weights takes a list of numpy arrays
logistic_layer.set_weights([set_w, set_b])
print(logistic_layer.get_weights())  a1 = model.predict(X_train[0].reshape(1,1))
print(a1)
alog = sigmoidnp(np.dot(set_w,X_train[0].reshape(1,1)) + set_b)
print(alog)  plt_logistic(X_train, Y_train, model, set_w, set_b, pos, neg)
```

这份代码中使用了TensorFlow库来实现单层神经网络的逻辑回归任务。具体代码流程如下：

首先定义了一个**Sequential对象作为模型，并通过添加Dense层来实现单层神经网络。Dense层的参数包括1个神经元、1个输入维度和一个sigmoid激活函数，并且设置了该层的名称为'L1'。**

然后使用model.summary()方法来打印模型的概述信息，包括模型的架构和参数数量。

接下来使用get_layer方法获取名称为'L1'的层，并使用get_weights方法获取该层的权重和偏置。打印出来可以看到，由于该层只有1个神经元，所以权重和偏置都是1维的。

然后定义了新的权重和偏置，分别为2和-4.5，并将其通过set_weights方法设置到之前获取的层上。

接着使用model.predict方法对X_train中的第一个样本进行预测，并打印出来。可以看到，由于新的权重和偏置已经被设置到模型中，所以预测结果已经发生了变化。

最后使用sigmoidnp函数和plt_logistic函数分别绘制了逻辑回归函数和分类结果的可视化图形，以展示模型的分类效果。

![截屏2023-03-23 21.49.19.png](2%201%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%208b63c9e7efcf4b7cae514f45ef82a37a/%25E6%2588%25AA%25E5%25B1%258F2023-03-23_21.49.19.png)

![截屏2023-03-23 21.39.04.png](2%201%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%208b63c9e7efcf4b7cae514f45ef82a37a/%25E6%2588%25AA%25E5%25B1%258F2023-03-23_21.39.04.png)

> **本节一句话总结：**
用TensorFlow 实现简单的神经网络
> 

---

# **TensorFlow implementation**

## 代码推理：[Inference in Code](https://www.coursera.org/learn/advanced-learning-algorithms/lecture/rJMKC/inference-in-code)

在这门课程中，你学习了如何使用 TensorFlow 实现深度学习算法。以下是课程中的关键知识点：

1. TensorFlow：TensorFlow 是实现深度学习算法的领先框架之一，与之竞争的另一个流行框架是 PyTorch。本课程重点介绍 TensorFlow。
2. 实现推理：课程中，你学习了如何使用 TensorFlow 实现神经网络的推理**。通过创建神经网络的层（称为“密集”或“全连接”层），然后应用激活函数（例如 sigmoid 函数），你可以一层一层地计算神经网络的输出。在推理过程中，你需要将输入数据（例如图像的像素值或咖啡烘焙的温度和持续时间）提供给神经网络，然后计算各层的激活值以获得最终输出**。
    
    ![Untitled](2%201%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%208b63c9e7efcf4b7cae514f45ef82a37a/Untitled%2019.png)
    
    ![Untitled](2%201%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%208b63c9e7efcf4b7cae514f45ef82a37a/Untitled%2020.png)
    
    ![Untitled](2%201%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%208b63c9e7efcf4b7cae514f45ef82a37a/Untitled%2021.png)
    
    ![Untitled](2%201%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%208b63c9e7efcf4b7cae514f45ef82a37a/Untitled%2022.png)
    
    ![Untitled](2%201%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%208b63c9e7efcf4b7cae514f45ef82a37a/Untitled%2023.png)
    
3. 处理数据：课程中简要介绍了如何使用 numpy 数组处理数据，并指出 TensorFlow 以特定方式处理数据，这对于正确实现算法非常重要。

通过学习这门课程，你应该对如何使用 TensorFlow 实现深度学习算法以及如何执行神经网络推理有了更深入的了解。在接下来的课程中，你将了解 TensorFlow 如何处理数据。

> **本节一句话总结：**
使用 TensorFlow 实现深度学习算法.
> 

## TensorFlow 中的数据：[Data in TensorFlow](https://www.coursera.org/learn/advanced-learning-algorithms/lecture/eTL7B/data-in-tensorflow)

在这节课中，你将学习以下关键知识点：

- 数据在NumPy和TensorFlow中的表示方式：NumPy和TensorFlow是两种用于表示和处理矩阵和向量的库。虽然它们在很多方面类似，但由于它们分别由不同的团队创建，因此在表示数据时有一些细微差别。了解这些差异有助于编写正确的代码并实现神经网络。
- 矩阵和向量：**矩阵是二维数组，其中每个元素都是一个数。向量是一维数组，也可以看作是只有一行或一列的矩阵。**矩阵和向量可以用于表示神经网络中的数据和参数。
    
    ![Untitled](2%201%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%208b63c9e7efcf4b7cae514f45ef82a37a/Untitled%2024.png)
    
- NumPy矩阵：在NumPy中，矩阵使用**`np.array`**创建，其形状由行数和列数确定。例如，一个2x3矩阵可以表示为**`np.array([[1, 2, 3], [4, 5, 6]])`**。
    - 🔰简单记忆：np.array([200,27]) 是一个单纯的 1D数组；每个[]都是一行手拉手的数组；[[],[]] 逗号相当于分行的两个手拉手的数组。  2*3 代表有 2 行，每行有3 个手拉手的小朋友
    
    ![Untitled](2%201%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%208b63c9e7efcf4b7cae514f45ef82a37a/Untitled%2025.png)
    
    ![Untitled](2%201%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%208b63c9e7efcf4b7cae514f45ef82a37a/Untitled%2026.png)
    
    ![Untitled](2%201%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%208b63c9e7efcf4b7cae514f45ef82a37a/Untitled%2027.png)
    
- TensorFlow张量：TensorFlow使用张量（tensor）来表示矩阵。张量是一种更通用的数据类型，用于在神经网络中存储和操作矩阵。要在TensorFlow中表示一个矩阵，可以使用类似**`tf.constant([[1, 2, 3], [4, 5, 6]])`**的语法。
    
    ![Untitled](2%201%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%208b63c9e7efcf4b7cae514f45ef82a37a/Untitled%2028.png)
    
- TensorFlow和NumPy之间的转换：在TensorFlow中处理数据时，可以将NumPy数组转换为TensorFlow张量。同样，从TensorFlow中读取数据时，可以将张量转换回NumPy数组。要将张量转换为NumPy数组，可以使用**`.numpy()`**方法。
    - 🔰名词解释：**`tf.Tensor`**是函数吗？
        
        **`tf.Tensor`** 不是一个函数，而是 TensorFlow 中的一个数据类型，用于存储多维数组的数据。它是 TensorFlow 在计算图中表示节点之间数据流的主要方式之一。
        
        **`tf.Tensor`** 可以存储各种数据类型的多维数组，如浮点数、整数、布尔值等。其形状（shape）表示了数组的维度，每个维度的大小可以是任意整数。例如，形状为 **`[3, 2]`** 的 **`tf.Tensor`** 表示一个有 3 行 2 列的矩阵。
        
- 使用TensorFlow构建神经网络：在本节课的后续部分，你将学习如何将这些知识应用于构建神经网络。神经网络由多个层组成，每个层都执行特定的计算。通过将输入数据传递给这些层并计算输出，神经网络可以执行各种任务，如分类、回归等。

了解这些关键概念将帮助你更好地理解如何在Python中使用NumPy和TensorFlow库来表示和操作数据，以便在实现神经网络时能够遵循一致的框架。

> **本节一句话总结：**
NumPy中，矩阵使用**`np.array`**创建
TensorFlow使用张量 **`tf.Tensor`** 来表示矩阵
> 

## 构建神经网络：[Building a neural network](https://www.coursera.org/learn/advanced-learning-algorithms/lecture/J6Vli/building-a-neural-network)

在这段课程中，关键知识点包括如何在TensorFlow中构建和训练神经网络。以下是一些关键概念：

- 层的创建：**在TensorFlow中，可以使用`Dense`类创建全连接层。你需要指定神经元数量以及激活函数。**
    
    ![截屏2023-04-14 21.46.12.png](2%201%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%208b63c9e7efcf4b7cae514f45ef82a37a/%25E6%2588%25AA%25E5%25B1%258F2023-04-14_21.46.12.png)
    
- 顺序模型：**TensorFlow提供了一个`Sequential`类，它允许你将多个层堆叠在一起，形成一个神经网络。**这使得构建神经网络变得非常简单，因为你只需要按顺序指定每个层。
- 编译模型：**`compile`**方法用于配置模型的学习过程。它需要指定损失函数、优化器和评估指标。
- 训练模型：通过调用**`fit`**方法，可以使用训练数据对模型进行训练。这个方法将输入数据与目标数据进行匹配，并执行一定数量的训练迭代（称为**`epochs`**）。
- 推理：对于新的输入数据，可以使用**`predict`**方法进行前向传播以获取预测值。
    
    ![Untitled](2%201%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%208b63c9e7efcf4b7cae514f45ef82a37a/Untitled%2029.png)
    
    - 🔰问题：通过设定的Dense 的每一层，其实只设定了输入的个数，如 64,1 。那这个值是以什么标准定制的呢，用什么样的排列组合传递给下一层的？
        
        在 TensorFlow 中，每一层的输出维度是由该层的权重矩阵（也称为内核）决定的。假设当前层的输入维度是 $N_{in}$，输出维度是 $N_{out}$，那么该层的权重矩阵的形状将是 $(N_{in}, N_{out})$。
        
        在 Dense 层中，当你设置该层的单元数（units）时，实际上就是设置了输出维度 $N_{out}$，而输入维度 $N_{in}$ 是由前一层的输出维度决定的。因此，在使用 Dense 层时，你只需要知道前一层的输出维度，而无需显式地指定该层的输入维度。
        
        至于具体是如何传递给下一层的，这是 TensorFlow 内部实现的细节，用户无需关心。在调用 **`model.fit()`** 时，TensorFlow 会根据网络结构自动进行前向传播，计算每一层的输出，并将其传递给下一层，以此完成整个神经网络的前向传播计算。
        

课程中的示例演示了如何使用这些概念为两个不同的任务（咖啡分类和数字分类）构建神经网络。此外，课程还强调了深入了解算法内部工作原理的重要性，以便在实践中能够更好地理解和解决问题。

在下一部分，课程将介绍如何从头开始用Python实现前向传播，以帮助你更好地理解神经网络的内部工作原理。虽然在实际应用中，大多数机器学习工程师会使用像TensorFlow和PyTorch这样的库，但了解底层原理有助于更好地掌握这些工具。

> **本节一句话总结：**
TensoFlow : 创建层`**Dense**`、将层顺序连起来**`Sequential`、**配置损失函数等**`compile`、**模型训练**`fit`**、预测**`predict`**
> 

## 实验室：[Coffee Roasting in Tensorflow](https://www.coursera.org/learn/advanced-learning-algorithms/ungradedLab/reSfw/coffee-roasting-in-tensorflow)

![截屏2023-03-24 21.12.06.png](2%201%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%208b63c9e7efcf4b7cae514f45ef82a37a/%25E6%2588%25AA%25E5%25B1%258F2023-03-24_21.12.06.png)

- 🔰关键点：在深度神经网络的训练过程中，对数据进行归一化处理可以加快反向传播算法（back-propagation）的训练速度，**这与第一门课程中特征缩放的过程类似。使用Keras的Normalization层可以方便地进行归一化，**该层的步骤如下：
    1. 创建一个Normalization层。注意，在这里应用Normalization层并不是模型中的一层。
    2. 通过调用adapt()方法，该层学习数据集的均值和方差，并将这些值保存在内部。
    3. 对数据进行归一化处理。

![截屏2023-03-24 21.15.00.png](2%201%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%208b63c9e7efcf4b7cae514f45ef82a37a/%25E6%2588%25AA%25E5%25B1%258F2023-03-24_21.15.00.png)

- 🔰关键点：这段代码中涉及到的关键知识点有：
    1. `tf.keras.Input`：这是定义神经网络输入层的函数，参数中的shape指定了输入数据的形状。
    2. `Dense`：这是定义神经网络中的全连接层的函数，参数中的**第一个参数指定了该层的输出维度**，**第二个参数指定了该层的激活函数**，第三个参数指定了该层的名称。
    3. `Sequential`：这是一个顺序模型，可以通过**将各个网络层按顺序添加到模型中**来构建神经网络。
    4. `tf.random.set_seed`：这是设置随机数种子，用于在每次运行程序时生成相同的随机数序列，保证模型训练结果的可重复性。
    5. `sigmoid`激活函数：这是一种常用的激活函数，它可以将输出值压缩到0和1之间。
    6. 在输出层使用sigmoid激活函数的问题：不应该在输出层使用sigmoid激活函数，因为这会影响损失函数的计算和数值稳定性，更好的做法是将sigmoid函数的影响放在损失函数中。
- 🔰问题：shape指定了输入数据的形状。这个形状是根据什么确定的？
    
    在这个例子中，我们指定了输入数据的形状为 **`(2,)`**，也就是一个包含两个元素的一维数组。这是因为在这个例子中，**我们有两个特征作为输入，分别是咖啡的温度和冲泡时间，因此我们需要一个长度为 2 的数组来表示输入数据。**我们可以将其视为一个特征向量，其中每个元素表示一个特征。在实际应用中，输入数据的形状通常是由数据集的特征维度决定的，需要根据实际情况来指定。在指定输入数据形状后，TensorFlow 会自动为网络参数分配正确的大小。
    

![截屏2023-03-24 21.25.13.png](2%201%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%208b63c9e7efcf4b7cae514f45ef82a37a/%25E6%2588%25AA%25E5%25B1%258F2023-03-24_21.25.13.png)

- 🔰解释
    
    这样取值是由神经网络的架构和设计决定的。在第一层中有3个单元，意味着该层将从输入数据中提取3个特征。因此，权重数组的大小应该是输入数据的维数（这里是2）和层中的单元数（这里是3）的乘积，即(2,3)。而偏置b应该与层中的单元数相同，即3个元素。类似地，在第二层中有1个单元，因此权重数组应该是(3,1)的形状，而偏置b只需要1个元素。这些值是由我们对网络设计的理解和需要所决定的。
    

![截屏2023-03-24 21.28.40.png](2%201%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%208b63c9e7efcf4b7cae514f45ef82a37a/%25E6%2588%25AA%25E5%25B1%258F2023-03-24_21.28.40.png)

- 🔰问题：这里还没有给原始输入数据吧？为什么就能算出来wb了呢？相当于随便设置了一个初始值？
    
    是的。这里并没有对原始输入数据进行计算，而是通过调用模型的 **`get_layer`**方法和 **`get_weights`**
     方法，获取了模型中对应层的权重参数 W 和偏置参数 b。因为在模型定义时，已经指定了每一层的输入形状和输出维度，TensorFlow 根据这些信息自动初始化了对应的权重参数和偏置参数，这里直接获取这些参数的值并打印出来。
    

![截屏2023-03-24 21.33.21.png](2%201%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%208b63c9e7efcf4b7cae514f45ef82a37a/%25E6%2588%25AA%25E5%25B1%258F2023-03-24_21.33.21.png)

- 🔰解释：
    
    在上述代码中，我们首先使用 **`model.compile`** 定义了一个损失函数和优化器，损失函数使用了 **`tf.keras.losses.BinaryCrossentropy`**，它是用于二分类问题的交叉熵损失函数，而优化器使用了 **`tf.keras.optimizers.Adam`**，它是一种基于梯度下降的优化器，学习率为0.01。
    
    然后我们使用 **`model.fit`** 进行模型的训练，其中 **`Xt`** 和 **`Yt`** 是训练集的输入特征和标签，**`epochs`** 参数指定了训练的轮数。在训练过程中，优化器会根据损失函数的梯度来更新模型的参数，以尽可能地减小损失函数。
    
- 🔰名词解释：
    
    在深度学习中，训练模型是一个很关键的步骤，训练的目标是找到使损失函数最小的模型参数。这里的损失函数是指神经网络预测结果与实际结果之间的差距，因此我们需要在训练过程中最小化损失函数，以便让模型尽可能地接近真实结果。
    
    而优化器则是用来调整模型参数以尽可能地减小损失函数，因此优化器的选择也对模型的训练有很大影响。
    

![截屏2023-03-24 21.45.30.png](2%201%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%208b63c9e7efcf4b7cae514f45ef82a37a/%25E6%2588%25AA%25E5%25B1%258F2023-03-24_21.45.30.png)

- 🔰解释：每个epoch表示模型对整个训练集的一次迭代，而每个batch则是对其中的一小部分数据进行训练。这里的进度条展示了模型对每个batch的训练进度，同时也显示了每个epoch的平均损失。这些信息可以帮助我们了解模型训练的情况，例如训练速度、是否存在过拟合等。
- 🔰问题：所以loss 越来越小，是收敛速度很好的意思是吗？怎么看出来过拟合呢？
    
    是的，**随着训练的进行，loss逐渐减小，说明模型在逐渐拟合训练数据**。如果训练集上的loss非常小，但是在测试集上的loss却很大，那么就有可能出现过拟合的情况，这时候可以通过加入正则化项或者使用其他技术来解决过拟合的问题。此外，如果训练过程中发现loss下降很快，但是后面几个epoch的loss却没怎么下降，这时候就可能是模型已经开始过拟合了。可以通过早停策略等方法来解决这个问题。
    
    ![截屏2023-03-24 21.48.34.png](2%201%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%208b63c9e7efcf4b7cae514f45ef82a37a/%25E6%2588%25AA%25E5%25B1%258F2023-03-24_21.48.34.png)
    
- 🔰解释：每个神经元的输出函数在输入空间中都有自己的决策区域，这是由于每个神经元的权重和偏置的不同而导致的。
    
    在训练过程中，神经网络逐渐调整每个神经元的权重和偏置，使得每个神经元在输入空间中能够更好地区分不同的类别或模式，因此每个神经元的输出函数在输入空间中对应了不同的决策区域。在本例中，**第一个隐藏层的三个神经元的输出函数分别对应了三个不同的“糟糕的烘焙”区域，**而输出层的神经元将这些区域的信息组合起来，得出最终的“好烘焙”和“糟糕烘焙”的决策。
    
    ![截屏2023-03-24 21.54.12.png](2%201%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%208b63c9e7efcf4b7cae514f45ef82a37a/%25E6%2588%25AA%25E5%25B1%258F2023-03-24_21.54.12.png)
    

![截屏2023-03-24 21.55.26.png](2%201%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%208b63c9e7efcf4b7cae514f45ef82a37a/%25E6%2588%25AA%25E5%25B1%258F2023-03-24_21.55.26.png)

- 🔰解释：
    
    左边的图显示了网络输出的原始概率值，右边的图显示了应用决策阈值后的预测结果，即将概率值转换为二元决策（好的烘焙或坏的烘焙）。
    
    左图是网络最后一层的原始输出，即对于每个输入数据，网络对“坏烘焙”和“好烘焙”的预测概率。蓝色的渐变背景表示预测的概率，训练数据用X和O表示。右图则是经过一个决策阈值后的输出结果，X和O代表网络的决策。
    
- 🔰问题：都不是对新的数据做预测是吧？只是验证模型对原始数据的预测是否准确？
    
    是的，这些图表只是用于展示模型对训练数据的预测和决策结果，而不是对新数据的预测。它们的目的是帮助我们理解模型如何做出决策，并评估模型在训练数据上的表现。要评估模型在新数据上的表现，我们需要使用测试数据或交叉验证等技术。
    

---

# Python 中的神经网络实现：**Neural network implementation in Python**

## 单层的前向传播：[Forward prop in a single layer](https://www.coursera.org/learn/advanced-learning-algorithms/lecture/AJc5g/forward-prop-in-a-single-layer)

在这段课程中，关键知识点是如何从头开始使用Python实现前向传播。这将帮助你更好地理解在TensorFlow和PyTorch等库中发生的事情。以下是实现前向传播的步骤：

1. 初始化权重和偏置：**为神经网络的每一层设置权重矩阵（W）和偏置向量（b）**。
2. **定义激活函数**：实现所选的激活函数，**如sigmoid函数**。
3. 计算Z：**对于每个神经元，计算线性组合值（Z）**，这是输入特征向量（x）与权重矩阵（W）的点积，再加上偏置向量（b）。
4. 计算激活值：对于每个神经元，**将激活函数（如sigmoid函数）应用于Z值，以获得激活值（A）。**
5. **传递激活值**：将当前层的激活值作为下一层的输入特征向量，并重复步骤3和4，直到到达输出层。
    
    ![Untitled](2%201%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%208b63c9e7efcf4b7cae514f45ef82a37a/Untitled%2030.png)
    
    - x = np.array([200,17])  温度 200，时间 17 分钟，相当于1 个 Traning Set。

课程中的示例是针对咖啡烘焙模型的前向传播，通过分别计算每个神经元的激活值来实现。接下来的课程将介绍如何简化此过程，以便更一般地实现前向传播，而不是针对每个神经元进行硬编码。这将帮助你更好地了解神经网络的底层原理，使得在使用像TensorFlow和PyTorch这样的库时，你可以更好地理解它们的工作方式。

> **本节一句话总结：**
> 
> 
> 单层的向前传播：为每个神经元计算线性组合Z  `**f(wb)**` `**2_1=np.dot(w2_1,a1)+b2_1**`→设置激活值A  `**a2_1=sigmoid(z1_1)**`，将前一层的激活值作为下一层的输入特征，重复前两步，直到输出层。
> 

## 前向传播的一般实现：[General implementation of forward propagation](https://www.coursera.org/learn/advanced-learning-algorithms/lecture/fZYiN/general-implementation-of-forward-propagation)

在这段课程中，关键知识点是如何使用Python实现更通用的前向传播，而不是针对每个神经元硬编码。以下是实现更通用前向传播的步骤：

- 定义一个密集层（Dense Layer）函数：这个函数接受前一层的激活值以及给定层神经元的权重矩阵（W）和偏置向量（b）作为输入。
- 计算神经元数量：通过检查权重矩阵（W）的形状，确定当前层中的神经元数量。
- 初始化激活值数组：创建一个与神经元数量相同的零数组，用于存储计算得到的激活值。
- 计算激活值：遍历当前层的每个神经元，计算它们的激活值。这包括提取权重矩阵中的相应列，计算Z值（输入特征向量与权重矩阵的点积，加上偏置向量），并将激活函数（如sigmoid函数）应用于Z值。
- 返回激活值：在计算完当前层所有神经元的激活值后，返回激活值数组。
    
    ![Untitled](2%201%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%208b63c9e7efcf4b7cae514f45ef82a37a/Untitled%2031.png)
    
    - 📌注释：这里的代码有错误，少一个逗号，应该是 **`np.array([[1,3,5],[2,4,6]])`**。这是一个二维的 NumPy 数组，包含两个行和三个列，类似于一个矩阵。每个元素是一个整数。其中第一行是 **`[1, 3, 5]`**，第二行是 **`[2, 4, 6]`**。
    - 关于 a_out：x[1,2]，w1[2,j] a1[j,t] ,w2[t,r]  好像连起来了输入的 j 和w 的 i 相等哦

**通过定义一个密集层函数，你可以轻松地将多个密集层串联在一起，实现神经网络中的前向传播。**具体操作如下：

1. 对于神经网络中的每一层，将上一层的激活值、当前层的权重矩阵（W）和偏置向量（b）传递给密集层函数。
2. **使用密集层函数的输出作为下一层的输入，直到达到输出层**。
3. 输出层的激活值即为神经网络的输出。

了解神经网络底层原理对于实际应用和调试机器学习算法非常有用，即使在使用像TensorFlow这样的高级库时。你现在已经学会了如何从头开始实现前向传播，这将有助于提高你作为机器学习工程师的能力。

> **本节一句话总结：**
用 NumPy 函数实现向前传播中的神经元计算。 `**z = np.dot(W[:,j], a_in)+b[j]`  `a_out[j] = g(z)`**    
 这种循环的形式可以用 `Z=np.matmult(A_in,W)+B`  替代，下一章会讲
> 

# AGI：[Is there a path to AGI?](https://www.coursera.org/learn/advanced-learning-algorithms/lecture/Ezrdf/is-there-a-path-to-agi)

作者讨论了人工智能的两个方面：**人工窄智能（ANI）和人工通用智能（AGI）**。ANI 是指在特定任务上表现出色的 AI 系统，如智能音箱、自动驾驶汽车、网络搜索等。近年来，ANI 取得了巨大进展，并为世界带来了很多价值。而 AGI 是指能够完成任何典型人类能做的事情的 AI 系统。尽管 ANI 取得了很多进展，但我们在 AGI 领域的进展仍不明朗。

作者认为，模拟大脑作为实现 AGI 的途径可能非常困难。这是因为我们构建的人工神经网络远比生物神经网络简单，而且我们对大脑的理解仍然非常有限。

然而，作者提到了一些关于动物大脑的实验，这些实验显示大脑能够适应各种任务，表明可能存在一个或少数几个学习算法。如果我们能找到这个算法，我们也许就能实现 AGI。尽管目前我们并不知道这个算法是什么，但作者认为这仍然是一个有趣的研究课题，值得未来继续探索。

在短期内，即使没有追求 AGI，机器学习和神经网络仍然是非常强大的工具，可以应用于各种场景。作者鼓励学习者继续研究这一领域，以便更好地利用神经网络解决实际问题。

---

# 向量化：**Vectorization(optional)**

## 神经网络如何高效实现视频：[How neural networks are implemented efficiently](https://www.coursera.org/learn/advanced-learning-algorithms/lecture/qkJy8/how-neural-networks-are-implemented-efficiently)

在这段文字中，作者强调了向量化在深度学习中的重要性。**向量化**是通过使用矩阵乘法来高效实现神经网络的计算过程。这使得**神经网络能够在并行计算硬件（如 GPU 和部分 CPU 功能）上进行高效计算，从**而使得深度学习能够扩展到大规模神经网络。

作者提供了一个向量化实现的示例，通过将前向传播（forward propagation）在单层神经网络中的计算过程转换为矩阵乘法。在这个示例中，作者使用 NumPy 的 matmul 函数来进行矩阵乘法。通过使用矩阵乘法，可以将原本需要使用循环来计算的代码简化为几行代码，从而实现高效计算。

![截屏2023-04-14 22.49.10.png](2%201%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%208b63c9e7efcf4b7cae514f45ef82a37a/%25E6%2588%25AA%25E5%25B1%258F2023-04-14_22.49.10.png)

值得注意的是，在向量化实现中，所有的变量（如 x、W、B、Z 和 A_out）都是二维数组（矩阵）。这使得神经网络在进行前向传播时能够高效地计算。

接下来的两个可选视频将介绍矩阵乘法的原理以及如何实现。如果你已经熟悉线性代数、向量、矩阵、转置和矩阵乘法，你可以快速浏览这两个视频并跳到本周的最后一个视频。在最后一个可选视频中，作者将详细解释如何通过矩阵乘法实现神经网络的向量化计算。

> **本节一句话总结：**
向量化让神经网络可以在计算硬件 GPU 上进行高效并行计算
> 

## 矩阵乘法视频：[Matrix multiplication](https://www.coursera.org/learn/advanced-learning-algorithms/lecture/YUhR0/matrix-multiplication)

在这段文字中，作者解释了矩阵乘法的概念，包括向量点积、向量-矩阵乘法和矩阵-矩阵乘法。以下是关键知识点的概述：

- **向量点积 `dot`**：向量点积是**将两个具有相同数量元素的向量进行对应元素相乘并求和的操作**。向量点积还可以表示为一个向量的转置与另一个向量相乘。
    
    ![Untitled](2%201%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%208b63c9e7efcf4b7cae514f45ef82a37a/Untitled%2032.png)
    
- **向量-矩阵乘法** vector matrix：向量-矩阵乘法实际上是**将向量与矩阵的每一列进行点积，结果是一个新向量，其中的每个元素是原向量与矩阵对应列的点积**。
    
    ![Untitled](2%201%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%208b63c9e7efcf4b7cae514f45ef82a37a/Untitled%2033.png)
    
- **矩阵-矩阵乘法** matrix matrix：矩阵-矩阵乘法是将一个矩阵的行与另一个矩阵的列进行点积，以构建结果矩阵。具体来说，**结果矩阵的第 i 行第 j 列的元素是第一个矩阵的第 i 行与第二个矩阵的第 j 列的点积。**
    
    ![Untitled](2%201%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%208b63c9e7efcf4b7cae514f45ef82a37a/Untitled%2034.png)
    

作者通过具体示例详细解释了这些概念，这有助于理解神经网络中的向量化计算以及如何使用矩阵乘法高效实现前向传播。在接下来的视频中，作者将讨论矩阵乘法的一般形式，以进一步阐明这些概念。

> **本节一句话总结：**
向量*向量（线性回归）：相乘并求和 f(x) = wx → 结果 1 个元素
****向量*矩阵(神经元)：向量与矩阵的每一列进行点积→结果1*n 列，与矩阵列数相同
矩阵*矩阵 ：行与列的点积
（向量指 1*n 维，矩阵是 m*n 维）
> 

## 矩阵乘法规则：[Matrix multiplication rules](https://www.coursera.org/learn/advanced-learning-algorithms/lecture/fKqUA/matrix-multiplication-rules)

这个视频讲解了如何将两个矩阵相乘，这是实现神经网络向量化的基础知识。以下是关键知识点：

- 矩阵乘法：将一**个矩阵A的每一行与另一个矩阵B的每一列进行点积（内积）**运算，得到一个新的矩阵C。计算过程中，A的每一行与B的每一列对应元素相乘，然后将结果相加。这样，**A的行数决定了C的行数，B的列数决定了C的列数**。
    
    ![Untitled](2%201%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%208b63c9e7efcf4b7cae514f45ef82a37a/Untitled%2035.png)
    
- 矩阵乘法的要求：为了将两个矩阵相乘，**第一个矩阵的列数必须等于第二个矩阵的行数。这是因为只有维度相同的向量才能进行点积运算。
- 🔰 At —| W** 维度相等 ****
    
    ![Untitled](2%201%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%208b63c9e7efcf4b7cae514f45ef82a37a/Untitled%2036.png)
    
1. 矩阵乘法与神经网络的关系：在神经网络中，向量化实现可以大大提高计算速度。这需要利用矩阵乘法将输入数据和权重矩阵相乘，以高效地计算神经网络的输出。这种实现方法可以避免显式地使用循环语句，从而加快计算速度。

总之，这个视频通过详细演示了矩阵乘法的计算过程，为下一步学习神经网络的向量化实现奠定了基础。

> **本节一句话总结：**
> 
> 
> Z[`3`,`4`] =At[`3`,**2**] * W[**2**,`4`] 
> 

## 矩阵乘法代码视频：[Matrix multiplication code](https://www.coursera.org/learn/advanced-learning-algorithms/lecture/ysRAb/matrix-multiplication-code)

这个视频展示了如何利用向量化实现神经网络的前向传播。以下是关键知识点：

- 利用矩阵乘法进行向量化：将输入矩阵A与权重矩阵W相乘，再加上偏置矩阵B，可以高效地计算神经网络的线性部分。这样可以避免使用显式循环，从而加快计算速度。
- NumPy实现：在Python中，可以使用NumPy库进行矩阵运算。**矩阵乘法可以用np.matmul()**函数实现，或者使用@符号。要计算矩阵的转置，可以使用A.T。
    
    ![Untitled](2%201%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%208b63c9e7efcf4b7cae514f45ef82a37a/Untitled%2037.png)
    
- 前向传播的向量化实现：对于单层神经网络，可以使用以下步骤实现向量化的前向传播：
    - **将输入矩阵A（或A.T）与权重矩阵W相乘，再加上偏置矩阵B，得到矩阵Z**。
    - 将激活函数g（如Sigmoid函数）逐元素应用于矩阵Z，得到输出矩阵A_out。
    
    ![Untitled](2%201%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%208b63c9e7efcf4b7cae514f45ef82a37a/Untitled%2038.png)
    
- 向量化实现的优势：现代计算机在实现矩阵乘法等操作时非常高效，因此使用向量化实现的神经网络前向传播可以显著提高计算速度。

通过向量化实现神经网络的前向传播，您可以更快地进行模型推理。在学习完这个视频后，您可以通过练习和测验来巩固这方面的知识。接下来的课程将讲解如何训练神经网络。

> **本节一句话总结：**
`z = np.matmul(AT,W)+b`   `a_out =  g(z)`
> 

## 考试

![截屏2023-03-26 22.40.45.png](2%201%20%E9%AB%98%E7%BA%A7%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%208b63c9e7efcf4b7cae514f45ef82a37a/%25E6%2588%25AA%25E5%25B1%258F2023-03-26_22.40.45.png)