# 3.3 无监督学习、推荐系统、强化学习：强化学习

復習: Not started
日付: 2023年4月16日
最終更新日時: 2023年4月17日 22:16

# 介绍强化学习：**Reinforcement learning introduction**

## 什么是强化学习：[What is Reinforcement Learning?](https://www.coursera.org/learn/unsupervised-learning-recommenders-reinforcement-learning/lecture/RrrOL/what-is-reinforcement-learning)

这段内容中，将学习什么是强化学习和一些应用。以下是关键知识点：

- 如何利用强化学习让直升机自行飞行？
    
    ![截屏2023-04-16 16.10.38.png](3%203%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%20728f256b941e4b34a0b4f5612a26a85c/%25E6%2588%25AA%25E5%25B1%258F2023-04-16_16.10.38.png)
    
    - 在强化学习中，我们称直升机的位置、方向、速度等为状态。任务是找到一个从直升机状态映射到动作a的功能，这意味着将两个控制杆推多远，以保持直升机在空中和飞行的平衡，并且不会坠毁。
    - 不推荐用监督学习：当直升机在空中移动时，实际上非常模棱两可，确切的正确行动是什么。**实际上很难获得x和理想动作y的数据集**。因此监督学习方法在控制机器人任务不起作用，应该用强化学习。
    - **强化学习的关键输入是所谓的奖励或奖励功能**。它告诉直升机什么时候做得好，什么时候做得不好，而不是如何做。所以我喜欢对奖励功能的看法有点像训练狗。
        
        ![截屏2023-04-16 16.10.47.png](3%203%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%20728f256b941e4b34a0b4f5612a26a85c/%25E6%2588%25AA%25E5%25B1%258F2023-04-16_16.10.47.png)
        
- 机器人狗：它使用强化学习，奖励它，向屏幕左侧移动，已经学会了如何小心地放置脚或爬过各种障碍物。
    
    ![截屏2023-04-16 17.03.38.png](3%203%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%20728f256b941e4b34a0b4f5612a26a85c/%25E6%2588%25AA%25E5%25B1%258F2023-04-16_17.03.38.png)
    
    ![截屏2023-04-16 16.10.52.png](3%203%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%20728f256b941e4b34a0b4f5612a26a85c/%25E6%2588%25AA%25E5%25B1%258F2023-04-16_16.10.52.png)
    
- 强化学习的应用：
    - 模拟中着陆月球着陆器。
    - 你如何重新安排工厂里的东西，以最大限度地提高吞吐量和效率
    - 金融股票交易，随着时间增长，怎样的交易是好的
    - 从跳棋到国际象棋，再到桥牌游戏，以及玩许多电子游戏。
    
    ![截屏2023-04-16 16.10.57.png](3%203%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%20728f256b941e4b34a0b4f5612a26a85c/%25E6%2588%25AA%25E5%25B1%258F2023-04-16_16.10.57.png)
    

关键想法是，你**不需要告诉算法每个输入的正确输出y是什么，相反，你所要做的就是指定一个奖励函数，**告诉它什么时候做得好，什么时候做得不好。**算法的工作是自动弄清楚如何选择好的动作。**下一个视频，我们将正式确定强化学习问题，并开始开发自动选择良好动作的算法。

> **一句话总结：**
强化学习：指定奖励，算法弄清如何选择好的动作。（不需要告诉算法每个输入的正确输出 y)
应用：月球登录器、工厂、金融股票交易、国际象棋等
> 

## 火星漫游者：[Mars rover example](https://www.coursera.org/learn/unsupervised-learning-recommenders-reinforcement-learning/lecture/UrcMA/mars-rover-example)

我们将使用受火星漫游者启发的简化示例来了解强化学习应用的状态是什么，以及根据它采取的行动，如何经历不同的状态，也可以享受不同的奖励。

- **强化学习中的状态**：漫游者可以处于六个位置中的任何一个，我将称这六个状态为状态1、状态2、状态3、状态4、状态5和状态6，所以漫游者从状态4开始。
- 反映**状态1可能更有价值的方式是通过奖励功能**。状态1的奖励是100，第6阶段的奖励是40，其他为 0。
- 在每一步中，漫游者可以从两个动作中选择一个。它既可以向左走，也可以向右走。在每个步骤中，**机器人都处于某种状态，称之为S，它可以选择一个动作，它也享受一些奖励，它从该状态中获得的R of S**。
- **终端状态**：在它到达这些终端状态之一后，在该状态获得奖励，那之后，它无法获得额外的奖励。-
    
    ![截屏2023-04-16 16.56.21.png](3%203%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%20728f256b941e4b34a0b4f5612a26a85c/%25E6%2588%25AA%25E5%25B1%258F2023-04-16_16.56.21.png)
    
- **强化学习算法在决定如何采取行动时会关注的核心元素：状态 State 、行动 Action、奖励 Reinforcement 和下一个状态 next State。**
    
    （s,a,R(s),s’)
    
    这里的奖励，R of S，与 S 相关，而不是 s’相关。
    

这就是强化学习应用程序如何运作的。在下一个视频中，让我们看看我们如何确切地指定我们希望强化学习算法做什么。特别是，我们将讨论强化学习中的一个重要想法，称为回归。

> **一句话总结：**
强化学习的状态：**`(s,a,R(s),s’)`**state 当前状态、action 动作、reinforcement  of s 当前状态初始奖励、nextState 动作后的状态。
> 

## 强化学习的回报：[The Return in reinforcement learning](https://www.coursera.org/learn/unsupervised-learning-recommenders-reinforcement-learning/lecture/5SCL1/the-return-in-reinforcement-learning)

如何知道一组特定的奖励比一组不同的奖励更好还是更坏？我们将在本视频中定义的强化学习的回报，使我们能够捕捉到这一点。

- 假设场景：如果你想象你的脚下有一张5美元的钞票，你可以伸手去拿，或者半小时穿过镇子，你可以步行半小时去拿一张10美元的钞票。你宁愿追求哪一个？十美元比五美元好得多，但如果你需要步行半小时去拿那张10美元的钞票，那么也许拿起五美元的钞票会更方便。
- **回报：系统获得的奖励的总和，由折扣系数加权，**其中遥远未来的奖励由提升到更高力量的折扣因素加权。您获得的回报取决于奖励，奖励取决于您采取的行动，因此回**报取决于您采取的行动。**
- **折扣系数的选择：**一个非常接近1的数字，如0.9，或0.99，甚至0.999。
    - 火星漫游者的例子。如果您从状态4开始向左走，我们看到您获得的奖励在状态4的第一步为零，从状态3为零，从状态2为零，然后在状态1获得100，即终端状态。
    - 示例中使用0.5的折扣系数。这非常沉重的权重或我们非常频繁地说未来的折扣奖励，因为每增加一个额外的解析时间戳，你只能获得比提前一步获得的奖励一半的积分。
        
        ![截屏2023-04-16 18.03.49.png](3%203%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%20728f256b941e4b34a0b4f5612a26a85c/%25E6%2588%25AA%25E5%25B1%258F2023-04-16_18.03.49.png)
        
    - 在金融应用中，贴现系数也有非常自然的解释，即货币的利率或时间价值。如果你今天能拥有一美元，那可能比你将来只能得到一美元更值钱。因为即使是今天一美元，你也可以存入银行，赚取一些利息，一年后最终会多赚一点钱。对于金融应用，通常，与今天的一美元相比，这个折扣因素代表着未来一美元要少得多。
- **获得的回报取决于奖励，奖励取决于您采取的行动，因此回报取决于您采取的行动。**更快地获得的奖励可能比需要很长时间才能获得的奖励更具吸引力。
    - 因为如果你从不同的州开始，如果你总是向右走。我们看到它总是向右走。在大多数州，您期望获得的回报率较低。也许总是向右走不如总是向左走那么好。但事实证明，我们不必总是向左走，总是向右走。我们也可以决定你是否在州2，向左转。如果你在状态3，请向左走。如果你在4号州，请向左走。但如果你处于5号州，那么你就离这个奖励太近了。
        
        ![截屏2023-04-16 18.09.00.png](3%203%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%20728f256b941e4b34a0b4f5612a26a85c/%25E6%2588%25AA%25E5%25B1%258F2023-04-16_18.09.00.png)
        
- 如果任何回报是负面的，那么折扣因素实际上会激励系统尽可能地将负面的回报推向未来。
    - 以财务为例，如果你必须付给某人10美元，那可能是负10的负回报。但是，如果你能推迟几年付款，那么你实际上会更好，因为几年后10美元，因为利率实际上低于你今天必须支付的10美元。

总而言之，强化学习的回报是系统获得的奖励的总和，由折扣系数加权，其中遥远未来的奖励由提升到更高力量的折扣因素加权。现在，当你拥有具有负回报的系统时，它会导致算法试图尽可能地将奖励推向未来。

您现在知道强化学习的回报是什么了，让我们继续下一个视频，正式确定强化学习算法的目标。

> **一句话总结：**
回报Return：系统获得的奖励的总和，由折扣系数加权。（达到终端状态的奖励总和）
> 

## 强化学习算法的策略：[Making decisions: Policies in reinforcement learning](https://www.coursera.org/learn/unsupervised-learning-recommenders-reinforcement-learning/lecture/BsteY/making-decisions-policies-in-reinforcement-learning)

让我们正式确定**强化学习算法如何选择动作**。在本视频中，您将了解强化学习算法的策略。

- 多种选择方式：
    - 总是选择更近的奖励，所以如果这个最左边的奖励更近，你就向左走，或者如果这个最右边的奖励更近，你就向右走。
    - 总是选择更大的奖励，或者我们总是可以选择更小的奖励，这似乎不是个好主意
    - 选择向左走，除非你离较小的奖励只有一步之遥，在这种情况下，你选择那个。
- **策略函数 Policy**：其工作是将任何状态S作为输入，并将其映射到它希望我们采取的一些行动a。
    - 例如，对于底部的这个政策，这个政策会说，如果你在州2，那么它会将我们映射到左边的行动。如果你在州3，政策说向左走。如果你在州4，也向左走，如果你在州5，也向右走。
    
    ![截屏2023-04-16 19.13.56.png](3%203%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%20728f256b941e4b34a0b4f5612a26a85c/%25E6%2588%25AA%25E5%25B1%258F2023-04-16_19.13.56.png)
    
- **强化学习的目标**：**找到一个S的Pi或Pi策略，告诉你在每个状态下采取什么行动，以最大限度地提高回报。**
    
    ![截屏2023-04-16 19.21.31.png](3%203%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%20728f256b941e4b34a0b4f5612a26a85c/%25E6%2588%25AA%25E5%25B1%258F2023-04-16_19.21.31.png)
    

在上一个视频中，我们在强化学习方面经历了不少概念，从状态到行动，奖励，再到回报，再到政策。让我们在下一个视频中快速回顾一下它们，然后我们开始开发寻找该策略的算法。

> **一句话总结：**
强化学习的目标：找到一个S的Pi或Pi策略，告诉你在每个状态下采取什么行动，以最大限度地提高回报。  s—-pi—->a ，最大化 return
> 

## 回顾一下关键概念：[Review of key concepts](https://www.coursera.org/learn/unsupervised-learning-recommenders-reinforcement-learning/lecture/fvSTb/review-of-key-concepts)

我们使用六态火星漫游者的例子开发了一种强化学习形式主义。让我们快速回顾一下关键概念，看看这组概念如何用于其他应用程序。

- 我们讨论的一些概念是强化学习问题的状态，一组行动，奖励，折扣因素，然后如何使用奖励和折扣因素来计算回报，最后，一项政策的工作是帮助您选择行动，以最大限度地提高回报。
    
    ![截屏2023-04-16 19.26.26.png](3%203%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%20728f256b941e4b34a0b4f5612a26a85c/%25E6%2588%25AA%25E5%25B1%258F2023-04-16_19.26.26.png)
    
    - 一架自动直升机。设置状态将是直升机的一组可能位置、方向和速度等。可能的行动是移动直升机控制杆的一系列可能方法，如果它飞行良好，奖励可能是加号，如果它没有摔得非常糟糕或坠毁，奖励可能是负1000。奖励功能，告诉你直升机飞行情况。
    - 折扣系数，一个略小于一个的数字，可以说是0.99，然后根据奖励和折扣系数，您使用相同的公式计算回报。
    - 强化学习算法的工作是找到一些s的策略Pi，以便作为输入，直升机的位置，它告诉你要采取什么行动。也就是说，告诉你如何移动控制杆。
- **马尔可夫决策过程**（Markov决策过程、**MDP**）：**指未来只取决于当前状态，而不取决于在进入当前状态之前可能发生的任何事情。**换句话说，在马尔可夫的决策过程中，未来只取决于你现在在哪里，而不是你是如何来到这里的。
    - 选择行动a的方式是决策Pi，根据世界上发生的事情，然后我们可以看到或观察我们处于什么状态，以及我们得到的奖励。
    
    ![截屏2023-04-16 19.31.13.png](3%203%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%20728f256b941e4b34a0b4f5612a26a85c/%25E6%2588%25AA%25E5%25B1%258F2023-04-16_19.31.13.png)
    

你现在知道强化学习问题是如何运作的了。在下一个视频中，我们将开始开发一种选择良好动作的算法。实现这一目标的第一步是定义，然后最终学习计算状态动作值函数。事实证明，这是我们想要开发学习算法的关键数量之一。让我们进入下一个视频，看看这是什么，状态操作值函数。

# **Practice quiz: Reinforcement learning introduction剩余 1 个评分作业**

---

- [购买订阅以解锁此课程内容。Reinforcement learning introduction**到期日：May 7, 11:59 PM CST**测验•4 个问题成绩：--](https://www.coursera.org/learn/unsupervised-learning-recommenders-reinforcement-learning/exam/522R6/reinforcement-learning-introduction)

# **State-action value function**

---

## **状态动作值函数的定义：**[State-action value function definition](https://www.coursera.org/learn/unsupervised-learning-recommenders-reinforcement-learning/lecture/FEU97/state-action-value-function-definition)

当我们本周晚些时候开始开发强化学习时间时，您会看到强化学习箭头将尝试计算一个关键数量，这被称为**状态动作值函数**。

- 状态操作值函数：是一个表示回报的函数，通常用大写字母Q(s,a)，等于当前状态为S，只采取一次行动为A，在采取行动A之后表现得最好回报。简单的说，**在当前状态下选择会带来最高回报的行动**。
    
    ![Q(2,→)=12.5 是因为从 2→3←2←1](3%203%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%20728f256b941e4b34a0b4f5612a26a85c/%25E6%2588%25AA%25E5%25B1%258F2023-04-16_19.50.56.png)
    
    Q(2,→)=12.5 是因为从 2→3←2←1
    
    - 最好的回报：状态s并选择能让 Q(s,a) 最大化的动作 a
    - 在状态 s 下最好的动作 a：可以让Q(s,a) 最大化。
        
        ![截屏2023-04-16 20.03.46.png](3%203%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%20728f256b941e4b34a0b4f5612a26a85c/%25E6%2588%25AA%25E5%25B1%258F2023-04-16_20.03.46.png)
        

总结一下，您是否可以**计算s,a的Q。对于每个状态和每个操作，这给了我们一个计算S的自动策略pi的好方法。这就是状态动作值函数或Q函数。**我们稍后将讨论如何想出一个算法来计算它们，尽管Q函数的定义略呈圆形。但首先让我们看看下一个视频，看看这些值Q of s,a的具体例子。d

> **一句话总结：**
状态动作值函数：`max a Q(s,a) = Return` ，状态 s 下 pi 的最好
> 

## 状态操作值函数示例：[State-action value function example](https://www.coursera.org/learn/unsupervised-learning-recommenders-reinforcement-learning/lecture/NG3vW/state-action-value-function-example)

使用状态操作值函数示例。你看到了QSA的价值观是什么样子的。

- 黄色箭头： 代表状态 s 下 Q(sa) 选择的a的方向。

![截屏2023-04-16 20.18.07.png](3%203%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%20728f256b941e4b34a0b4f5612a26a85c/%25E6%2588%25AA%25E5%25B1%258F2023-04-16_20.18.07.png)

- 降低终端奖励：只有10，看看SA的Q是如何变化的。如果你处于状态5。当右边的奖励如此之小时，只有10。即使你离你这么近，也宁愿一直向左走。
    
    ![截屏2023-04-16 20.24.36.png](3%203%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%20728f256b941e4b34a0b4f5612a26a85c/%25E6%2588%25AA%25E5%25B1%258F2023-04-16_20.24.36.png)
    
- **提高折扣系数**：终端正确的奖励改回40，但折扣系数改为0.9，折扣系数更接近1。这使得火星漫游者不那么不耐烦了，**愿意花更长的时间来获得更高的奖励**，因为未来的奖励不会乘以0.5到某些高功率乘以0.9到某些高功率。
    
    ![截屏2023-04-16 20.25.20.png](3%203%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%20728f256b941e4b34a0b4f5612a26a85c/%25E6%2588%25AA%25E5%25B1%258F2023-04-16_20.25.20.png)
    
- **减少折扣系数**：现在让我们把伽玛改成一个更小的数字，比如0.3。因此，更多的时间回报越来越少，这使得它非常不耐烦。
    
    ![截屏2023-04-16 20.25.48.png](3%203%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%20728f256b941e4b34a0b4f5612a26a85c/%25E6%2588%25AA%25E5%25B1%258F2023-04-16_20.25.48.png)
    

因此，我希望您能通过自己玩弄这些数字并运行此代码来获得感觉。SA的Q值如何变化，就像您注意到这两个数字QSA中较大的最佳回报一样变化。这如何变化，以及最佳政策如何变化。你到实验室玩完后，我们准备回来讨论强化学习中最重要的一个方程，也就是所谓的贝尔曼方程。

## [购买订阅以解锁此课程内容。State-action value function (optional lab)实验室•. Duration: 1 hour1h](https://www.coursera.org/learn/unsupervised-learning-recommenders-reinforcement-learning/ungradedLab/X09lt/state-action-value-function-optional-lab)

## 贝尔曼方程：[Bellman Equation](https://www.coursera.org/learn/unsupervised-learning-recommenders-reinforcement-learning/lecture/3Wpee/bellman-equation)

如果您可以计算S，A的状态动作值函数Q，那么它为您提供了一种从每个场景中选择良好动作的方法。只需选择动作A，就能给你S,A中Q的最大值。问题是，你如何计算这些值Q of S,A？在强化学习中，有一个叫做贝尔曼方程的关键方程，它将帮助我们计算状态作用值函数。

- **贝尔曼方程**：将总回报分解为立即获得的奖励，直接奖励加上伽玛乘以下一个状态的素数的回报。即**将奖励分解成你立即得到的东西加上你未来得到的东西。**

让我们看看这个等式是什么。提醒一下，这是S，A的Q的定义。如果我们从状态S开始，采取一次行动，然后他们表现得最好，这就是返回。为了描述贝尔曼方程，我将使用以下符号。我将用S来表示当前状态。接下来，我将使用S的R来表示当前状态的奖励。对于我们的小MDP示例，我们将有一个状态1的r是100。状态2的奖励是0，以这样。国家6的奖励是40。我将使用字母表A来表示当前操作，即您在状态S中采取的操作。采取行动a后，你会进入一些新的状态。例如，如果你在州4，你采取左边的行动，那么你就到了州3。我将使用S prime来表示您在从当前状态S采取该操作后获得的状态。我还打算用A prime来表示你可能在状态S prime中采取的行动，这是你得到的新稳定。顺便说一下，符号约定是S，A对应于当前状态和动作。当我们添加素数时，这是下一个状态，然后是下一个动作。贝尔曼方程如下。它说，S，A的Q，即这组假设下的回报，等于S的r，说你因为处于该状态而获得的奖励加上所有可能动作的最大减数伽马倍，S素数的q的素数，你刚刚获得的新状态，然后是素数。这个等式中发生了很多事情。

![截屏2023-04-16 20.49.26.png](3%203%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%20728f256b941e4b34a0b4f5612a26a85c/%25E6%2588%25AA%25E5%25B1%258F2023-04-16_20.49.26.png)

让我们先看看一些例子。我们会回来看看为什么这个等式可能有意义。让我们看一个例子。让我们看看状态2的Q和行动。将贝尔曼方程应用于此，看看它给我们带来了什么价值。如果当前状态是状态二，并且操作是向右转，那么第二天你到达，写完S素数后将是状态3。贝尔曼方程说Q为2，右是S的R。这个R状态2只是奖励零加上我们在此示例中设置为0.5的折扣因子伽玛，乘以状态3中状态S素数中Q值的最大值。这将是25和6.25的最大值，因为这是S素数逗号的q的素数的最大值。这在25或6.25中占较大，因为这是州3的两个选择。事实证明，这等于零加0.5乘以25，等于12.5，幸运的是，这是二的Q，然后是动作正确。让我们再看一个例子。如果你决定向左走，让我走4号州，看看4号州的Q是什么。在这种情况下，当前状态是四个当前操作是向左走。下一个州，如果你能从四个开始向左走。你最终也会在州3。让我们再次为这三个上基，贝尔曼方程，我们会说这等于S的R。我们的状态四，是零加0.5，是S素数q的素数上最大值的减数伽玛。那又是状态3，逗号是素数。同样，状态3的Q值是25和6.25，其中较大的是25。这相当于我们的40加0.5乘以25，这同样等于12.5。这就是为什么左动作的四的q也等于12.5，只有一个音符，如果你处于终端状态，那么贝尔曼方程简化为SA的q等于S的r，因为没有状态S素数，所以第二个项会消失。这就是为什么终端状态下的S，A的Q只有100、100或40 40。如果您愿意，请随时暂停视频，并将贝尔曼方程应用于此MDP中的任何其他状态操作，并自行检查此数学是否有效。

![截屏2023-04-16 20.53.12.png](3%203%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%20728f256b941e4b34a0b4f5612a26a85c/%25E6%2588%25AA%25E5%25B1%258F2023-04-16_20.53.12.png)

回顾一下，这就是我们如何定义S,A的Q。我们早些时候看到，任何状态S的最佳回报率都是S,A的Q。事实上，只是为了重命名SNA，事实证明，状态S素数的最佳回报是素数的S素数的最大值。除了将S、S素数和a重命名为素数外，我真的没有做任何事情。但这以后会让一些直觉变得容易一些。但对于任何状态S素数，如状态3，从状态3获得的最佳可能回报是S素数E素数Q的所有可能动作的最大值。这又是贝尔曼方程。这捕捉到的直觉是，如果你从状态开始，你要采取行动，然后在那之后采取最佳行动，那么随着时间的推移，你会看到一些奖励序列。特别是，回报将从第一步的奖励中计算出来，加上第二步的伽玛倍奖励加上第三步的伽玛平方奖励，以其他步骤。加上点，点，点，直到你到达终端状态。贝尔曼方程说的是，这种奖励序列，折扣因素是什么，可以分为两个部分。首先，这个R of s，这就是你立即得到的奖励。在强化学习文献中，这有时也被称为即时奖励，但这就是R_1。这是你在某些州起步得到的奖励。第二个项如下；在你从状态s开始并采取行动a后，你会得到一些新的状态的素数。s的Q的定义，a假设我们将在那之后表现得最好。到达s素数后，我们将以最佳方式行事，并从状态s素数中获得最佳返回。这是什么，q的素数的最大值是一个素数，这是从状态的素数开始的最佳行为的返回。这正是我们在这里写的，当你从州素数开始时，这是最好的回报。另一种措辞是，这里的总回报也等于R_1加，然后我们将在地图中分解伽玛，是伽玛乘以R_2加，而不是伽玛平方只是伽玛乘以R_3加伽玛平方乘以R_4加点点。请注意，如果您从状态的素数开始，您获得的奖励序列将是R_2，R_3，然后是R_4，等等。

这就是为什么这里的这个表达式，如果你要从状态的素数开始，这就是总回报。如果你要表现得最好，那么这个表达式应该是从状态素数开始的最佳回报，这就是为什么这个折扣奖励序列等于s素数的Q的素数的最大值，那里还有这个额外的折扣因子伽玛的剩余部分，这就是为什么s的Q，a也等于这里的这个表达式。如果您认为这相当复杂，并且您没有遵循所有细节，请不要担心。只要你应用这个方程，你就能得到正确的结果。但我希望你带走的高级直觉是，你在**强化学习问题中获得的总回报有两部分**。**第一部分是你立即获得的奖励，然后第二部分是伽玛乘以你从下一个状态的素数开始的回报。**当这两个成分在一起时，s的R加上伽玛乘以下一个状态的返回，这等于当前状态s的总返回。这就是贝尔曼方程的本质。

![截屏2023-04-16 20.57.29.png](3%203%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%20728f256b941e4b34a0b4f5612a26a85c/%25E6%2588%25AA%25E5%25B1%258F2023-04-16_20.57.29.png)

只是把这个和我们之前的例子联系起来，Q of 4，左边。这是从第4州开始和向左走的总回报。如果您在州4中向左走，您将获得的奖励在州4中为0，在州3中为0，在州2中为0，然后为100，这就是为什么总回报是这个；0.5平方加0.5立方，即12.5。贝尔曼方程说的是，我们可以把它分成两部分。有这个零，是状态四的R，然后加0.5乘以另一个序列，0加0.50加0.5平方乘以100。但是，如果你看看这个序列是什么，这实际上是你从状态四离开的动作后得到的下一个状态素数的最佳返回。这就是为什么这等于奖励4加状态3最佳回报的0.5倍。因为如果你从状态3开始，你得到的奖励将是零，然后是零，然后是100，所以这是来自状态3的最佳回报，这就是为什么这只是R的4加0.5的最大值，在状态3的素数Q，一个素数。

![截屏2023-04-16 21.00.55.png](3%203%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%20728f256b941e4b34a0b4f5612a26a85c/%25E6%2588%25AA%25E5%25B1%258F2023-04-16_21.00.55.png)

我知道贝尔曼方程，这是一个有点复杂的方程，将你的总回报分解为你立即获得的奖励。直接奖励加上伽玛乘以下一个状态的素数的回报。如果这对你来说有意义，但不是完全的，没关系。别担心。你仍然可以应用贝尔曼方程来使强化学习算法正常工作，但我希望至少有高水平的直觉，即**为什么将奖励分解成你立即得到的东西加上你未来得到的东西**。

在继续开发强化学习算法之前，我们接下来要制作一个关于随机马尔可夫决策过程或强化学习应用程序的可选视频，如果您采取这些操作，可能会产生略微随机的影响。如果您愿意，请观看可选视频。然后，我们将开始开发强化学习算法。

> **一句话总结：**
贝尔曼方程：`**Q(s,a) = R(s) + r *max_a’ Q(s’,a’)**`，立即得到的+未来得到的
> 

## 随机环境：[Random (stochastic) environment (Optional)](https://www.coursera.org/learn/unsupervised-learning-recommenders-reinforcement-learning/lecture/rL525/random-stochastic-environment-optional)

在某些应用程序中，当您采取行动时，结果并不总是完全可靠。

例如，如果你命令你的火星漫游者向左走，也许有一点岩石滑梯，或者地板真的很滑，所以它滑了，朝错误的方向走。**在实践中，由于风吹离航线，车轮打滑或其他原因，许多机器人并不总是能够完全按照你告诉他们的去做。**到目前为止，我们谈到了强化学习框架的推广，该框架模拟了随机或随机环境。

- 随机环境的例子：**如果你命令它向左走，它有90%的机会或0.9的机会正确地向左走。但0.1的几率实际上向右走，**
    
    ![截屏2023-04-16 21.05.01.png](3%203%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%20728f256b941e4b34a0b4f5612a26a85c/%25E6%2588%25AA%25E5%25B1%258F2023-04-16_21.05.01.png)
    
- 让我们看看这个强化学习问题会发生什么。假设你使用这里显示的这个政策，你在2 3 4阶段向左走，然后向右走，或者尝试在第五阶段向右走。如果您从状态四开始，并且要遵循此政策，那么您访问的状态的实际顺序可能是随机的。例如，在状态四中，你会向左走，也许你的循环和幸运，它实际上得到了状态三，然后你再次尝试向左走，也许它实际上到达了那里。你告诉它再次向左走，它就会到达那个状态。如果发生这种情况，你最终会得到000100的奖励序列。但如果你第二次尝试完全相同的政策，也许你第二次从这里开始就不那么幸运了。试着向左走，看到它成功了，所以从状态四的零，从状态三的零，听到你告诉它向左走，但这次你很不走，机器人滑倒了，最终回到了第四态。然后你被教导叫左，左，左，左，最终获得100的奖励。在这种情况下，这将是你观察到的奖励顺序。这个从四到三到四，三二，然后一，甚至有可能，如果你从四号州告诉按照政策向左走，即使在第一步，你也可能会不走，你最终会去五号州，因为它滑倒了。然后陈述五，你命令它向右走，当你在这里结束时，它就成功了。在这种情况下，您看到的奖励顺序将是0040，因为它从四到五，然后是六，我们之前将回报写成这个折扣奖励的总和。但是，当强化学习问题是随机的时，你肯定不会看到一个奖励序列，相反，你会看到这个不同奖励序列。在随机强化学习问题中，我们感兴趣的不是最大化回报，因为这是一个随机数。我们感兴趣的是最大化折扣奖励总和的平均价值。按平均值，我的意思是，如果你拿你的保单尝试一千次或10万次或一百万次，你会得到很多类似的不同奖励序列，如果你以所有这些不同序列的折扣奖励总和的平均值，那么这就是我们所说的预期回报。在统计学中，预期一词只是表示平均值的另一种方式。但这意味着我们希望在折扣奖励总额方面最大化我们预期的平均水平。这个的数学符号是把这个写成E。E代表期望值R1加伽玛R2加，以这样。强化学习算法的工作是选择一个策略Pi，以最大化折扣奖励的平均或预期总和。

![截屏2023-04-16 21.08.50.png](3%203%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%20728f256b941e4b34a0b4f5612a26a85c/%25E6%2588%25AA%25E5%25B1%258F2023-04-16_21.08.50.png)

总之，当您遇到随机强化学习问题或随机马尔可夫决策过程时，目标是选择一项政策，告诉我们在S状态中采取什么行动，以最大限度地提高预期回报。我们谈论的最后一个改变方式是它稍微修改了贝尔曼方程。这是我们写下的贝尔曼方程。但现在的区别是，**当你在状态s中采取动作a时，你得到的下一个状态的质数是随机的。**当你在状态3时，你告诉它离开下一个状态的素数，它可能是状态2，也可能是状态4。S素数现在是随机的，这就是为什么我们也在这里放了一个平均运算符或意外运算符。我们说，状态的总回报，一旦采取最佳行动，就等于你立即获得的回报，也称为即时回报加上折扣系数，伽玛加上你期望获得的平均未来回报。

![截屏2023-04-16 21.10.46.png](3%203%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%20728f256b941e4b34a0b4f5612a26a85c/%25E6%2588%25AA%25E5%25B1%258F2023-04-16_21.10.46.png)

- 如果你想敏锐地直觉这些随机强化学习问题会发生什么。你会回到我刚才给你看的可选实验室
    - 采取这个最佳策略，但机器人10%的时间会朝着错误的方向走，这些是这个随机NTP的q值。
        
        ![截屏2023-04-16 21.12.10.png](3%203%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%20728f256b941e4b34a0b4f5612a26a85c/%25E6%2588%25AA%25E5%25B1%258F2023-04-16_21.12.10.png)
        
    - 如果你要增加失误的概率，比如说40%的时间，机器人甚至没有进入方向。你只有60%的时间命令它。它去了你告诉它的地方，然后这些值最终会更低，因为你对机器人的控制程度已经下降。
        
        ![截屏2023-04-16 21.12.18.png](3%203%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%20728f256b941e4b34a0b4f5612a26a85c/%25E6%2588%25AA%25E5%25B1%258F2023-04-16_21.12.18.png)
        

现在，在我们迄今为止所做的一切中，我们一直在使用这个马尔可夫决策过程，这个只有六个状态的火星漫游者。对于许多实际应用来说，州的数量会大得多。在下一个视频中，我们将采用我们到目前为止所谈论的强化学习或马尔可夫决策过程框架，并将其推广到这组更丰富甚至更有趣的问题，这些问题要大得多，特别是连续状态空间，让我们在下一个视频中看看。

> **一句话总结：**
随机环境：当你在状态s中采取动作a时，你得到的下一个状态是随机的。（实际动作和命令动作不相符）
`**Q(s,a) = R(s) + r *E[max_a’ Q(s’,a’)]**`
> 

# **Quiz: State-action value function剩余 1 个评分作业**

---

- [购买订阅以解锁此课程内容。State-action value function**到期日：May 7, 11:59 PM CST**测验•3 个问题成绩：--](https://www.coursera.org/learn/unsupervised-learning-recommenders-reinforcement-learning/exam/yGtub/state-action-value-function)
- 

---

# **Continuous state spaces**

## 连续状态空间的应用例：[Example of continuous state space applications](https://www.coursera.org/learn/unsupervised-learning-recommenders-reinforcement-learning/lecture/kVphz/example-of-continuous-state-space-applications)

许多机器人控制应用程序，包括您在实践实验室工作的月球着陆器应用程序，都有**连续的状态空间。**让我们看看这意味着什么，以及如何将我们谈论的概念推广到这些连续状态空间。

我们使用的简化火星漫游者示例，我使用一组离散的状态，这意味着简化火星漫游者只能在六个可能的位置之一。但大多数机器人可以处于六个或任何离散数量位置中的多个位置，相反，它们可以处于非常多的连续值位置中的任何一个。例如，如果火星漫游者可以在一条线上的任何地方，那么它的位置由0-6公里不等的数字表示，中间的任何数字都是有效的。这将是连续状态空间的一个例子，因为位置将由一个数字表示，例如沿2.7公里或4.8公里或零到6之间的任何其他数字。让我们看看另一个例子。在这个例子中，我将使用控制汽车或卡车的应用。这是一辆玩具车，俄罗斯玩具卡车。这个是我女儿的。如果您正在建造一辆自动驾驶汽车或自动驾驶卡车，并且您想控制它以平稳驾驶，那么这辆卡车的状态可能包括一些数字，例如，x位置，它的y位置，也许是方向。它朝向什么方向？假设卡车停在地上，你可能不需要担心它有多高，有多高。这个状态将包括x，y，是角度Theta，以及它可能在x方向的速度，在y方向的速度，以及它的转动速度。它是以每秒1度的速度转动，还是以每秒30度的速度转动，还是以每秒90度的速度快速转动？对于卡车或汽车，状态可能不仅包括一个数字，就像这条线上有多少公里，但它们可能包括六个数字，是x位置，是y位置，是方向，我将用希腊字母Theta表示，以及它在x方向的速度，我将用x点表示，所以这意味着这个x坐标变化有多快，y点y坐标变化有多快，最后，Theta点，这是汽车而对于60火星漫游者的例子，状态只是六个可能的数字之一。可能是一、二、三、四、五或六。对于汽车，状态将包括这个由六个数字组成的向量，这些数字中的任何一个都可以在有效范围内接受任何值。例如，Theta的范围应该在零度到360度之间。

![截屏2023-04-16 21.52.34.png](3%203%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%20728f256b941e4b34a0b4f5612a26a85c/%25E6%2588%25AA%25E5%25B1%258F2023-04-16_21.52.34.png)

让我们看看另一个例子。如果你正在构建一个加固学习算法来控制一架自主直升机，你会如何描述直升机的位置？为了说明，我这里有一架小型玩具直升机。直升机的定位将包括x位置，例如直升机向北或向南有多远，是y位置。也许直升机在东西轴上有多远，然后还有z，直升机在地面上的高度。但除了位置外，直升机还有一个方向，传统上，捕获其方向的一种方法是增加三个数字，其中一个捕获直升机的行。它向左滚动还是向右滚动？投球，是向前投球还是向上投球，向后投球，最后是向西的偏航，指南针方向朝向。如果面向北方、东方、南方或西方？总之，直升机的状态包括位置，比如，南北方向，定位东西方向，y是地面以上的高度，还有行，俯仰，还有直升机的偏航。因此，要写下来，状态包括位置x、y、z，然后是行间距，以及用希腊字母Phi、Theta和Omega表示的偏航。但要控制直升机，我们还需要知道它在x方向、y方向和z方向的速度，以及它的转向速度，也称为角速度。这行的变化有多快，这个音高的变化有多快，它的偏航变化有多快？这实际上是用来控制自主直升机的国家。这是输入到政策的12个数字的列表，政策的工作是查看这12个数字，并决定在直升机上采取什么适当的行动。

![截屏2023-04-16 21.54.32.png](3%203%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%20728f256b941e4b34a0b4f5612a26a85c/%25E6%2588%25AA%25E5%25B1%258F2023-04-16_21.54.32.png)

因此，任何连续状态强化学习问题或连续状态马尔可夫决策过程，连续MTP。问题的状态不仅仅是少数可能的离散值之一，就像1-6的数字一样。相反，它是一个数字向量，其中任何一个都可以接受大量值中的任何一个。在本周的实践实验室中，您可以为自己实现应用于模拟月球着陆器应用的强化学习算法。在月球上着陆是模拟。让我们在下一个视频中看看该应用程序需要什么，因为会有另一个连续状态应用程序。

> **一句话总结：**
连续值的向量，如 x，y，z，速度、角度等
> 

## 月球着陆器：[Lunar lander](https://www.coursera.org/learn/unsupervised-learning-recommenders-reinforcement-learning/lecture/C9BJf/lunar-lander)

月球着陆器允许您将模拟飞行器降落在月球上。这就像一个有趣的小游戏，被许多强化学习研究人员使用过。让我们看看它是什么。在这个应用程序中，你指挥着一个正在快速接近月球表面的月球着陆器。你的工作是在适当的时候使用火推进器，将其安全降落在着陆台上。

![截屏2023-04-16 21.57.06.png](3%203%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%20728f256b941e4b34a0b4f5612a26a85c/%25E6%2588%25AA%25E5%25B1%258F2023-04-16_21.57.06.png)

![截屏2023-04-16 21.57.33.png](3%203%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%20728f256b941e4b34a0b4f5612a26a85c/%25E6%2588%25AA%25E5%25B1%258F2023-04-16_21.57.33.png)

让你了解它是什么样子。这是月球着陆器成功着陆，它向下、向左和向右发射推进器，以定位在这两面黄旗之间着陆。或者，如果加固着陆算法策略做得不好，那么这就是着陆器不幸在月球表面被破坏的样子。在这个应用程序中，您每个步骤都有四种可能的操作。你要么什么都不做，在这种情况下，惯性和重力会把你拉向月球表面。或者，当你看到左边有一个小红点出来时，你可以发射左推进器，那就是发射左边。他们会倾向于将月球着陆器推向右侧，或者你可以发射在这里底部推下的主发动机。或者你可以发射右推进器，那就是发射右推进器，这将把你推到左边，你的工作是随着时间的推移继续选择动作。所以这是着陆台上这两面旗帜之间的月球着陆器。为了给这些动作一个更短的名字，我有时会把这些动作称为无意义什么都不做或左意味着更远的左推进器或主要意味着向下或向右发射主发动机。因此，我将把行动称为什么都没有了。缅因州，稍后在本视频中简短地写作。各州面临的情况如何？Mtp各州是它的位置X和Y。那么，它向左或向右有多远，有多高，以及速度x.y，它在水平和垂直方向上移动的速度有多快，然后也是角度。那么，月球着陆器向左倾斜或向右倾斜多远？角速度注定不是。最后，因为定位上的微小差异会对它是否着陆产生很大影响。在状态向量中，我们将有另外两个变量，我们称之为l和r。这对应于左腿是否接地，即左腿是否坐在地上，以及r对应于右腿是否坐在地上。因此，xy x.theta theta.我们的数字l和r将是二进制的，并且只能具有零或1的值，这取决于左腿和右腿是否接触地面。

![截屏2023-04-16 22.00.27.png](3%203%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%20728f256b941e4b34a0b4f5612a26a85c/%25E6%2588%25AA%25E5%25B1%258F2023-04-16_22.00.27.png)

最后，他对月球着陆器的奖励功能。**如果它设法到达着陆台，就没有收到100到140之间的奖励，**这取决于它到达着陆台中心时的飞行情况。我们还给它一个额外的奖励，**因为它向垫子移动或离开垫子，这样它就会靠近垫子，它就会收到积极的奖励，然后它就会离开并漂移。它获得了负面奖励。如果它崩溃了，它会获得巨大的-100奖励，**它会实现软着陆，即着陆。还有一次碰撞，**每条腿、左腿或被接地的右链接都会获得+100的奖励**。它获得+10的奖励，并**最终鼓励它不要浪费太多的燃料和火力推进器**。每次它发射主发动机时，我们都会给它**-0.3**的奖励，每次它发射左侧或右侧推进器时，我们都会给它**-0.03**的奖励。请注意，这是一个中等复杂的奖励函数。月球着陆器应用程序的设计者实际上对您想要的行为进行了一些思考，并将其编纂到奖励功能中。激励更多你想要的行为，并害怕你不想要的崩溃等行为。

![截屏2023-04-16 22.04.48.png](3%203%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%20728f256b941e4b34a0b4f5612a26a85c/%25E6%2588%25AA%25E5%25B1%258F2023-04-16_22.04.48.png)

您发现，当您构建自己的强化学习应用程序时，通常会考虑具体说明您想要或不想要什么，并在奖励功能中编纂。但指定奖励函数仍然应该更容易指定从每个状态采取的确切正确行动。对于这个和许多其他强化学习应用程序来说，这要难得多。

因此，月球着陆器问题如下。**我们的目标是学习政策pi。当给出这里写的状态S时，选择一个动作a等于S的pi。为了使折扣奖励的回报最大化。**通常，对于月球着陆器来说，伽马射线会使用相当大的值。事实上，将使用等于0.985的相机值，所以非常接近一个。

![截屏2023-04-16 22.05.46.png](3%203%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%20728f256b941e4b34a0b4f5612a26a85c/%25E6%2588%25AA%25E5%25B1%258F2023-04-16_22.05.46.png)

如果你能学到一个能做到这一点的政策pi，那么你就能成功着陆这个令人兴奋的月球着陆器应用程序，我们现在终于准备好开发一种学习算法，该算法将使用深度学习或神经网络来制定着陆月球着陆器的政策。让我们进入下一个视频，在那里我们开始学习深度强化。

## 学习状态值函数：[Learning the state-value function](https://www.coursera.org/learn/unsupervised-learning-recommenders-reinforcement-learning/lecture/EH7Zf/learning-the-state-value-function)

让我们看看如何使用强化学习来控制月球着陆器或其他强化学习问题。关键的想法是，我们**将训练一个神经网络来计算或近似s，a的状态动作值函数Q，这反过来将让我们选择好的动作。**

让我们看看它是如何运作的。**学习算法的核心是训练一个神经网络，该神经网络输入当前状态和当前动作，并计算或近似s的Q，a。**特别是，对于月球着陆器，我们将采取状态和任何行动，并把它们放在一起。具体来说，状态是我们之前看到的八个数字的列表，所以你有xy，x点，y点，Theta，Theta点，然后LR用于腿接地的位置，所以这是描述状态的八个数字的列表。最后，我们有**四种可能的行动：什么都没有，左，主，主引擎和右**。我们可以使用**单热特征向量对这四个动作中的任何一个进行编码**。如果操作是第一个操作，我们可以使用1，0，0，0对其进行编码，或者如果是找到左集群的第二个操作，我们可以将其编码为0，1，0，0。这个12个数字的列表，状态的8个数字，然后是4个数字，动作的单热编码是我们对神经网络的输入，我将称之为X。然后，我们将把这12个数字输入到神经网络，比如说，第一个隐藏层有64个单位，第二个隐藏层有64个单位，然后在输出层中输入单个输出。神经网络的工作是s的输出Q，a。给定输入s和a的月球着陆器的状态动作值函数。由于我们稍后将使用神经网络训练算法，我还将参考这个值Q of s，a作为训练神经网络的目标值Y。请注意，我确实说过强化学习与监督学习不同，但我们要做的不是输入状态并让它输出一个操作。我们要做的是输入一个状态动作对，并让它尝试输出s、a的Q，并在强化学习算法中使用神经网络，这样效果会很好。我们稍后会看到细节，所以如果它还没有意义，也不要担心。但是，如果你能训练一个神经网络，在隐藏层和上层有适当的参数选择，给你一个很好的Q of s，a，那么每当你的月球着陆器处于某种状态时，你就可以使用神经网络来计算Q of s，a。对于所有四个操作，您可以计算s的Q，无，s的Q，左，s的Q，主，s的Q，右，最后，无论这些值最高，您选择相应的操作a。例如，如果在这四个值中，Q of s，main是最大的，那么你会决定去发射月球着陆器的主发动机。问题变成了，**你如何训练神经网络来输出s的Q，a**？事实证明，**方法将是使用贝尔曼方程创建一个包含大量示例x和y的训练集，然后我们将使用监督学习，就像我们在谈论神经网络时在第二门课程中学到的那样。**要学习使用监督学习，从x到y的映射，即从状态操作对到s，a的目标值Q的映射。

![截屏2023-04-16 22.14.44.png](3%203%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%20728f256b941e4b34a0b4f5612a26a85c/%25E6%2588%25AA%25E5%25B1%258F2023-04-16_22.14.44.png)

但是，如何获得具有x和y值的训练集，然后可以训练神经网络？让我们来看看。这是贝尔曼方程，s的Q，a等于s的R加伽玛，素数的最大值，s素数的Q，素数。右手边是你想要的s的Q，a等于，所以我将在右手边叫这个值y，对神经网络的输入是一个状态和一个动作，所以我要叫它x。神经网络的工作是输入x，即输入状态动作对，并尝试准确预测右侧的值。在监督学习中，我们正在训练一个神经网络来学习一个函数f，该函数取决于一堆参数，W和B，神经网络各层的参数，神经网络的工作是输入x。希望我会放一些接近目标值y的东西。问题是**，我们如何想出一个值为x和y的训练集，让新网络从中学习**。这是我们要做的。我们将使用月球着陆器，并尝试在里面采取不同的行动。如果我们还没有一个好的政策，我们将随机采取行动，进一步，左fasser，进一步，右fasser进一步，主发动机，什么都不做。通过在月球着陆器模拟器中尝试不同的东西，我们将观察到很多例子，当我们处于某种状态时，我们采取了一些行动，这可能是一个好动作，也可能是一个可怕的行动。然后，我**们因为处于那个状态而得到了一些奖励，由于我们的行动，我们得到了一些新的状态，S'。当你在月球着陆器中采取不同的行动时，你会看到这个S，a，s的R，S'，**我们在Python代码中多次称它们为元组。例如，也许有一次你处于某种状态S，只是为了给它一个索引，我们称之为S^1，你碰巧采取了一些行动a^1，这可能什么都没有了，主要推力在那里或正确。结果，你得到了一些奖励，你想在某个州S^'1。也许在不同的时间，你处于其他状态S2，你采取了一些其他行动，可能是好的行动，可能是糟糕的行动，可能是四个行动中的任何一个，你得到了奖励，然后你想多次使用S'2等等。也许你已经这样做了10,000次，甚至超过10,000次，所以你必须不仅用S^1，a^1等，而且最多用S^10,000，a^10,000来保存。事实证明，这些元组中的每一个都足以创建单个训练示例，x^1，y^1。特别是，你是如何做到的。

第一个元组中有四个元素。前两个将用于计算x^1，后两个将用于计算y^1。特别是，x^1只是S^1，a^1放在一起。S^1将是八个数字，月球着陆器的状态，a^1将是四个数字，这是任何动作的单热编码，y^1将使用贝尔曼方程的右侧计算。特别是，贝尔曼方程说，当你输入S^1，a^1时，你希望S^1的Q，a^1是这个右手边，等于S^1的R加上Gamma max超过S^1'a'素数Q的a'。请注意，右侧元组的这两个元素为您提供了足够的信息来计算这一点。你知道S^1的R是什么吗？这就是你在这里拯救的回报。加上所有操作的减值因子伽马倍最大值，S'^1的Q的a'，这是你在这个例子中必须达到的状态，然后对所有可能的动作采取最大值，a'。我要把这个叫做y^1。当你计算这个时，这将是一些数字，如12.5或17，或0.5或其他数字。我们将在这里将该数字保存为y^1，以便这个对x^1，y^1成为我们正在计算的这个低数据集中的第一个训练示例。现在，你可能想知道，S'的Q，a'或S'^1的Q，a'来自哪里。好吧，最初我们不知道什么是Q函数。但事实证明，当你不知道什么是Q函数时，你可以从完全随机的猜测开始。什么是Q函数？我们将在下一张幻灯片上看到，尽管如此，[听不见]还是会起作用。但在每一步中，Q都只是一些猜测。随着时间的推移，它们会变得更好，事实证明，实际的Q函数是什么。

让我们看看第二个例子。如果你有第二次经历，你处于S^2状态，到达a^2，得到那个奖励，然后到达那个状态。然后，我们将在这个数据集中创建第二个训练示例，x^2，其中输入现在是S^2，a^2，所以前两个元素用于计算输入x，然后y^2将等于s^2的R加上gamma max的a'Q of S'到a'，无论这个数字是什么，y^2。我们把这个放在我们小但不断增长的训练套装中，以此类推，直到也许你最终会用这些x，y对来提供10,000个训练示例。我们稍后会看到的是，我们实际上将采用这个训练集，其中x是具有12个特征的输入，而y只是数字。我们将用平均平方误差损失来训练一个新网络，试图将y预测为输入x的函数。我在这里描述的只是我们将使用的学习算法的一部分。

![截屏2023-04-16 22.28.49.png](3%203%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%20728f256b941e4b34a0b4f5612a26a85c/%25E6%2588%25AA%25E5%25B1%258F2023-04-16_22.28.49.png)

让我们在下一张幻灯片上把它们放在一起，看看它是如何组合成一个算法的。让我们看看学习Q函数的完整算法是什么样子的。首先，我们将利用我们的神经网络，随机初始化神经网络的所有参数。最初，我们不知道它是否是Q函数，让我们选择宽度的完全随机值。我们将假装这个神经网络是我们对Q函数的最初随机猜测。**这有点像你训练线性回归时，你随机初始化所有参数，然后使用梯度下降来改进参数。现在随机初始化就可以了**。重要的是算法是否可以慢慢改进参数以获得更好的估计。接下来，我们将反复做以下事情；我们将在月球着陆器上采取行动，所以随机漂浮，采取一些好行动，采取一些坏行动。不管怎样都没关系。**但当它处于某种状态时，你会得到很多这些元组，你实际上收集了一些S的R，然后你得到了一些状态的素数。**我们将做的是存储这些元组的10,000个最新示例。当你运行这个算法时，你会在月球着陆器中看到许多步骤，也许有数十万个步骤。但为了确保我们最终不会使用过多的计算机内存**，通常的做法是记住我们在MTP中看到的10,000个最近发生的此类元组。这种仅存储最新示例的技术有时被称为强化学习算法中的重播缓冲区。**目前，我们只是随机飞行月球着陆器，有时坠毁，有时不坠毁，并为我们的学习算法获得这些元组。偶尔我们会训练神经网络。为了训练神经网络，以下是我们要做的。我们将查看我们最近保存的10,000个元组，并创建一个由10,000个示例组成的培训集。**训练集需要很多对x和y。对于我们的训练示例，x将是s，a来自元组的这一部分。将有一个12个数字的列表，状态的8个数字和动作的单热编码的4个数字。**我们希望神经网络尝试预测的目标值是y等于S的R加上素数的伽马最大值，s素数的Q等于素数。我们如何获得Q的这个值？嗯，最初是我们随机初始化的这个神经网络。这可能不是一个很好的猜测，但这是一个猜测。创建这10,000个培训示例后，我们将有培训示例x1，y1到x10,000，y10,000。我们将训练一个神经网络，我将称新的神经网络为Q new，例如Q new of s，a学习近似y。这正是训练神经网络用参数w和b输出f，输入x尝试近似目标值y。现在，这个神经网络应该更好地估计Q函数或状态作用值函数应该是什么。我们要做的是把Q设置为我们刚刚学到的这个新的神经网络。该算法中的许多想法都归功于Mnih等人。事实证明，**如果你运行这个算法，从对Q函数的真正随机猜测开始，然后使用贝尔曼方程反复尝试改进Q函数的估计值。然后，通过一遍又一遍地这样做，采取许多行动，训练一个模型，这将提高您对Q功能的猜测**。

对于您训练的下一个模型，您现在对什么是Q函数有了稍微更好的估计。然后你训练的下一个模型会更好。当您更新Q等于Q新时。然后，下次你训练s素数的模型Q时，素数将是一个更好的估计。当您在每次迭代（s素数的Q）上运行此算法时，素数有望成为Q函数的更好估计值，**这样当您运行算法足够长的时间时，这实际上将成为对s的Q的真实值的相当好的估计，**a，这样您就可以用它来选择，希望是好的动作或MTP。

![截屏2023-04-16 22.33.47.png](3%203%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%20728f256b941e4b34a0b4f5612a26a85c/%25E6%2588%25AA%25E5%25B1%258F2023-04-16_22.33.47.png)

您刚刚看到的算法有时被称为DQN算法，它代表**Deep Q-Network**，因为您正在使用深度学习和神经网络来训练模型来学习Q函数。因此，DQN或DQ使用神经网络。如果你像我描述的那样使用算法，它会在月球着陆器上起作用。也许需要很长时间才能收敛，也许它不会完美地着陆，但它会起作用。但事实证明，通过对算法进行一些改进，它可以工作得更好。在接下来的几个视频中，让我们来看看您刚刚看到的算法的一些改进。

## 算法改进：改进的神经网络架构：[Algorithm refinement: Improved neural network architecture](https://www.coursera.org/learn/unsupervised-learning-recommenders-reinforcement-learning/lecture/hpmUe/algorithm-refinement-improved-neural-network-architecture)

在上一个视频中，我们看到了一个神经网络架构，它将**输入状态和动作，并尝试输出Q函数，Q of s，a。**事实上，DQN的大多数实现实际上都使用了我们将在本视频中看到的更高效的架构。

- 初始版 DQN 算法：这是低效的，因为我们必须用神经网络进行四次推断。
    
    输入12个数字并输出s的Q，a。每当我们处于某种状态时，我们必须在神经网络中分别进行四次推理，以计算这四个值，以便选择给我们最大Q值的动作a。
    
    ![截屏2023-04-17 20.10.15.png](3%203%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%20728f256b941e4b34a0b4f5612a26a85c/%25E6%2588%25AA%25E5%25B1%258F2023-04-17_20.10.15.png)
    
- 改进版 DQN 算法：训练单个神经网络同时输出所有这四个值的效率更高。
    
    对于给定状态 s ，我们只需运行一次推理，即可获得所有 a 对应的 Q(s,a) 四个值，然后非常快速地选择最大化s的Q的动作a，a。
    
    ![截屏2023-04-17 20.09.46.png](3%203%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%20728f256b941e4b34a0b4f5612a26a85c/%25E6%2588%25AA%25E5%25B1%258F2023-04-17_20.09.46.png)
    

接下来，还有一个想法对算法有很大帮助，这是一种叫做Epsilon-greedy策略的东西，它会影响你选择动作的方式，即使你还在学习。让我们看看下一个视频，以及这意味着什么。

> **一句话总结：**
DQN 的改进：神经网络输出层，对于 s 和 a，一次输出所有 Q(s,a)。
> 

## 算法改进：ϵ-贪婪策略[Algorithm refinement: ϵ-greedy policy](https://www.coursera.org/learn/unsupervised-learning-recommenders-reinforcement-learning/lecture/GyBzo/algorithm-refinement-greedy-policy)

我们开发的学习算法，即使你还在学习如何近似Q（s，a），你也需要在月球着陆器中采取一些行动。**当你还在学习时，你如何选择这些行动？**最常见的方法是使用ϵ-greedy policy。让我们看看它是如何运作的。

- 这是你之前看到的算法。算法中的一个步骤是在月球着陆器中采取行动。当学习算法仍在运行时，我们并不真正知道在每个州采取的最佳行动是什么。如果我们这样做了，我们就已经完成了学习。
    
    ![截屏2023-04-17 20.23.28.png](3%203%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%20728f256b941e4b34a0b4f5612a26a85c/%25E6%2588%25AA%25E5%25B1%258F2023-04-17_20.23.28.png)
    
- **即使我们仍在学习，并且还没有对Q（s，a）有很好的估计，我们如何在学习算法的这一步骤中采取行动？**
    - 1-只选择最好的：
        
        产生问题，也许神经网络参数被初始化了，所以Q（s，main）总是很低。如果是这样的话，那么神经网络，因为它试图选择最大化Q（s，a）的动作a，它永远不会尝试发射主推进器。因为它从未尝试发射主推进器，所以它永远不会知道发射主推进器有时实际上是一个好主意。
        
    - 2-**探索步骤（**Epsilon-greedy policy**）**：大多数时候，假设概率为0.95，选择最大化Q（s，a）的动作。但一小部分时间，比方说，百分之五的时间，我们会随机选择一个动作。**在每个步骤的选项2下，我们尝试不同行动的概率很小，这样神经网络就可以学会克服自己对可能是一个坏主意的可能成见**。
        - **探索与剥削的权衡**，**这是指你多久随机采取行动或采取可能不是最好的行动来了解更多信息，而不是试图通过采取最大化Q（s，a）的行动来最大化你的回报。**
        - Epsilon为0.05是随机选择一个动作的概率。
        - 技巧之一是从Epsilon高开始。
            
            例如，在月球着陆器练习中，你可能会从Epsilon非常非常高开始，甚至Epsilon等于1.0。你最初只是完全随机选择行动，然后逐渐减少到0.01，这样最终你99%的时间都在采取贪婪的行动，而随机行动的时间只有很小的1%。
            
        
        ![截屏2023-04-17 20.38.42.png](3%203%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%20728f256b941e4b34a0b4f5612a26a85c/%25E6%2588%25AA%25E5%25B1%258F2023-04-17_20.38.42.png)
        
- 强化学习算法与监督学习相比，它们在选择超参数方面更挑剔。
    - 例如，在监督学习中，如果您将学习率设置得太小，那么算法可能需要更长的时间来学习。也许训练需要**三倍**的时间，这很烦人，但也许没那么糟糕。
    - 而在强化学习中，**发现如果你设置Epsilon的值不太好，或者设置其他参数不太好，学习时间不会是学习的三倍。学习可能需要10倍或100倍的时间。**

在下一个可选视频中，我希望我们再进行一些算法改进，迷你批处理，并使用软更新。即使没有这些额外的细化，算法也会正常工作，但这些都是额外的细化，使算法运行得更快。让我们在下一个视频中看看，迷你批处理和软更新。

> **一句话总结：**
ϵ-贪婪策略：ϵ设置随机选择动作的概率，不是每次都选择 Q 最高的动作。通常从最高值开始训练，最终逐渐变低。
克服神经网络对坏主意可能性的成见。
强化学习比监督学习对参数更挑剔，不好的参数会把学习时间变长（10~100 倍）
> 

## 算法优化：迷你批次下降和软更新[Algorithm refinement: Mini-batch and soft updates (optional)](https://www.coursera.org/learn/unsupervised-learning-recommenders-reinforcement-learning/lecture/TsaXj/algorithm-refinement-mini-batch-and-soft-updates-optional)

在本视频中，我们将查看您看到的强化学习算法的两项进一步改进：迷你批次下降和软更新。

- **迷你批次下降：**迷你批次梯度下降的作用是通过**算法进行第一次迭代，可能是它查看了数据的子集。**在下一次迭代中，也许它会查看数据的子集，等等。因此每次迭代都只查看数据的子集，因此每次迭代运行得更快。
    - **既可以加快您的强化学习算法，又适用于监督学习。**它们还可以帮助您加快监督学习算法，例如训练神经网络，或训练线性回归或逻辑回归模型。
    - 适用于**训练集大小非常大的情况，如 1 亿个训练示例。在每一步，我们都会选择1000或m个素例的子集，而不是使用所有1亿个示例。**
        
        ![截屏2023-04-17 21.10.50.png](3%203%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%20728f256b941e4b34a0b4f5612a26a85c/%25E6%2588%25AA%25E5%25B1%258F2023-04-17_21.10.50.png)
        
    - 住房数据集。如果在第一次迭代中，我们只看五个例子，这不是整个数据集。但在下一次迭代中，你举了五个不同的例子，如图所示。
        
        ![截屏2023-04-17 21.13.00.png](3%203%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%20728f256b941e4b34a0b4f5612a26a85c/%25E6%2588%25AA%25E5%25B1%258F2023-04-17_21.13.00.png)
        
- 迷你梯度下降与梯度下降：
    - 梯度下降的每一步都会导致参数可靠地接近中间成本函数的全局最小值。
    - 迷你**批次梯度下降将趋向于全局最小值，不可靠，而且有点吵闹**，但每次迭代在计算上都更便宜，因此**，当您有一个非常大的训练集时，迷你批次学习或迷你批次梯度下降被证明是一个更快的算法**。
        
        ![截屏2023-04-17 21.15.07.png](3%203%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%20728f256b941e4b34a0b4f5612a26a85c/%25E6%2588%25AA%25E5%25B1%258F2023-04-17_21.15.07.png)
        
    
    对于监督学习，如果你有一个非常大的训练集，**迷你批量学习或迷你批量梯度下降，或带有Atom等其他优化算法的迷你批量版本，比批量梯度下降更常见**。
    
- **软更新：**因为每当我们训练一个新的神经网络W_new时，我们只会接受一点点的新值。这将帮助您的**强化学习算法更好地收敛到一个好的解决方案**。使得强化学习算法**不太可能振荡或转移或具有其他不良属性**。
    - 原始版本：**Set Q等于Q_new的这个步骤**。。如果你将一个新的神经网络训练成新的 Q，也许只是碰巧不是一个非常好的神经网络。也许甚至比旧的差一点，然后你只是用一个可能更糟糕的噪音神经网络覆盖了你的Q函数。**软更新方法有助于防止Q_new通过一个不幸的步骤变得更糟。**
        
        ![截屏2023-04-17 21.18.03.png](3%203%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%20728f256b941e4b34a0b4f5612a26a85c/%25E6%2588%25AA%25E5%25B1%258F2023-04-17_21.18.03.png)
        
- **软更新允许您对Q或神经网络参数W和B进行更渐进的更改**，这些更改会影响您当前对s，a的Q函数Q的猜测。
    
    W =  0.01 W_new + 0.99W
    
    B =  0.01 B_new + 0.99B
    
    ![截屏2023-04-17 21.22.46.png](3%203%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%20728f256b941e4b34a0b4f5612a26a85c/%25E6%2588%25AA%25E5%25B1%258F2023-04-17_21.22.46.png)
    

在我们结束之前，我想与您分享我对强化学习状况的想法，以便当您通过监督、无监督、强化学习技术使用不同的机器学习技术构建应用程序时，您就有一个框架来了解强化学习在当今机器学习世界中的位置。让我们在下一个视频中看看那个。

> **一句话总结：**
迷你批次下降：每次迭代只查看子集。训练集数据特别大时，加速强化学习速度。也适用于监督学习。
软更新：每次训练一个新的神经网络W_new时，只接受一点点的新值。使算法不容易震荡或转移不会的特性，更好的收敛。`**W =  0.01 W_new + 0.99W**`
> 

## 强化学习的地位：[The state of reinforcement learning](https://www.coursera.org/learn/unsupervised-learning-recommenders-reinforcement-learning/lecture/uqtNp/the-state-of-reinforcement-learning)

强化学习是一套令人兴奋的技术。我希望与您分享一种实用的感觉，即就他对应用程序的效用而言，强化学习今天的地位。

- 强化学习在**模拟或视频游戏中工作比在真实机器人中工作要容易得多**。因此，即使在他们让它在模拟中工作后，在现实世界或真正的机器人中工作也出乎意料地具有挑战性（失败）。
- 今天强化学习的应用远远少于监督和非监督学习。因此，如果您正在**构建实际应用，您会发现监督学习或无监督学习有用或适合工作工具的几率远高于您最终使用强化学习的几率**。
- 现在有很多令人兴奋的强化学习研究，我认为强化学习对未来应用的潜力非常大。

![截屏2023-04-17 22.02.29.png](3%203%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%20728f256b941e4b34a0b4f5612a26a85c/%25E6%2588%25AA%25E5%25B1%258F2023-04-17_22.02.29.png)

因此，我希望您喜欢本周的强化学习材料，特别是我希望您玩得开心，让月球着陆器为自己着陆。我希望当你实现一个算法，然后看到月球着陆器因你写的代码而安全降落在月球上时，这将是一次令人满意的体验。

- [购买订阅以解锁此课程内容。Continuous state spaces**到期日：May 7, 11:59 PM CST**测验•3 个问题成绩：--](https://www.coursera.org/learn/unsupervised-learning-recommenders-reinforcement-learning/exam/dLfZs/continuous-state-spaces)
- [购买订阅以解锁此课程内容。Reinforcement Learning**到期日：May 7, 11:59 PM CST**编程作业•. Duration: 3 hours3h成绩：--](https://www.coursera.org/learn/unsupervised-learning-recommenders-reinforcement-learning/programming/q5vKI/reinforcement-learning)

# **Summary and thank you**

---

## [Summary and thank you](https://www.coursera.org/learn/unsupervised-learning-recommenders-reinforcement-learning/lecture/ZNo1X/summary-and-thank-you)

欢迎来到这个机器学习专业的最终视频。我们一起看了很多视频，这是最后一个。让我们总结一下我们讨论过的主要话题，然后我想说几句话，然后我们结束课程。回想起来，我想我们一起经历了很多。

我们学习的第一门课程是关于监督机器学习，包括回归和分类。在这里，您了解了线性回归、逻辑回归、成本函数和梯度下降算法。

在第二门课程中，我们研究了更高级的学习算法，包括神经网络、决策树集合，还探讨了机器学习的建议，如偏见和方差，如何使用火车保持交叉验证和测试集，以及如何有效地改进您的学习算法。然后，

第三门课程是关于无监督学习推荐和强化学习，我们讨论了聚类算法、异常检测算法、协作过滤和基于内容的过滤，然后在过去的一周里，强化学习。有了这套广泛的工具，您现在完全有资格构建大量可能的机器学习应用程序。

![截屏2023-04-17 22.03.39.png](3%203%20%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%20728f256b941e4b34a0b4f5612a26a85c/%25E6%2588%25AA%25E5%25B1%258F2023-04-17_22.03.39.png)

恭喜你一直到最后一个视频。如果你一直致力于这个专业，你现在在机器学习方面有了一个非常坚实的基础。我认为你在成为机器学习专家方面有了一个良好的开端。

如你所知，机器学习正在对社会产生巨大影响，作为数十亿人每天通过网络搜索、产品推荐、语音识别和许多其他应用程序使用的强大工具。甚至通过科学发现的希望来提高人类知识，这正在推动数十亿美元的价值，并正在实现几年前难以想象的新应用。

- [Acknowledgments阅读材料•. Duration: 2 minutes2 min](https://www.coursera.org/learn/unsupervised-learning-recommenders-reinforcement-learning/supplement/dogoE/acknowledgments)
- [(Optional) Opportunity to Mentor Other Learners阅读材料•. Duration: 1 minute1 min](https://www.coursera.org/learn/unsupervised-learning-recommenders-reinforcement-learning/supplement/eqe8s/optional-opportunity-to-mentor-other-learners)